{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_utils\n",
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        q_values = self.model(obs)\n",
    "        self.set((f\"{self.prefix}q_values\", t), q_values)\n",
    "\n",
    "class ArgmaxActionSelector(Agent):\n",
    "    \"\"\"BBRL agent that selects the best action based on Q(s,a)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        action = q_values.argmax(-1)\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values\n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values: torch.Tensor = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.shape\n",
    "\n",
    "        # Flag\n",
    "        is_random = torch.rand(size) > self.epsilon\n",
    "        \n",
    "        # Actions (random / argmax)\n",
    "        random_action = torch.randint(nb_actions, size=(size,))\n",
    "        max_action = q_values.argmax(-1)\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = torch.where(is_random, random_action, max_action)\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "class DQN(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Get the two agents (critic and target critic)\n",
    "        critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
    "        target_critic = copy.deepcopy(critic).with_prefix(\"target/\")\n",
    "\n",
    "        # Builds the train agent that will produce transitions\n",
    "        explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "        self.train_policy = Agents(critic, explorer)\n",
    "\n",
    "        self.eval_policy = Agents(critic, ArgmaxActionSelector())\n",
    "\n",
    "        # Creates two temporal agents just for \"replaying\" some parts\n",
    "        # of the transition buffer\n",
    "        self.t_q_agent = TemporalAgent(critic)\n",
    "        self.t_target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "        # Get an agent that is executed on a complete workspace\n",
    "        self.optimizer = setup_optimizer(cfg.optimizer, self.t_q_agent)\n",
    "\n",
    "        self.last_critic_update_step = 0\n",
    "\n",
    "def dqn_compute_critic_loss(\n",
    "    cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "):\n",
    "    \"\"\"Compute the critic loss\n",
    "\n",
    "    :param reward: The reward $r_t$ (shape 2 x B)\n",
    "    :param must_bootstrap: The must bootstrap flag at $t+1$ (shape 2 x B)\n",
    "    :param q_values: The Q-values (shape 2 x B x A)\n",
    "    :param target_q_values: The target Q-values (shape 2 x B x A)\n",
    "    :param action: The chosen actions (shape 2 x B)\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement the DQN loss\n",
    "\n",
    "    # Adapt from the previous notebook and adapt to our case (target Q network)\n",
    "\n",
    "    #current_q_values = q_values[0] #Q(st,at)\n",
    "    #next_q_values = q_values[1] #Q(st1,a)\n",
    "\n",
    "    #max_next_q_values = next_q_values.max(dim=1).values.detach()  #maxQ(st1,a)\n",
    "    #target = reward[1:] + cfg.algorithm.discount_factor * max_next_q_values * must_bootstrap[1:] #r(st,at) + y* maxQ(st1,a)\n",
    "    #q_values = torch.gather(current_q_values, dim=1, index=action[0].unsqueeze(-1)).squeeze(-1)  #calcul new Qvals\n",
    "    \n",
    "    # Compute critic loss (no need to use must_bootstrap here since we are dealing with \"full\" transitions)\n",
    "   # mse = nn.MSELoss()\n",
    "    #critic_loss = mse(target, q_values)\n",
    "\n",
    "    \n",
    "    max_q = target_q_values[1].max(-1).values.detach()\n",
    "    target = (reward[1] + cfg.algorithm.discount_factor * max_q * must_bootstrap[1].int())\n",
    "    act = action[0].unsqueeze(-1)\n",
    "    qvals = torch.gather(q_values[0],dim = 1, index=act).squeeze(-1)\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target,qvals)\n",
    "\n",
    "    return critic_loss\n",
    "\n",
    "def run(dqn: DQN, compute_critic_loss):\n",
    "    for rb in dqn.iter_replay_buffers():\n",
    "        for _ in range(dqn.cfg.algorithm.n_updates):\n",
    "            rb_workspace = rb.get_shuffled(dqn.cfg.algorithm.batch_size)\n",
    "\n",
    "            # The q agent needs to be executed on the rb_workspace workspace\n",
    "            dqn.t_q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "            with torch.no_grad():\n",
    "                dqn.t_target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "\n",
    "            q_values, terminated, reward, action, target_q_values = rb_workspace[\n",
    "                \"q_values\", \"env/terminated\", \"env/reward\", \"action\", \"target/q_values\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            must_bootstrap = ~terminated\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = compute_critic_loss(\n",
    "                dqn.cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "            )\n",
    "            # Store the loss for tensorboard display\n",
    "            dqn.logger.add_log(\"critic_loss\", critic_loss, dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/min\", q_values.max(-1).values.min(), dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/max\", q_values.max(-1).values.max(), dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/mean\", q_values.max(-1).values.mean(), dqn.nb_steps)\n",
    "\n",
    "            dqn.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                dqn.t_q_agent.parameters(), dqn.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            dqn.optimizer.step()\n",
    "\n",
    "            # Update target\n",
    "            if (\n",
    "                dqn.nb_steps - dqn.last_critic_update_step\n",
    "                > dqn.cfg.algorithm.target_critic_update\n",
    "            ):\n",
    "                dqn.last_critic_update_step = dqn.nb_steps\n",
    "                copy_parameters(dqn.t_q_agent, dqn.t_target_q_agent)\n",
    "\n",
    "            dqn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1953), started 0:19:33 ago. (Use '!kill 1953' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3f681e298f829ca3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3f681e298f829ca3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n",
      "(array([[ 0.        ,  0.3       ,  0.825     ,  0.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.02762136,  0.0390625 ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.22097087,  0.3125    ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.24859223,  0.3515625 ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ]]), 0, 0, array([[ 0.        ,  0.3390625 ,  0.825     ,  0.        , -1.        ],\n",
      "       [ 0.03695613, -0.0390625 , -0.03472222,  0.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.19334951,  0.2734375 ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.22097087,  0.3125    ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.        , -0.        ,  0.        ,  0.        ]]), False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (256, 5) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m multi_agent_dqn \u001b[38;5;241m=\u001b[39m MultiAgentDQN(env, params)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m multi_agent_dqn\u001b[38;5;241m.\u001b[39mtrain(n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    125\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[33], line 61\u001b[0m, in \u001b[0;36mMultiAgentDQN.train\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# Update agent\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 61\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_agent(agent)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Periodically update target networks\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_critic_update\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[33], line 74\u001b[0m, in \u001b[0;36mMultiAgentDQN.update_agent\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 74\u001b[0m batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m obs, actions, rewards, next_obs, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     77\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(obs)\n",
      "File \u001b[0;32mmtrand.pyx:920\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (256, 5) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import numpy as np\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class MultiAgentDQN:\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    "        self.params = params\n",
    "        self.agents = env.possible_agents  # List of agents\n",
    "        obs_sample = env.observation_space(self.agents[0]).sample()\n",
    "        self.state_dim = np.prod(obs_sample.shape)\n",
    "\n",
    "        self.action_dim = env.action_space(self.agents[0]).n\n",
    "        \n",
    "        # DQN architecture for each agent\n",
    "\n",
    "        self.critics = {agent: DiscreteQAgent(self.state_dim, params['algorithm']['architecture']['hidden_size'], self.action_dim) for agent in self.agents}\n",
    "        self.target_critics = {agent: copy.deepcopy(self.critics[agent]) for agent in self.agents}\n",
    "        \n",
    "        # Epsilon-greedy explorers\n",
    "        self.explorers = {agent: EGreedyActionSelector(params['algorithm']['epsilon']) for agent in self.agents}\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizers = {agent: Adam(self.critics[agent].parameters(), lr=params['optimizer']['lr']) for agent in self.agents}\n",
    "        \n",
    "        self.buffer = []  # A shared buffer for all agents\n",
    "\n",
    "    def choose_action(self, agent, observation, epsilon=0.02):\n",
    "        # Choose action based on ε-greedy\n",
    "        observation_flat = torch.flatten(torch.Tensor(observation))\n",
    "\n",
    "        q_values = self.critics[agent].model(torch.Tensor(observation_flat).unsqueeze(0))\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space(agent).sample()  # Random action\n",
    "        else:\n",
    "            return q_values.argmax().item()  # Best action\n",
    "\n",
    "    def train(self, n_episodes=1000):\n",
    "        for episode in range(n_episodes):\n",
    "            self.env.reset(seed=42)  # Reset environment\n",
    "            for agent in self.env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "\n",
    "                if termination or truncation:\n",
    "                    self.env.step(None)  # No action if agent is done\n",
    "                else:\n",
    "                    # Choose action based on ε-greedy DQN policy\n",
    "                    action = self.choose_action(agent, observation, epsilon=self.params['algorithm']['epsilon'])\n",
    "                    self.env.step(action)\n",
    "\n",
    "                    # Collect transitions (s, a, r, s') for training\n",
    "                    next_obs, next_rew, next_done, _, _ = self.env.last()\n",
    "                    self.buffer.append((observation, action, reward, next_obs, next_done))\n",
    "\n",
    "                    # Update agent\n",
    "                    if len(self.buffer) >= self.params['algorithm']['batch_size']:\n",
    "                        self.update_agent(agent)\n",
    "\n",
    "            # Periodically update target networks\n",
    "            if episode % self.params['algorithm']['target_critic_update'] == 0:\n",
    "                self.update_target_networks()\n",
    "\n",
    "    def update_agent(self, agent):\n",
    "        if len(self.buffer) < self.params['algorithm']['batch_size']:\n",
    "            return\n",
    "              \n",
    "        print(len(self.buffer), self.params['algorithm']['batch_size'])\n",
    "        print(self.buffer[0])\n",
    "\n",
    "        batch = np.random.choice(self.buffer, self.params['algorithm']['batch_size'])\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "\n",
    "        obs = torch.Tensor(obs)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(-1)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        dones = torch.Tensor(dones)\n",
    "\n",
    "        q_values = self.critics[agent].model(obs).gather(1, actions).squeeze(-1)\n",
    "        next_q_values = self.target_critics[agent].model(next_obs).max(1)[0]\n",
    "        target = rewards + self.params['algorithm']['discount_factor'] * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute critic loss\n",
    "        loss = nn.MSELoss()(q_values, target)\n",
    "\n",
    "        # Optimize the critic network\n",
    "        self.optimizers[agent].zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critics[agent].parameters(), self.params['algorithm']['max_grad_norm'])\n",
    "        self.optimizers[agent].step()\n",
    "\n",
    "    def update_target_networks(self):\n",
    "        for agent in self.agents:\n",
    "            self.target_critics[agent].load_state_dict(self.critics[agent].state_dict())\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"algorithm\": {\n",
    "        \"epsilon\": 0.02,\n",
    "        \"batch_size\": 256,\n",
    "        \"target_critic_update\": 1000,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [256, 256]},\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"n_updates\": 32,\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize the PettingZoo environment\n",
    "env = knights_archers_zombies_v10.env()\n",
    "\n",
    "# Initialize the DQN agent\n",
    "multi_agent_dqn = MultiAgentDQN(env, params)\n",
    "\n",
    "# Train the agent\n",
    "multi_agent_dqn.train(n_episodes=1000)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "\n",
    "env = knights_archers_zombies_v10.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        \n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/dqn-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    \"save_best\": False,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 4,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"n_updates\": 32,\n",
    "        \"eval_interval\": 2000,\n",
    "        \"learning_starts\": 5000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"buffer_size\": 100_000,\n",
    "        \"batch_size\": 256,\n",
    "        \"target_critic_update\": 1_000,\n",
    "        \"max_epochs\": 3_000,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [256, 256]},\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "dqn = DQN(OmegaConf.create(params))\n",
    "run(dqn, dqn_compute_critic_loss)\n",
    "dqn.visualize_best()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
