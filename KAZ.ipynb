{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAZ env  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 DQN pour tous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.18GB > 1.84GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.194    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1720     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 848      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00599  |\n",
      "|    n_updates        | 46       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1778     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1584     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00188  |\n",
      "|    n_updates        | 92       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1798     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2320     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000324 |\n",
      "|    n_updates        | 138      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1849     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2976     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000103 |\n",
      "|    n_updates        | 179      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1849     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 3632     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000337 |\n",
      "|    n_updates        | 220      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1849     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4288     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000397 |\n",
      "|    n_updates        | 261      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1846     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4944     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000596 |\n",
      "|    n_updates        | 302      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1868     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 368      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1871     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6656     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 409      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1865     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 7552     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00025  |\n",
      "|    n_updates        | 465      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1861     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 8208     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000194 |\n",
      "|    n_updates        | 506      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1870     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 9024     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.45e-05 |\n",
      "|    n_updates        | 557      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1872     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 9760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000169 |\n",
      "|    n_updates        | 603      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/supersuit/vector/sb3_vector_wrapper.py:52: UserWarning: PettingZoo environments do not take the `render(mode)` argument, to change rendering mode, re-initialize the environment using the `render_mode` argument.\n",
      "  warnings.warn(\n",
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/pettingzoo/butterfly/knights_archers_zombies/knights_archers_zombies.py:799: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode.\u001b[0m\n",
      "  gymnasium.logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger l'environnement PettingZoo AEC (par défaut)\n",
    "aec_env = knights_archers_zombies_v10.env()\n",
    "\n",
    "# Convertir l'environnement AEC en ParallelEnv\n",
    "parallel_env = aec_to_parallel(aec_env)\n",
    "\n",
    "# Appliquer le wrapper 'black_death_v3' pour gérer les agents inactifs\n",
    "parallel_env = ss.black_death_v3(parallel_env)\n",
    "\n",
    "# Utiliser SuperSuit pour convertir l'environnement en un environnement Gym compatible\n",
    "gym_env = ss.pettingzoo_env_to_vec_env_v1(parallel_env)\n",
    "gym_env = ss.concat_vec_envs_v1(gym_env, 1, base_class='stable_baselines3')\n",
    "\n",
    "# Initialiser le modèle DQN\n",
    "model = DQN('MlpPolicy', gym_env, verbose=1)\n",
    "\n",
    "# Entraîner l'agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save(\"dqn_knights_archers_zombies\")\n",
    "\n",
    "# Charger le modèle pour l'utiliser ou l'évaluer\n",
    "model = DQN.load(\"dqn_knights_archers_zombies\")\n",
    "\n",
    "# Évaluer le modèle\n",
    "env = knights_archers_zombies_v10.parallel_env()\n",
    "env = ss.black_death_v3(env)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 1, base_class='stable_baselines3')\n",
    "model.set_env(env)\n",
    "\n",
    "obs = env.reset()\n",
    "rewards = []\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if done.any():\n",
    "        obs = env.reset()\n",
    "\n",
    "# Afficher les récompenses\n",
    "plt.plot(np.cumsum(rewards))\n",
    "plt.show()\n",
    "\n",
    "# # Make a video of the trained model\n",
    "# env = knights_archers_zombies_v10.parallel_env()\n",
    "# env = ss.black_death_v3(env)\n",
    "# env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "# env = ss.concat_vec_envs_v1(env, 1, base_class='stable_baselines3')\n",
    "# model.set_env(env)\n",
    "# ss.record_video(env, model, \"dqn_knights_archers_zombies.mp4\", video_length=1000, fps=10)\n",
    "\n",
    "# # Afficher la vidéo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 DQN indep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Temps écoulé: 00:00:00 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'checkpoints' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 - Temps écoulé: 00:02:18 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 3 - Temps écoulé: 00:04:38 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 4 - Temps écoulé: 00:06:57 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 5 - Temps écoulé: 00:09:17 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 6 - Temps écoulé: 00:11:37 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 7 - Temps écoulé: 00:13:57 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 8 - Temps écoulé: 00:16:17 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 9 - Temps écoulé: 00:18:38 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 10 - Temps écoulé: 00:21:00 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 11 - Temps écoulé: 00:23:21 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 12 - Temps écoulé: 00:25:41 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 13 - Temps écoulé: 00:28:01 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 14 - Temps écoulé: 00:30:20 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 15 - Temps écoulé: 00:32:40 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 16 - Temps écoulé: 00:35:00 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 17 - Temps écoulé: 00:37:21 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 18 - Temps écoulé: 00:39:42 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 19 - Temps écoulé: 00:42:04 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 20 - Temps écoulé: 00:44:25 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Entraînement terminé !\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "# Charger l'environnement PettingZoo AEC (par défaut)\n",
    "aec_env = knights_archers_zombies_v10.env()\n",
    "\n",
    "# Convertir l'environnement AEC en ParallelEnv\n",
    "parallel_env = aec_to_parallel(aec_env)\n",
    "\n",
    "# Appliquer le wrapper 'black_death_v3' pour gérer les agents inactifs\n",
    "parallel_env = ss.black_death_v3(parallel_env)\n",
    "\n",
    "# Utiliser SuperSuit pour convertir l'environnement en un environnement Gym compatible\n",
    "gym_env = ss.pettingzoo_env_to_vec_env_v1(parallel_env)\n",
    "gym_env = ss.concat_vec_envs_v1(gym_env, 1, base_class='stable_baselines3')\n",
    "\n",
    "# Initialiser les modèles DQN pour chaque agent\n",
    "num_agents = gym_env.num_envs  # Récupérer le nombre d'agents à partir de l'environnement\n",
    "models_archer = [DQN(  \n",
    "                'MlpPolicy', \n",
    "                gym_env, \n",
    "                verbose=0,\n",
    "                learning_rate=0.001,\n",
    "                buffer_size=10000,\n",
    "                batch_size=64,\n",
    "                learning_starts=200,\n",
    "                train_freq=1,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=100,\n",
    "                exploration_fraction=0.3,\n",
    "                exploration_initial_eps=0.05,\n",
    "                exploration_final_eps=0.03,\n",
    "                gamma=0.96\n",
    "\n",
    "            ) for _ in range(2)]\n",
    "\n",
    "models_knight = [DQN(  \n",
    "                'MlpPolicy', \n",
    "                gym_env, \n",
    "                verbose=0,\n",
    "                learning_rate=0.001,\n",
    "                buffer_size=10000,\n",
    "                batch_size=64,\n",
    "                learning_starts=200,\n",
    "                train_freq=1,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=100,\n",
    "                exploration_fraction=0.3,\n",
    "                exploration_initial_eps=0.05,\n",
    "                exploration_final_eps=0.03,\n",
    "                gamma=0.96\n",
    "            ) for _ in range(2)]\n",
    "\n",
    "models = models_archer + models_knight\n",
    "\n",
    "### TODO TRAIN AGENT INDENPENDENTLY FIRST THEN TRAIN THEM TOGETHER\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "# Entraîner les agents\n",
    "timesteps = 10_000\n",
    "nb_episodes = 20\n",
    "for episode in range(nb_episodes):  # Nombre d'épisodes d'entraînement\n",
    "    t = time.time() - start_t\n",
    "    hours, remainder = divmod(t, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Episode {episode + 1} - Temps écoulé: {int(hours):02}:{int(minutes):02}:{int(seconds):02} s\")\n",
    "    obs = gym_env.reset()\n",
    "    done = np.array([False] * num_agents)\n",
    "\n",
    "    for step in range(timesteps):\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step}/{timesteps}\")\n",
    "        actions = np.array([models[i].predict(obs[i])[0] for i in range(num_agents)])  # Prédire les actions pour chaque agent\n",
    "        obs, rewards_batch, done, _ = gym_env.step(actions)  # Appliquer les actions dans l'environnement\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            models[i].learn(total_timesteps=1)  # Mettre à jour chaque agent après chaque étape\n",
    "        \n",
    "        if done.all():\n",
    "            obs = gym_env.reset()\n",
    "            done = np.array([False] * num_agents)\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        models[i].learn(total_timesteps=100)\n",
    "\n",
    "    # Sauvegarder les modèles\n",
    "    for i in range(num_agents):\n",
    "        models[i].save(f\"./checkpoints/dqn_agent_{i + 1}_knights_archers_zombies_{episode}\")\n",
    "\n",
    "# Sauvegarder les modèles\n",
    "for i in range(num_agents):\n",
    "    models[i].save(f\"dqn_agent_{i + 1}_knights_archers_zombies\")\n",
    "\n",
    "print(\"Entraînement terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent archer_0 a choisi l'action 0\n",
      "Agent archer_1 a choisi l'action 3\n",
      "Agent knight_0 a choisi l'action 1\n",
      "Agent knight_1 a choisi l'action 2\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 1/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 2\n",
      "Agent archer_1 a choisi l'action 0\n",
      "Agent knight_0 a choisi l'action 5\n",
      "Agent knight_1 a choisi l'action 0\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 2/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 3\n",
      "Agent archer_1 a choisi l'action 1\n",
      "Agent knight_0 a choisi l'action 1\n",
      "Agent knight_1 a choisi l'action 4\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 3/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 0\n",
      "Agent archer_1 a choisi l'action 3\n",
      "Agent knight_0 a choisi l'action 1\n",
      "Agent knight_1 a choisi l'action 1\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 4/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 4\n",
      "Agent archer_1 a choisi l'action 5\n",
      "Agent knight_0 a choisi l'action 1\n",
      "Agent knight_1 a choisi l'action 3\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 5/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 4\n",
      "Agent archer_1 a choisi l'action 0\n",
      "Agent knight_0 a choisi l'action 3\n",
      "Agent knight_1 a choisi l'action 0\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 6/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 5\n",
      "Agent archer_1 a choisi l'action 4\n",
      "Agent knight_0 a choisi l'action 2\n",
      "Agent knight_1 a choisi l'action 4\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 7/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 2\n",
      "Agent archer_1 a choisi l'action 1\n",
      "Agent knight_0 a choisi l'action 1\n",
      "Agent knight_1 a choisi l'action 1\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 8/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n",
      "Agent archer_0 a choisi l'action 1\n",
      "Agent archer_1 a choisi l'action 4\n",
      "Agent knight_0 a choisi l'action 3\n",
      "Agent knight_1 a choisi l'action 0\n",
      "Erreur lors du passage d'actions à l'environnement: action is not in action space\n",
      "Episode 9/100 - Rewards: {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 5 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done[agent]:\n\u001b[0;32m---> 93\u001b[0m         action \u001b[38;5;241m=\u001b[39m select_action(state[agent], q_networks[agent], epsilon, action_dims)\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# Vérification de l'action\u001b[39;00m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m<\u001b[39m action_dims, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pour \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mest pas dans l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mespace d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_dims\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(state, q_net, epsilon, action_dim)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m#print(\"state,\",state)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m        \u001b[38;5;66;03m# state = state.flatten()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         q_values \u001b[38;5;241m=\u001b[39m q_net(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state\u001b[38;5;241m.\u001b[39mT))\n\u001b[0;32m---> 31\u001b[0m         action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(q_values,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Assurez-vous que l'action est valide\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m<\u001b[39m action_dim, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hors limites [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_dim\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 5 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "# Configuration du réseau Q pour chaque agent\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Fonction de sélection d'action avec epsilon-greedy\n",
    "def select_action(state, q_net, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, action_dim - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            #print(\"state,\",state)\n",
    "           # state = state.flatten()\n",
    "            q_values = q_net(torch.FloatTensor(state.T))\n",
    "            action = torch.argmax(q_values,dim=1).item()\n",
    "    \n",
    "    # Assurez-vous que l'action est valide\n",
    "    assert 0 <= action < action_dim, f\"Action {action} hors limites [0, {action_dim - 1}]\"\n",
    "    return action\n",
    "\n",
    "# Buffer de replay pour stocker les expériences de chaque agent\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialisation de l'environnement et de chaque agent\n",
    "env = knights_archers_zombies_v10.env()\n",
    "env.reset()\n",
    "agents = env.agents  # Obtenir la liste des agents après reset\n",
    "num_agents = len(agents)\n",
    "state_dims = env.observation_space(agents[0]).shape[0]\n",
    "action_dims = env.action_space(agents[0]).n\n",
    "\n",
    "# Hyperparamètres\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.95\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "target_update_freq = 100\n",
    "buffer_capacity = 10000\n",
    "num_episodes = 100  # Augmentez le nombre d'épisodes pour un meilleur entraînement\n",
    "\n",
    "# Initialisation des réseaux Q et des buffers pour chaque agent\n",
    "q_networks = {agent: QNetwork(state_dims, action_dims) for agent in agents}\n",
    "target_networks = {agent: QNetwork(state_dims, action_dims) for agent in agents}\n",
    "optimizers = {agent: optim.Adam(q_networks[agent].parameters(), lr=lr) for agent in agents}\n",
    "buffers = {agent: ReplayBuffer(buffer_capacity) for agent in agents}\n",
    "\n",
    "# Synchronisation initiale des réseaux cibles\n",
    "for agent in agents:\n",
    "    target_networks[agent].load_state_dict(q_networks[agent].state_dict())\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    state = {agent: env.observe(agent) for agent in agents}  # Initialiser l'état pour chaque agent\n",
    "    done = {agent: False for agent in agents}\n",
    "    episode_reward = {agent: 0 for agent in agents}\n",
    "\n",
    "    while not all(done.values()):\n",
    "        actions = {}\n",
    "        \n",
    "        # Sélection d'actions pour chaque agent\n",
    "        for agent in agents:\n",
    "            if not done[agent]:\n",
    "                action = select_action(state[agent], q_networks[agent], epsilon, action_dims)\n",
    "                \n",
    "                # Vérification de l'action\n",
    "                assert 0 <= action < action_dims, f\"Action {action} pour {agent} n'est pas dans l'espace d'actions [0, {action_dims - 1}]\"\n",
    "                actions[agent] = action\n",
    "                print(f\"Agent {agent} a choisi l'action {action}\")\n",
    "\n",
    "        # Environnement procède aux actions\n",
    "        try:\n",
    "            next_state, rewards, dones, _ = env.step(actions)\n",
    "        except AssertionError as e:\n",
    "            print(f\"Erreur lors du passage d'actions à l'environnement: {e}\")\n",
    "            break  # Sortir de la boucle si une erreur se produit\n",
    "\n",
    "        # Enregistrement de chaque expérience dans le buffer\n",
    "        for agent in agents:\n",
    "            if not done[agent]:\n",
    "                buffers[agent].push(state[agent], actions[agent], rewards[agent], next_state[agent], dones[agent])\n",
    "                episode_reward[agent] += rewards[agent]\n",
    "\n",
    "            # Mise à jour des états et terminaux\n",
    "            done[agent] = dones[agent]\n",
    "        \n",
    "        # Mise à jour des états pour le prochain pas de temps\n",
    "        for agent in agents:\n",
    "            if not done[agent]:\n",
    "                state[agent] = next_state[agent]  # Mettre à jour l'état pour chaque agent\n",
    "\n",
    "        # Apprentissage pour chaque agent\n",
    "        for agent in agents:\n",
    "            if len(buffers[agent]) >= batch_size and not done[agent]:\n",
    "                # Extraction d'un mini-lot d'expériences\n",
    "                batch = buffers[agent].sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Conversion des expériences en tenseurs\n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards = torch.FloatTensor(rewards)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.FloatTensor(dones)\n",
    "\n",
    "                # Calcul des valeurs Q cibles\n",
    "                current_q_values = q_networks[agent](states).gather(1, actions).squeeze()\n",
    "                with torch.no_grad():\n",
    "                    max_next_q_values = target_networks[agent](next_states).max(1)[0]\n",
    "                    target_q_values = rewards + (gamma * max_next_q_values * (1 - dones))\n",
    "\n",
    "                # Calcul de la perte et rétropropagation\n",
    "                loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "                optimizers[agent].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizers[agent].step()\n",
    "\n",
    "        # Mise à jour des réseaux cibles\n",
    "        if episode % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                target_networks[agent].load_state_dict(q_networks[agent].state_dict())\n",
    "\n",
    "    # Décroissance de epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} - Rewards: {episode_reward}\")\n",
    "\n",
    "print(\"Entraînement terminé!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
