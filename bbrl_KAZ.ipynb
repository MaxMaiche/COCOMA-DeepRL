{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc10861",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_utils\n",
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "# # Prepare the environment\n",
    "\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, copy_parameters\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d49589",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        q_values = self.model(obs)\n",
    "        self.set((f\"{self.prefix}q_values\", t), q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb9e90",
   "metadata": {},
   "source": [
    "### Creating an actor agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a409f30e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ArgmaxActionSelector(Agent):\n",
    "    \"\"\"BBRL agent that selects the best action based on Q(s,a)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        action = q_values.argmax(-1)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6f58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values\n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values: torch.Tensor = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.shape\n",
    "\n",
    "        # Flag\n",
    "        is_random = torch.rand(size) > self.epsilon\n",
    "        \n",
    "        # Actions (random / argmax)\n",
    "        random_action = torch.randint(nb_actions, size=(size,))\n",
    "        max_action = q_values.argmax(-1)\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = torch.where(is_random, random_action, max_action)\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa677918",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DQN(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Get the two agents (critic and target critic)\n",
    "        critic = DiscreteQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.hidden_size, act_size\n",
    "        )\n",
    "        target_critic = copy.deepcopy(critic).with_prefix(\"target/\")\n",
    "\n",
    "        # Builds the train agent that will produce transitions\n",
    "        explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "        self.train_policy = Agents(critic, explorer)\n",
    "\n",
    "        self.eval_policy = Agents(critic, ArgmaxActionSelector())\n",
    "\n",
    "        # Creates two temporal agents just for \"replaying\" some parts\n",
    "        # of the transition buffer\n",
    "        self.t_q_agent = TemporalAgent(critic)\n",
    "        self.t_target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "        # Get an agent that is executed on a complete workspace\n",
    "        self.optimizer = setup_optimizer(cfg.optimizer, self.t_q_agent)\n",
    "\n",
    "        self.last_critic_update_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd62e1b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def dqn_compute_critic_loss(\n",
    "    cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "):\n",
    "    \"\"\"Compute the critic loss\n",
    "\n",
    "    :param reward: The reward $r_t$ (shape 2 x B)\n",
    "    :param must_bootstrap: The must bootstrap flag at $t+1$ (shape 2 x B)\n",
    "    :param q_values: The Q-values (shape 2 x B x A)\n",
    "    :param target_q_values: The target Q-values (shape 2 x B x A)\n",
    "    :param action: The chosen actions (shape 2 x B)\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement the DQN loss\n",
    "\n",
    "    # Adapt from the previous notebook and adapt to our case (target Q network)\n",
    "\n",
    "    #current_q_values = q_values[0] #Q(st,at)\n",
    "    #next_q_values = q_values[1] #Q(st1,a)\n",
    "\n",
    "    #max_next_q_values = next_q_values.max(dim=1).values.detach()  #maxQ(st1,a)\n",
    "    #target = reward[1:] + cfg.algorithm.discount_factor * max_next_q_values * must_bootstrap[1:] #r(st,at) + y* maxQ(st1,a)\n",
    "    #q_values = torch.gather(current_q_values, dim=1, index=action[0].unsqueeze(-1)).squeeze(-1)  #calcul new Qvals\n",
    "    \n",
    "    # Compute critic loss (no need to use must_bootstrap here since we are dealing with \"full\" transitions)\n",
    "   # mse = nn.MSELoss()\n",
    "    #critic_loss = mse(target, q_values)\n",
    "\n",
    "    \n",
    "    max_q = target_q_values[1].max(-1).values.detach()\n",
    "    target = (reward[1] + cfg.algorithm.discount_factor * max_q * must_bootstrap[1].int())\n",
    "    act = action[0].unsqueeze(-1)\n",
    "    qvals = torch.gather(q_values[0],dim = 1, index=act).squeeze(-1)\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target,qvals)\n",
    "\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c788b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fea1c82",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def run(dqn: DQN, compute_critic_loss):\n",
    "    for rb in dqn.iter_replay_buffers():\n",
    "        for _ in range(dqn.cfg.algorithm.n_updates):\n",
    "            rb_workspace = rb.get_shuffled(dqn.cfg.algorithm.batch_size)\n",
    "\n",
    "            # The q agent needs to be executed on the rb_workspace workspace\n",
    "            dqn.t_q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "            with torch.no_grad():\n",
    "                dqn.t_target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "\n",
    "            q_values, terminated, reward, action, target_q_values = rb_workspace[\n",
    "                \"q_values\", \"env/terminated\", \"env/reward\", \"action\", \"target/q_values\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            must_bootstrap = ~terminated\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = compute_critic_loss(\n",
    "                dqn.cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "            )\n",
    "            # Store the loss for tensorboard display\n",
    "            dqn.logger.add_log(\"critic_loss\", critic_loss, dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/min\", q_values.max(-1).values.min(), dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/max\", q_values.max(-1).values.max(), dqn.nb_steps)\n",
    "            dqn.logger.add_log(\"q_values/mean\", q_values.max(-1).values.mean(), dqn.nb_steps)\n",
    "\n",
    "            dqn.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                dqn.t_q_agent.parameters(), dqn.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            dqn.optimizer.step()\n",
    "\n",
    "            # Update target\n",
    "            if (\n",
    "                dqn.nb_steps - dqn.last_critic_update_step\n",
    "                > dqn.cfg.algorithm.target_critic_update\n",
    "            ):\n",
    "                dqn.last_critic_update_step = dqn.nb_steps\n",
    "                copy_parameters(dqn.t_q_agent, dqn.t_target_q_agent)\n",
    "\n",
    "            dqn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15dfc6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6e69aefc603198c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6e69aefc603198c3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e6e66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigAttributeError",
     "evalue": "Missing key gym_env\n    full_key: gym_env\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigAttributeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m     act_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space(agent_name)\u001b[38;5;241m.\u001b[39mn  \u001b[38;5;66;03m# Assuming discrete action space\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     agent_params \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)  \u001b[38;5;66;03m# Create separate params for each agent if needed\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     agent \u001b[38;5;241m=\u001b[39m DQN(agent_params)\n\u001b[1;32m     46\u001b[0m     agents\u001b[38;5;241m.\u001b[39mappend(agent)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_multi_agent\u001b[39m(dqns, compute_critic_loss):\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(cfg)\n\u001b[1;32m      5\u001b[0m     obs_size, act_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39mget_obs_and_actions_sizes()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Get the two agents (critic and target critic)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bbrl_utils/algorithms.py:217\u001b[0m, in \u001b[0;36mEpochBasedAlgo.__init__\u001b[0;34m(self, cfg, env_wrappers)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg, env_wrappers\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new epoch-based RL algorithm\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    :param cfg: The configuration\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    :param env_wrappers: A list of factories, defaults to []\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(cfg, env_wrappers\u001b[38;5;241m=\u001b[39menv_wrappers)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# We use a non-autoreset workspace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env \u001b[38;5;241m=\u001b[39m ParallelGymAgent(\n\u001b[1;32m    221\u001b[0m         partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_env, autoreset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    222\u001b[0m         cfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_envs,\n\u001b[1;32m    223\u001b[0m     )\u001b[38;5;241m.\u001b[39mseed(cfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bbrl_utils/algorithms.py:69\u001b[0m, in \u001b[0;36mRLBase.__init__\u001b[0;34m(self, cfg, env_wrappers)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg, env_wrappers\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Basic initialization\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_env \u001b[38;5;241m=\u001b[39m partial(make_env, cfg\u001b[38;5;241m.\u001b[39mgym_env\u001b[38;5;241m.\u001b[39menv_name, wrappers\u001b[38;5;241m=\u001b[39menv_wrappers)\n\u001b[1;32m     70\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(cfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Sets the base directory and logger directory\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/dictconfig.py:355\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[1;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[0;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     format_and_raise(\n\u001b[1;32m    232\u001b[0m         node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m    234\u001b[0m         value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    235\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(cause) \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg,\n\u001b[1;32m    236\u001b[0m         cause\u001b[38;5;241m=\u001b[39mcause,\n\u001b[1;32m    237\u001b[0m         type_override\u001b[38;5;241m=\u001b[39mtype_override,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[1;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[0;32m--> 899\u001b[0m _raise(ex, cause)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[1;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/dictconfig.py:442\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[0;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_impl\u001b[39m(\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: DictKeyType, default_value: Any, validate_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    440\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_child(\n\u001b[1;32m    443\u001b[0m             key\u001b[38;5;241m=\u001b[39mkey, throw_on_missing_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validate_key\u001b[38;5;241m=\u001b[39mvalidate_key\n\u001b[1;32m    444\u001b[0m         )\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ConfigAttributeError, ConfigKeyError):\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_MARKER_:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/basecontainer.py:73\u001b[0m, in \u001b[0;36mBaseContainer._get_child\u001b[0;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_child\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     throw_on_missing_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Optional[Node], List[Optional[Node]]]:\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node(\n\u001b[1;32m     74\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m     75\u001b[0m         validate_access\u001b[38;5;241m=\u001b[39mvalidate_access,\n\u001b[1;32m     76\u001b[0m         validate_key\u001b[38;5;241m=\u001b[39mvalidate_key,\n\u001b[1;32m     77\u001b[0m         throw_on_missing_value\u001b[38;5;241m=\u001b[39mthrow_on_missing_value,\n\u001b[1;32m     78\u001b[0m         throw_on_missing_key\u001b[38;5;241m=\u001b[39mthrow_on_missing_key,\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, UnionNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_special(child):\n\u001b[1;32m     81\u001b[0m         value \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39m_value()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/omegaconf/dictconfig.py:480\u001b[0m, in \u001b[0;36mDictConfig._get_node\u001b[0;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m throw_on_missing_key:\n\u001b[0;32m--> 480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m throw_on_missing_value \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39m_is_missing():\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConfigAttributeError\u001b[0m: Missing key gym_env\n    full_key: gym_env\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "\n",
    "# Define your parameters\n",
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/dqn-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    \"save_best\": False,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 4,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"n_updates\": 32,\n",
    "        \"eval_interval\": 2000,\n",
    "        \"learning_starts\": 5000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"buffer_size\": 100_000,\n",
    "        \"batch_size\": 256,\n",
    "        \"target_critic_update\": 1_000,\n",
    "        \"max_epochs\": 3_000,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [256, 256]},\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"knights_archers_zombies_v10\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize the environment\n",
    "env = knights_archers_zombies_v10.env()\n",
    "env.reset()\n",
    "# Get the list of agents\n",
    "agents_list = env.agents\n",
    "\n",
    "# Initialize DQN agents for each agent in the environment\n",
    "agents = []\n",
    "for agent_name in agents_list:\n",
    "    obs_size = env.observation_space(agent_name).shape[0]  # Assuming 1D observation\n",
    "    act_size = env.action_space(agent_name).n  # Assuming discrete action space\n",
    "    agent_params = OmegaConf.create(params)  # Create separate params for each agent if needed\n",
    "    agent = DQN(agent_params)\n",
    "    agents.append(agent)\n",
    "\n",
    "def run_multi_agent(dqns, compute_critic_loss):\n",
    "    for rb in dqns[0].iter_replay_buffers():  # Assuming all agents share the same buffer\n",
    "        for _ in range(dqns[0].cfg.algorithm.n_updates):\n",
    "            rb_workspace = rb.get_shuffled(dqns[0].cfg.algorithm.batch_size)\n",
    "\n",
    "            for dqn in dqns:\n",
    "                # Execute the agent on the replay buffer workspace\n",
    "                dqn.t_q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "                with torch.no_grad():\n",
    "                    dqn.t_target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "\n",
    "            # Retrieve necessary values\n",
    "            q_values, terminated, reward, action, target_q_values = rb_workspace[\n",
    "                \"q_values\", \"env/terminated\", \"env/reward\", \"action\", \"target/q_values\"\n",
    "            ]\n",
    "\n",
    "            # Determine whether values of the critic should be propagated\n",
    "            must_bootstrap = ~terminated\n",
    "\n",
    "            # Compute critic loss for each agent\n",
    "            for dqn in dqns:\n",
    "                critic_loss = compute_critic_loss(\n",
    "                    dqn.cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "                )\n",
    "\n",
    "                # Store the loss for tensorboard display\n",
    "                dqn.logger.add_log(\"critic_loss\", critic_loss, dqn.nb_steps)\n",
    "                dqn.optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    dqn.t_q_agent.parameters(), dqn.cfg.algorithm.max_grad_norm\n",
    "                )\n",
    "                dqn.optimizer.step()\n",
    "\n",
    "                # Update target\n",
    "                if (\n",
    "                    dqn.nb_steps - dqn.last_critic_update_step\n",
    "                    > dqn.cfg.algorithm.target_critic_update\n",
    "                ):\n",
    "                    dqn.last_critic_update_step = dqn.nb_steps\n",
    "                    copy_parameters(dqn.t_q_agent, dqn.t_target_q_agent)\n",
    "\n",
    "            # Evaluate agents\n",
    "            for dqn in dqns:\n",
    "                dqn.evaluate()\n",
    "\n",
    "# Run the multi-agent DQN\n",
    "run_multi_agent(agents, dqn_compute_critic_loss)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
