{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAZ env  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 DQN pour tous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prise en main de l'environement, avec 1 DQN pour tous les agents. Implémentation avec Stable Baselines 3. Cette approche est très peu efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "\n",
    "class RewardTrackingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback pour suivre et enregistrer les récompenses par épisode.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardTrackingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []  # Liste pour enregistrer les récompenses\n",
    "        self.current_rewards = 0  # Récompenses cumulées pour l'épisode\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Ajouter la récompense actuelle à la récompense cumulée\n",
    "        self.current_rewards += np.sum(self.locals['rewards'])  # Somme des récompenses des agents\n",
    "\n",
    "        # Vérifier si un épisode est terminé\n",
    "        if np.any(self.locals['dones']):  # Si un agent termine son épisode\n",
    "            self.episode_rewards.append(self.current_rewards)\n",
    "            self.current_rewards = 0\n",
    "\n",
    "            # Optionnel : Afficher les récompenses en temps réel\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Épisode terminé, récompense cumulée : {self.episode_rewards[-1]}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # À la fin de l'entraînement, afficher ou enregistrer les récompenses\n",
    "        print(\"Entraînement terminé. Récompenses cumulées par épisode :\")\n",
    "        print(self.episode_rewards)\n",
    "\n",
    "        # Optionnel : Sauvegarder les récompenses dans un fichier\n",
    "        np.save(\"episode_rewards.npy\", self.episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./logs/dqn_knights_archers_zombies_SoloDQN_4\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.798    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 2303     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 656      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.794    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 2198     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1632     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 37       |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.791    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2152     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2368     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000687 |\n",
      "|    n_updates        | 83       |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.788    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 2096     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 3104     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00828  |\n",
      "|    n_updates        | 129      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 2075     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 3920     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000258 |\n",
      "|    n_updates        | 180      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 2063     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4736     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00767  |\n",
      "|    n_updates        | 231      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 2060     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 5632     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000221 |\n",
      "|    n_updates        | 287      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.776    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 2034     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6288     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000261 |\n",
      "|    n_updates        | 328      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.773    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 2036     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 7184     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 384      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.77     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 2033     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000155 |\n",
      "|    n_updates        | 435      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.768    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 2028     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 8656     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000733 |\n",
      "|    n_updates        | 476      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.765    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 2030     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 9312     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000231 |\n",
      "|    n_updates        | 517      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.762    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 2024     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 10048    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00804  |\n",
      "|    n_updates        | 563      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.76     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 2022     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 10704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000633 |\n",
      "|    n_updates        | 604      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 2021     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 11360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000303 |\n",
      "|    n_updates        | 645      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.754    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 2019     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 12176    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000194 |\n",
      "|    n_updates        | 696      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.752    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 2020     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 12912    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00692  |\n",
      "|    n_updates        | 742      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.749    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 2018     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 13568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000209 |\n",
      "|    n_updates        | 783      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.746    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 2017     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 14464    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000166 |\n",
      "|    n_updates        | 839      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.743    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 2017     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 15200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00696  |\n",
      "|    n_updates        | 885      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.74     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 2018     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 15936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000243 |\n",
      "|    n_updates        | 931      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.738    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 2000     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 16592    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000176 |\n",
      "|    n_updates        | 972      |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1999     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 17568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000775 |\n",
      "|    n_updates        | 1033     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.731    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1996     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 18304    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000154 |\n",
      "|    n_updates        | 1079     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 9.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.728    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1994     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 19280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00765  |\n",
      "|    n_updates        | 1140     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.725    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 1996     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 19936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00755  |\n",
      "|    n_updates        | 1181     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.723    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 1986     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 20592    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000133 |\n",
      "|    n_updates        | 1222     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 1986     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 21248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0024   |\n",
      "|    n_updates        | 1263     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 9.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.716    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 1990     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 22464    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00045  |\n",
      "|    n_updates        | 1339     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.713    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 1986     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 23280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000994 |\n",
      "|    n_updates        | 1390     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.71     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 1985     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 23936    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0147   |\n",
      "|    n_updates        | 1431     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.707    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 24752    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00735  |\n",
      "|    n_updates        | 1482     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.704    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 25568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000394 |\n",
      "|    n_updates        | 1533     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 1984     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 26304    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00219  |\n",
      "|    n_updates        | 1579     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.698    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 1985     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 27200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0015   |\n",
      "|    n_updates        | 1635     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.695    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 1986     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 28016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000273 |\n",
      "|    n_updates        | 1686     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.692    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 28672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000243 |\n",
      "|    n_updates        | 1727     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.69     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 29328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000245 |\n",
      "|    n_updates        | 1768     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.687    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 1978     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 30144    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00836  |\n",
      "|    n_updates        | 1819     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 1978     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 30800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000442 |\n",
      "|    n_updates        | 1860     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.681    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 31696    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00821  |\n",
      "|    n_updates        | 1916     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.678    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 32432    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 1962     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 33168    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000224 |\n",
      "|    n_updates        | 2008     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.673    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 33824    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000397 |\n",
      "|    n_updates        | 2049     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.671    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 34480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000258 |\n",
      "|    n_updates        | 2090     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 35216    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00842  |\n",
      "|    n_updates        | 2136     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.665    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 36112    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000368 |\n",
      "|    n_updates        | 2192     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.662    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 36768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00708  |\n",
      "|    n_updates        | 2233     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.659    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 37584    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 2284     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.657    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 38240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000637 |\n",
      "|    n_updates        | 2325     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 38896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000445 |\n",
      "|    n_updates        | 2366     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.651    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 39792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00037  |\n",
      "|    n_updates        | 2422     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.647    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 40688    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00899  |\n",
      "|    n_updates        | 2478     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.644    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 41584    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 2534     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.642    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 42240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0145   |\n",
      "|    n_updates        | 2575     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.639    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 42976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00797  |\n",
      "|    n_updates        | 2621     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.636    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 43712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0147   |\n",
      "|    n_updates        | 2667     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.633    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 44448    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000243 |\n",
      "|    n_updates        | 2713     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.63     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 1980     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 45424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00161  |\n",
      "|    n_updates        | 2774     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.626    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 1980     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 46320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00384  |\n",
      "|    n_updates        | 2830     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.624    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 1980     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 47056    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00799  |\n",
      "|    n_updates        | 2876     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.621    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 47712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00805  |\n",
      "|    n_updates        | 2917     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.617    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 48768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 2983     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.614    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 49584    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00614  |\n",
      "|    n_updates        | 3034     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 1981     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 50320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000261 |\n",
      "|    n_updates        | 3080     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.608    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 51216    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00804  |\n",
      "|    n_updates        | 3136     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.605    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 52032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000702 |\n",
      "|    n_updates        | 3187     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.602    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 52688    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00873  |\n",
      "|    n_updates        | 3228     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 12.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.597    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 54144    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0152   |\n",
      "|    n_updates        | 3319     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 1982     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 55040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00203  |\n",
      "|    n_updates        | 3375     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.59     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 56016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00963  |\n",
      "|    n_updates        | 3436     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.587    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 1980     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 56672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000635 |\n",
      "|    n_updates        | 3477     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.585    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 57328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00669  |\n",
      "|    n_updates        | 3518     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.583    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 57984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00984  |\n",
      "|    n_updates        | 3559     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 9.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.578    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 59120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000907 |\n",
      "|    n_updates        | 3630     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.576    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 1980     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 59856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 3676     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 60592    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000631 |\n",
      "|    n_updates        | 3722     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.57     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 61328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00608  |\n",
      "|    n_updates        | 3768     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.568    |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 1979     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 61984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00817  |\n",
      "|    n_updates        | 3809     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.564    |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 1978     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 62960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000828 |\n",
      "|    n_updates        | 3870     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.561    |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 1978     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 63776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000362 |\n",
      "|    n_updates        | 3921     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.557    |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 1975     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 64752    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00806  |\n",
      "|    n_updates        | 3982     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.555    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 1974     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 65408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000903 |\n",
      "|    n_updates        | 4023     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.552    |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 1974     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 66064    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00346  |\n",
      "|    n_updates        | 4064     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.55     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 1975     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 66720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00882  |\n",
      "|    n_updates        | 4105     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.546    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 1974     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 67616    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 4161     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.543    |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 1972     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 68512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00752  |\n",
      "|    n_updates        | 4217     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.54     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 1972     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 69248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000904 |\n",
      "|    n_updates        | 4263     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.537    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 1972     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 70064    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000591 |\n",
      "|    n_updates        | 4314     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.535    |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 1973     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 70720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00953  |\n",
      "|    n_updates        | 4355     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.532    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 1973     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 71376    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.001    |\n",
      "|    n_updates        | 4396     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.53     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 1972     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 72032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0138   |\n",
      "|    n_updates        | 4437     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.527    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 1971     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 72848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00227  |\n",
      "|    n_updates        | 4488     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.524    |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 1971     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 73504    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0011   |\n",
      "|    n_updates        | 4529     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.522    |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 1971     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 74160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00123  |\n",
      "|    n_updates        | 4570     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 9.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.518    |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 1971     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 75136    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 4631     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 1971     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 75952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000886 |\n",
      "|    n_updates        | 4682     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.513    |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 1970     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 76608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00157  |\n",
      "|    n_updates        | 4723     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.51     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 1970     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 77264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00183  |\n",
      "|    n_updates        | 4764     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.507    |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 1969     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 78240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000924 |\n",
      "|    n_updates        | 4825     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.504    |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 1968     |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 78896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000879 |\n",
      "|    n_updates        | 4866     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.5      |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 1968     |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 80032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0191   |\n",
      "|    n_updates        | 4937     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 1968     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 80768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00287  |\n",
      "|    n_updates        | 4983     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.495    |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 1966     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 81424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00171  |\n",
      "|    n_updates        | 5024     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.492    |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 1966     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 82080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00156  |\n",
      "|    n_updates        | 5065     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.489    |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 1965     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 82896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00373  |\n",
      "|    n_updates        | 5116     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.486    |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 1965     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 83792    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00105  |\n",
      "|    n_updates        | 5172     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.483    |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 1965     |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 84528    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.004    |\n",
      "|    n_updates        | 5218     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 1964     |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 85264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 5264     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 1964     |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 85920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 5305     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 1965     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 86656    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000665 |\n",
      "|    n_updates        | 5351     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.472    |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 1964     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 87552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00175  |\n",
      "|    n_updates        | 5407     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.467    |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 1964     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 88848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00895  |\n",
      "|    n_updates        | 5488     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.463    |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 1963     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 89744    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00993  |\n",
      "|    n_updates        | 5544     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.461    |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 1963     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 90400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00978  |\n",
      "|    n_updates        | 5585     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.458    |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 1962     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 91296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00959  |\n",
      "|    n_updates        | 5641     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.455    |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 1962     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 91952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00918  |\n",
      "|    n_updates        | 5682     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 1962     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 92688    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00289  |\n",
      "|    n_updates        | 5728     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.45     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 1962     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 93424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00281  |\n",
      "|    n_updates        | 5774     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.447    |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 1961     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 94160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0015   |\n",
      "|    n_updates        | 5820     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.444    |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 1961     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 94976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000658 |\n",
      "|    n_updates        | 5871     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 1960     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 95712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 5917     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.438    |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 1960     |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 96528    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000485 |\n",
      "|    n_updates        | 5968     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 1960     |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 97424    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0275   |\n",
      "|    n_updates        | 6024     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.432    |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 1959     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 98080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00112  |\n",
      "|    n_updates        | 6065     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.429    |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 1959     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 98976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00064  |\n",
      "|    n_updates        | 6121     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.426    |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 1958     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 99632    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0302   |\n",
      "|    n_updates        | 6162     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.422    |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 1957     |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 100688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00147  |\n",
      "|    n_updates        | 6228     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.419    |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 1957     |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 101504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00894  |\n",
      "|    n_updates        | 6279     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.417    |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 1956     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 102240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00909  |\n",
      "|    n_updates        | 6325     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.414    |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 1957     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 102976   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00404  |\n",
      "|    n_updates        | 6371     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.411    |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 1956     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 103712   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 6417     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.409    |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 1956     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 104368   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0124   |\n",
      "|    n_updates        | 6458     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 1955     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 105104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00314  |\n",
      "|    n_updates        | 6504     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.403    |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 1953     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 105920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0165   |\n",
      "|    n_updates        | 6555     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.4      |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 1953     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 106656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00257  |\n",
      "|    n_updates        | 6601     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.398    |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 1953     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 107312   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0259   |\n",
      "|    n_updates        | 6642     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.394    |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 1953     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 108288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 6703     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.391    |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 1952     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 109184   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 6759     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.388    |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 1951     |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 109840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 6800     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 10.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.384    |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 1951     |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 110976   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00144  |\n",
      "|    n_updates        | 6871     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.381    |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 1951     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 111792   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00268  |\n",
      "|    n_updates        | 6922     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.378    |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 1951     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 112448   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00163  |\n",
      "|    n_updates        | 6963     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 9.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.373    |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 1951     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 113744   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0016   |\n",
      "|    n_updates        | 7044     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.37     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 1951     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 114640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 7100     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.368    |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 1950     |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 115296   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 7141     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.365    |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 1950     |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 115952   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00104  |\n",
      "|    n_updates        | 7182     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.362    |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 1950     |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 116688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 7228     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.36     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 1949     |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 117424   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 7274     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.357    |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 1949     |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 118080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00243  |\n",
      "|    n_updates        | 7315     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 1948     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 118896   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00906  |\n",
      "|    n_updates        | 7366     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.352    |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 1948     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 119552   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00236  |\n",
      "|    n_updates        | 7407     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.349    |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 1948     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 120288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00368  |\n",
      "|    n_updates        | 7453     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.345    |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 1947     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 121264   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 7514     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.343    |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 1947     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 121920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0147   |\n",
      "|    n_updates        | 7555     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 1946     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 122656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00102  |\n",
      "|    n_updates        | 7601     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.338    |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 1946     |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 123312   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00232  |\n",
      "|    n_updates        | 7642     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.334    |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 1946     |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 124208   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0147   |\n",
      "|    n_updates        | 7698     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.332    |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 1946     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 124864   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00208  |\n",
      "|    n_updates        | 7739     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.329    |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 1946     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 125520   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 7780     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.327    |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 126176   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00112  |\n",
      "|    n_updates        | 7821     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.324    |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 126992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00453  |\n",
      "|    n_updates        | 7872     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.321    |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 127648   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 7913     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.319    |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 128384   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00784  |\n",
      "|    n_updates        | 7959     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.315    |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 129280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00283  |\n",
      "|    n_updates        | 8015     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.313    |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 129936   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00221  |\n",
      "|    n_updates        | 8056     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.31     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 130592   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00299  |\n",
      "|    n_updates        | 8097     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.308    |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 131248   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00406  |\n",
      "|    n_updates        | 8138     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.305    |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 131904   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 8179     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.303    |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 1944     |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 132640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00948  |\n",
      "|    n_updates        | 8225     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.298    |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 1944     |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 133776   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00296  |\n",
      "|    n_updates        | 8296     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.296    |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 134432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00733  |\n",
      "|    n_updates        | 8337     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 135408   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00366  |\n",
      "|    n_updates        | 8398     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.289    |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 136144   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0275   |\n",
      "|    n_updates        | 8444     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.287    |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 136880   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00215  |\n",
      "|    n_updates        | 8490     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.284    |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 137616   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 8536     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.281    |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 138432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00986  |\n",
      "|    n_updates        | 8587     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 139168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00158  |\n",
      "|    n_updates        | 8633     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.275    |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 140064   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0172   |\n",
      "|    n_updates        | 8689     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.271    |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 140960   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00253  |\n",
      "|    n_updates        | 8745     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.269    |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 141696   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 8791     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.266    |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 142432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00325  |\n",
      "|    n_updates        | 8837     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.263    |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 1944     |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 143088   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0075   |\n",
      "|    n_updates        | 8878     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.261    |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 1945     |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 143744   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 8919     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.258    |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 1944     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 144480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00509  |\n",
      "|    n_updates        | 8965     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 6.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.255    |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 1944     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 145296   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0215   |\n",
      "|    n_updates        | 9016     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.253    |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 145952   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0199   |\n",
      "|    n_updates        | 9057     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.25     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 146608   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 9098     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.247    |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 147424   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00778  |\n",
      "|    n_updates        | 9149     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.245    |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 148080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0254   |\n",
      "|    n_updates        | 9190     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.242    |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 148816   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00298  |\n",
      "|    n_updates        | 9236     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.239    |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 149472   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 9277     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.236    |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 1942     |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 150288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0064   |\n",
      "|    n_updates        | 9328     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.234    |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 150944   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0266   |\n",
      "|    n_updates        | 9369     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.232    |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 151600   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00918  |\n",
      "|    n_updates        | 9410     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.229    |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 152256   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00439  |\n",
      "|    n_updates        | 9451     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.226    |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 153072   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00408  |\n",
      "|    n_updates        | 9502     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.224    |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 153728   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00815  |\n",
      "|    n_updates        | 9543     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.221    |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 1942     |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 154464   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0265   |\n",
      "|    n_updates        | 9589     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.218    |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 155120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00391  |\n",
      "|    n_updates        | 9630     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.216    |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 155776   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00418  |\n",
      "|    n_updates        | 9671     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.213    |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 156432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 9712     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.211    |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 157088   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 9753     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 157904   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00383  |\n",
      "|    n_updates        | 9804     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.205    |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 158640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 9850     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 1943     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 159456   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0161   |\n",
      "|    n_updates        | 9901     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.199    |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 1942     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 160272   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00675  |\n",
      "|    n_updates        | 9952     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.196    |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 1941     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 161008   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 9998     |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 1941     |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 161824   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00368  |\n",
      "|    n_updates        | 10049    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.191    |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 1940     |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 162480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 10090    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.188    |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 1940     |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 163216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00604  |\n",
      "|    n_updates        | 10136    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.185    |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 1940     |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 163952   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.016    |\n",
      "|    n_updates        | 10182    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.183    |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 164608   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00949  |\n",
      "|    n_updates        | 10223    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.18     |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 165264   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00453  |\n",
      "|    n_updates        | 10264    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.178    |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 165920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00615  |\n",
      "|    n_updates        | 10305    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.175    |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 166656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00692  |\n",
      "|    n_updates        | 10351    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.173    |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 167312   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 10392    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.17     |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 167968   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00165  |\n",
      "|    n_updates        | 10433    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.168    |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 168624   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0188   |\n",
      "|    n_updates        | 10474    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.165    |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 169440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00149  |\n",
      "|    n_updates        | 10525    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.162    |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 170176   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00432  |\n",
      "|    n_updates        | 10571    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.159    |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 170992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 10622    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 12.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.154    |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 172208   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 10698    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.151    |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 1938     |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 172944   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00225  |\n",
      "|    n_updates        | 10744    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.149    |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 173600   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0077   |\n",
      "|    n_updates        | 10785    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.146    |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 174336   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00225  |\n",
      "|    n_updates        | 10831    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.144    |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 174992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00262  |\n",
      "|    n_updates        | 10872    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.141    |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 175648   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000989 |\n",
      "|    n_updates        | 10913    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.138    |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 176464   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00484  |\n",
      "|    n_updates        | 10964    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.136    |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 177120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00617  |\n",
      "|    n_updates        | 11005    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.133    |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 177776   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00258  |\n",
      "|    n_updates        | 11046    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.13     |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 178672   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 11102    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.128    |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 179328   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000998 |\n",
      "|    n_updates        | 11143    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.124    |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 180144   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0143   |\n",
      "|    n_updates        | 11194    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.122    |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 180800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 11235    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.12     |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 181456   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00345  |\n",
      "|    n_updates        | 11276    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.117    |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 182192   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0236   |\n",
      "|    n_updates        | 11322    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.114    |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 182848   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00205  |\n",
      "|    n_updates        | 11363    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.112    |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 183504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00227  |\n",
      "|    n_updates        | 11404    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.109    |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 184160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0228   |\n",
      "|    n_updates        | 11445    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.107    |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 184816   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 11486    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.104    |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 185472   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 11527    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.102    |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 186128   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00387  |\n",
      "|    n_updates        | 11568    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0993   |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 186864   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0051   |\n",
      "|    n_updates        | 11614    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0956   |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 187840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00199  |\n",
      "|    n_updates        | 11675    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0928   |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 188576   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 11721    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0904   |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 1937     |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 189232   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00158  |\n",
      "|    n_updates        | 11762    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 8.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0858   |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 1936     |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 190448   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00851  |\n",
      "|    n_updates        | 11838    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0834   |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 191104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00638  |\n",
      "|    n_updates        | 11879    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0809   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 191760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00994  |\n",
      "|    n_updates        | 11920    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0781   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 1935     |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 192496   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00344  |\n",
      "|    n_updates        | 11966    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0757   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 1934     |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 193152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 12007    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0732   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 193808   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0173   |\n",
      "|    n_updates        | 12048    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0705   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1016     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 194544   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00861  |\n",
      "|    n_updates        | 12094    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0677   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1020     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 195280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0248   |\n",
      "|    n_updates        | 12140    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0652   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1024     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 195936   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00395  |\n",
      "|    n_updates        | 12181    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0625   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1028     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 196672   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00229  |\n",
      "|    n_updates        | 12227    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0594   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1032     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 197488   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00385  |\n",
      "|    n_updates        | 12278    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.057    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1036     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 198144   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0138   |\n",
      "|    n_updates        | 12319    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0545   |\n",
      "| time/               |          |\n",
      "|    episodes         | 1040     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 198800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00485  |\n",
      "|    n_updates        | 12360    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.052    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1044     |\n",
      "|    fps              | 1933     |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 199456   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00475  |\n",
      "|    n_updates        | 12401    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1048     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 200112   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00324  |\n",
      "|    n_updates        | 12442    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1052     |\n",
      "|    fps              | 1931     |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 200768   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00146  |\n",
      "|    n_updates        | 12483    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1056     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 201504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00601  |\n",
      "|    n_updates        | 12529    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1060     |\n",
      "|    fps              | 1931     |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 202240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00236  |\n",
      "|    n_updates        | 12575    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1064     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 202896   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00478  |\n",
      "|    n_updates        | 12616    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1068     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 203552   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00467  |\n",
      "|    n_updates        | 12657    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1072     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 204288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00208  |\n",
      "|    n_updates        | 12703    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1076     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 205024   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00197  |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1080     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 205760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00908  |\n",
      "|    n_updates        | 12795    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1084     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 206416   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00284  |\n",
      "|    n_updates        | 12836    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1088     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 207072   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00207  |\n",
      "|    n_updates        | 12877    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1092     |\n",
      "|    fps              | 1932     |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 207728   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00953  |\n",
      "|    n_updates        | 12918    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1096     |\n",
      "|    fps              | 1931     |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 208464   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000563 |\n",
      "|    n_updates        | 12964    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 1930     |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 209280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000994 |\n",
      "|    n_updates        | 13015    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1104     |\n",
      "|    fps              | 1930     |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 210016   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00261  |\n",
      "|    n_updates        | 13061    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1108     |\n",
      "|    fps              | 1929     |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 210672   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00425  |\n",
      "|    n_updates        | 13102    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1112     |\n",
      "|    fps              | 1928     |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 211328   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00178  |\n",
      "|    n_updates        | 13143    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1116     |\n",
      "|    fps              | 1928     |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 211984   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00044  |\n",
      "|    n_updates        | 13184    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1120     |\n",
      "|    fps              | 1926     |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 212640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0023   |\n",
      "|    n_updates        | 13225    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1124     |\n",
      "|    fps              | 1925     |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 213296   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 13266    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1128     |\n",
      "|    fps              | 1924     |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 214112   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00154  |\n",
      "|    n_updates        | 13317    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1132     |\n",
      "|    fps              | 1924     |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 214848   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00299  |\n",
      "|    n_updates        | 13363    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1136     |\n",
      "|    fps              | 1923     |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 215504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0017   |\n",
      "|    n_updates        | 13404    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1140     |\n",
      "|    fps              | 1922     |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 216240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0166   |\n",
      "|    n_updates        | 13450    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1144     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 216896   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00029  |\n",
      "|    n_updates        | 13491    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1148     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 217712   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00526  |\n",
      "|    n_updates        | 13542    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1152     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 218368   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00215  |\n",
      "|    n_updates        | 13583    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1156     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 219024   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00319  |\n",
      "|    n_updates        | 13624    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1160     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 219840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00675  |\n",
      "|    n_updates        | 13675    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1164     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 220496   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00114  |\n",
      "|    n_updates        | 13716    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1168     |\n",
      "|    fps              | 1921     |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 221152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 13757    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1172     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 221808   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00917  |\n",
      "|    n_updates        | 13798    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1176     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 222544   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 13844    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1180     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 223200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00326  |\n",
      "|    n_updates        | 13885    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1184     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 223856   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00109  |\n",
      "|    n_updates        | 13926    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1188     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 224512   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 13967    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1192     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 225168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000536 |\n",
      "|    n_updates        | 14008    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1196     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 225824   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00247  |\n",
      "|    n_updates        | 14049    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 7.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 226720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00218  |\n",
      "|    n_updates        | 14105    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1204     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 227376   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0048   |\n",
      "|    n_updates        | 14146    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1208     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 228032   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000552 |\n",
      "|    n_updates        | 14187    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1212     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 228688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00217  |\n",
      "|    n_updates        | 14228    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1216     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 229344   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00162  |\n",
      "|    n_updates        | 14269    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1220     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00298  |\n",
      "|    n_updates        | 14310    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 0.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1224     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 230656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00312  |\n",
      "|    n_updates        | 14351    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1228     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 231392   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00164  |\n",
      "|    n_updates        | 14397    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1232     |\n",
      "|    fps              | 1918     |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 232048   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00166  |\n",
      "|    n_updates        | 14438    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1236     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 232704   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00949  |\n",
      "|    n_updates        | 14479    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1240     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 233360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0013   |\n",
      "|    n_updates        | 14520    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1244     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 234016   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000628 |\n",
      "|    n_updates        | 14561    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1248     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 234752   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00151  |\n",
      "|    n_updates        | 14607    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1252     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 235408   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00146  |\n",
      "|    n_updates        | 14648    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1256     |\n",
      "|    fps              | 1919     |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 236064   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000532 |\n",
      "|    n_updates        | 14689    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1260     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 236720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 14730    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1264     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 237376   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00117  |\n",
      "|    n_updates        | 14771    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1268     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 238032   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 14812    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1272     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 238688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00197  |\n",
      "|    n_updates        | 14853    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1276     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 239424   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00166  |\n",
      "|    n_updates        | 14899    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1280     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 240240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0296   |\n",
      "|    n_updates        | 14950    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1284     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 240896   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00701  |\n",
      "|    n_updates        | 14991    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1288     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 241632   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 15037    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1292     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 242288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00156  |\n",
      "|    n_updates        | 15078    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 5.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1296     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 243184   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00332  |\n",
      "|    n_updates        | 15134    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 4.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 243920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00197  |\n",
      "|    n_updates        | 15180    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1304     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 244656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00293  |\n",
      "|    n_updates        | 15226    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1308     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 245312   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0017   |\n",
      "|    n_updates        | 15267    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 3.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1312     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 245968   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 15308    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1316     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 246624   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00651  |\n",
      "|    n_updates        | 15349    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1320     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 247280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0011   |\n",
      "|    n_updates        | 15390    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1324     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 247936   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000458 |\n",
      "|    n_updates        | 15431    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1328     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 248592   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 15472    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 2.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1332     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 249248   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0174   |\n",
      "|    n_updates        | 15513    |\n",
      "----------------------------------\n",
      "Épisode terminé, récompense cumulée : 1.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1336     |\n",
      "|    fps              | 1920     |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 249904   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00142  |\n",
      "|    n_updates        | 15554    |\n",
      "----------------------------------\n",
      "Entraînement terminé. Récompenses cumulées par épisode :\n",
      "[1.0, 6.0, 1.0, 4.0, 3.0, 5.0, 6.0, 1.0, 6.0, 6.0, 0.0, 0.0, 6.0, 4.0, 2.0, 6.0, 1.0, 0.0, 4.0, 5.0, 3.0, 2.0, 4.0, 3.0, 9.0, 3.0, 6.0, 1.0, 9.0, 4.0, 3.0, 6.0, 6.0, 2.0, 6.0, 2.0, 2.0, 3.0, 6.0, 3.0, 5.0, 3.0, 5.0, 2.0, 5.0, 5.0, 5.0, 3.0, 4.0, 3.0, 0.0, 5.0, 7.0, 4.0, 3.0, 2.0, 3.0, 3.0, 6.0, 5.0, 5.0, 3.0, 7.0, 6.0, 6.0, 5.0, 4.0, 4.0, 12.0, 5.0, 8.0, 3.0, 4.0, 3.0, 9.0, 2.0, 3.0, 5.0, 1.0, 7.0, 5.0, 5.0, 3.0, 1.0, 3.0, 6.0, 7.0, 5.0, 5.0, 1.0, 3.0, 3.0, 3.0, 2.0, 2.0, 9.0, 8.0, 2.0, 3.0, 8.0, 3.0, 8.0, 2.0, 1.0, 1.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 7.0, 8.0, 5.0, 2.0, 4.0, 2.0, 4.0, 3.0, 3.0, 4.0, 2.0, 5.0, 8.0, 0.0, 6.0, 2.0, 7.0, 4.0, 3.0, 3.0, 3.0, 2.0, 4.0, 4.0, 3.0, 1.0, 7.0, 4.0, 3.0, 10.0, 3.0, 4.0, 9.0, 5.0, 2.0, 0.0, 4.0, 5.0, 4.0, 6.0, 4.0, 1.0, 7.0, 2.0, 5.0, 1.0, 3.0, 2.0, 2.0, 2.0, 6.0, 2.0, 5.0, 5.0, 6.0, 2.0, 1.0, 3.0, 5.0, 8.0, 3.0, 8.0, 2.0, 2.0, 3.0, 5.0, 3.0, 7.0, 4.0, 3.0, 1.0, 1.0, 2.0, 5.0, 6.0, 2.0, 3.0, 4.0, 3.0, 4.0, 1.0, 3.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 3.0, 4.0, 1.0, 4.0, 4.0, 5.0, 3.0, 4.0, 4.0, 1.0, 3.0, 3.0, 2.0, 2.0, 1.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 5.0, 12.0, 5.0, 1.0, 2.0, 1.0, 0.0, 5.0, 2.0, 2.0, 5.0, 3.0, 3.0, 2.0, 4.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 1.0, 1.0, 5.0, 5.0, 3.0, 8.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 2.0, 4.0, 4.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 1.0, 4.0, 2.0, 1.0, 3.0, 2.0, 3.0, 2.0, 2.0, 1.0, 0.0, 1.0, 3.0, 4.0, 2.0, 3.0, 1.0, 2.0, 4.0, 3.0, 3.0, 5.0, 4.0, 1.0, 1.0, 4.0, 3.0, 3.0, 2.0, 1.0, 1.0, 7.0, 1.0, 1.0, 2.0, 1.0, 3.0, 0.0, 4.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 2.0, 1.0, 3.0, 3.0, 4.0, 4.0, 2.0, 3.0, 1.0, 5.0, 4.0, 1.0, 1.0, 3.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"env = knights_archers_zombies_v10.parallel_env()\\nenv = ss.black_death_v3(env)\\nenv = ss.pettingzoo_env_to_vec_env_v1(env)\\nenv = ss.concat_vec_envs_v1(env, 1, base_class='stable_baselines3')\\nmodel.set_env(env)\\n\\nobs = env.reset()\\nrewards = []\\nfor _ in range(1000):\\n    action, _ = model.predict(obs)\\n    obs, reward, done, _ = env.step(action)\\n    rewards.append(reward)\\n    if done.any():\\n        obs = env.reset()\\n\\n# Affiche les récompenses\\nplt.plot(np.cumsum(rewards))\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env KAZ\n",
    "aec_env = knights_archers_zombies_v10.env()\n",
    "\n",
    "# conversion en parallele\n",
    "parallel_env = aec_to_parallel(aec_env)\n",
    "\n",
    "# wrapper 'black_death_v3' pour les agents morts/inactifs\n",
    "parallel_env = ss.black_death_v3(parallel_env)\n",
    "\n",
    "# SuperSuit pour environnement Gym et stablebaselines 3\n",
    "gym_env = ss.pettingzoo_env_to_vec_env_v1(parallel_env)\n",
    "gym_env = ss.concat_vec_envs_v1(gym_env, 1, base_class='stable_baselines3')\n",
    "\n",
    "\n",
    "# Hyperparams\n",
    "epsilon = 0.8\n",
    "epsilon_decay = 0.98\n",
    "epsilon_min = 0.05\n",
    "gamma = 0.999\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "buffer_capacity = 1024\n",
    "target_update_freq = 1000\n",
    "num_episodes = 1_000_000\n",
    "reward_penalty = 1\n",
    "\n",
    "# init DQN\n",
    "model = DQN('MlpPolicy', \n",
    "            gym_env, \n",
    "            verbose=1,\n",
    "            tensorboard_log=\"./logs/\",\n",
    "            learning_rate=lr,\n",
    "            buffer_size=buffer_capacity,\n",
    "            learning_starts=buffer_capacity,\n",
    "            batch_size=batch_size,\n",
    "            target_update_interval=target_update_freq,\n",
    "            gamma=gamma,\n",
    "            exploration_fraction=epsilon,\n",
    "            exploration_final_eps=epsilon_min,\n",
    "            exploration_initial_eps=epsilon,\n",
    "            )\n",
    "\n",
    "# Initialiser le callback\n",
    "reward_callback = RewardTrackingCallback(verbose=1)\n",
    "\n",
    "# Entraînement avec le callback\n",
    "model.learn(\n",
    "    total_timesteps=250_000,\n",
    "    tb_log_name=\"dqn_knights_archers_zombies_SoloDQN\",\n",
    "    callback=reward_callback\n",
    ")\n",
    "\n",
    "#model.learn(total_timesteps=250_000, tb_log_name=\"dqn_knights_archers_zombies_SoloDQN\")\n",
    "\n",
    "# save modele\n",
    "model.save(\"dqn_knights_archers_zombies\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 DQN indep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation d'un DQN par agents avec Stable Baselines 3. Cette approche à été remplacée par un DQN fait à la main dans la dernière partie car les modification des récompenses est très compliquées avec les wrappers utilisées pour la parallélisation et la gestions des morts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Temps écoulé: 00:00:00 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angeleramauge/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'checkpoints' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 - Temps écoulé: 00:02:18 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 3 - Temps écoulé: 00:04:38 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 4 - Temps écoulé: 00:06:57 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 5 - Temps écoulé: 00:09:17 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 6 - Temps écoulé: 00:11:37 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 7 - Temps écoulé: 00:13:57 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 8 - Temps écoulé: 00:16:17 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 9 - Temps écoulé: 00:18:38 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 10 - Temps écoulé: 00:21:00 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 11 - Temps écoulé: 00:23:21 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 12 - Temps écoulé: 00:25:41 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 13 - Temps écoulé: 00:28:01 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 14 - Temps écoulé: 00:30:20 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 15 - Temps écoulé: 00:32:40 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 16 - Temps écoulé: 00:35:00 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 17 - Temps écoulé: 00:37:21 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 18 - Temps écoulé: 00:39:42 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 19 - Temps écoulé: 00:42:04 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Episode 20 - Temps écoulé: 00:44:25 s\n",
      "Step 0/10000\n",
      "Step 1000/10000\n",
      "Step 2000/10000\n",
      "Step 3000/10000\n",
      "Step 4000/10000\n",
      "Step 5000/10000\n",
      "Step 6000/10000\n",
      "Step 7000/10000\n",
      "Step 8000/10000\n",
      "Step 9000/10000\n",
      "Entraînement terminé !\n"
     ]
    }
   ],
   "source": [
    "# charge env\n",
    "aec_env = knights_archers_zombies_v10.env()\n",
    "\n",
    "# Convertir en parallele\n",
    "parallel_env = aec_to_parallel(aec_env)\n",
    "\n",
    "parallel_env = ss.black_death_v3(parallel_env)\n",
    "\n",
    "# SuperSuit pour convertir en un Gym compatible\n",
    "gym_env = ss.pettingzoo_env_to_vec_env_v1(parallel_env)\n",
    "gym_env = ss.concat_vec_envs_v1(gym_env, 1, base_class='stable_baselines3')\n",
    "\n",
    "# Initialiser les modèles DQN pour chaque agent\n",
    "num_agents = gym_env.num_envs  # Récupérer le nombre d'agents à partir de l'environnement\n",
    "models_archer = [DQN(  \n",
    "                'MlpPolicy', \n",
    "                gym_env, \n",
    "                verbose=0,\n",
    "                learning_rate=0.001,\n",
    "                buffer_size=10000,\n",
    "                batch_size=64,\n",
    "                learning_starts=200,\n",
    "                train_freq=1,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=100,\n",
    "                exploration_fraction=0.3,\n",
    "                exploration_initial_eps=0.05,\n",
    "                exploration_final_eps=0.03,\n",
    "                gamma=0.96\n",
    "\n",
    "            ) for _ in range(2)]\n",
    "\n",
    "models_knight = [DQN(  \n",
    "                'MlpPolicy', \n",
    "                gym_env, \n",
    "                verbose=0,\n",
    "                learning_rate=0.001,\n",
    "                buffer_size=10000,\n",
    "                batch_size=64,\n",
    "                learning_starts=200,\n",
    "                train_freq=1,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=100,\n",
    "                exploration_fraction=0.3,\n",
    "                exploration_initial_eps=0.05,\n",
    "                exploration_final_eps=0.03,\n",
    "                gamma=0.96\n",
    "            ) for _ in range(2)]\n",
    "\n",
    "models = models_archer + models_knight\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "# Entraîner les agents\n",
    "timesteps = 10_000\n",
    "nb_episodes = 20\n",
    "for episode in range(nb_episodes):  # Nombre d'épisodes d'entraînement\n",
    "    t = time.time() - start_t\n",
    "    hours, remainder = divmod(t, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Episode {episode + 1} - Temps écoulé: {int(hours):02}:{int(minutes):02}:{int(seconds):02} s\")\n",
    "    obs = gym_env.reset()\n",
    "    done = np.array([False] * num_agents)\n",
    "\n",
    "    for step in range(timesteps):\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step}/{timesteps}\")\n",
    "        actions = np.array([models[i].predict(obs[i])[0] for i in range(num_agents)])  # Prédire les actions pour chaque agent\n",
    "        obs, rewards_batch, done, _ = gym_env.step(actions)  # Appliquer les actions dans l'environnement\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            models[i].learn(total_timesteps=1)  # Mettre à jour chaque agent après chaque étape\n",
    "        \n",
    "        if done.all():\n",
    "            obs = gym_env.reset()\n",
    "            done = np.array([False] * num_agents)\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        models[i].learn(total_timesteps=100)\n",
    "\n",
    "    # Sauvegarder les modèles\n",
    "    for i in range(num_agents):\n",
    "        models[i].save(f\"./checkpoints/dqn_agent_{i + 1}_knights_archers_zombies_{episode}\")\n",
    "\n",
    "# Sauvegarder les modèles\n",
    "for i in range(num_agents):\n",
    "    models[i].save(f\"dqn_agent_{i + 1}_knights_archers_zombies\")\n",
    "\n",
    "print(\"Entraînement terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation d'un DQN à la main pour faciliter la modification des récompenses et de l'environnement (type des agents, récompenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config du réseau Q pour chaque agent\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)  # Adjust input_dim to match actual observation dimension\n",
    "        self.fc2 = nn.Linear(64, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# Fonction de selection d'action avec e-greedy\n",
    "def select_action(state, q_net, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, action_dim - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state.flatten())  \n",
    "            q_values = q_net(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "    return action\n",
    "\n",
    "# Buffer de replay pour stocker les expériences des agents\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 243/250000 (0.097%) - Rewards: (-2.8000000000000007, -1.2000000000000002, -0.10000000000000031, -0.30000000000000004)\n",
      "Steps 406/250000 (0.162%) - Rewards: (-4.440892098500626e-16, -1.2000000000000004, -2.700000000000001, -0.9500000000000002)\n",
      "Steps 709/250000 (0.284%) - Rewards: (-0.8500000000000005, -1.2499999999999998, 0.9500000000000004, -1.8500000000000005)\n",
      "Steps 872/250000 (0.349%) - Rewards: (-2.1500000000000004, -1.0499999999999998, -1.5, -2.1500000000000004)\n",
      "Steps 1055/250000 (0.422%) - Rewards: (-0.6, -2.2500000000000004, -0.25000000000000044, -1.8000000000000003)\n",
      "Steps 1478/250000 (0.591%) - Rewards: (-1.0000000000000004, -0.6000000000000014, -0.20000000000000084, -0.10000000000000053)\n",
      "Steps 1681/250000 (0.672%) - Rewards: (-0.6500000000000004, -1.9000000000000004, -0.6500000000000001, -2.500000000000001)\n",
      "Steps 2004/250000 (0.802%) - Rewards: (-1.950000000000001, -2.650000000000001, -0.8000000000000012, -1.150000000000001)\n",
      "Steps 2347/250000 (0.939%) - Rewards: (-3.55, -0.05000000000000071, -0.2000000000000013, -2.7500000000000013)\n",
      "Steps 2530/250000 (1.012%) - Rewards: (-2.8000000000000025, -2.3000000000000007, 0.25, -1.7500000000000009)\n",
      "Steps 2813/250000 (1.125%) - Rewards: (-1.3000000000000012, -3.5999999999999988, -2.550000000000001, -1.3500000000000019)\n",
      "Steps 3056/250000 (1.222%) - Rewards: (-1.3000000000000005, -1.7500000000000016, -2.6000000000000014, -1.450000000000001)\n",
      "Steps 3299/250000 (1.32%) - Rewards: (-0.7000000000000004, -3.1, -1.6500000000000008, -5.3499999999999925)\n",
      "Steps 3482/250000 (1.393%) - Rewards: (-3.2, -2.3500000000000005, -3.000000000000001, -4.049999999999997)\n",
      "Steps 3982/250000 (1.593%) - Rewards: (-1.2500000000000004, 1.200000000000001, -3.649999999999989, -1.549999999999992)\n",
      "Steps 4165/250000 (1.666%) - Rewards: (-2.1000000000000014, -1.1, -1.350000000000001, -5.3499999999999925)\n",
      "Steps 4388/250000 (1.755%) - Rewards: (-2.4500000000000015, 1.3500000000000005, -3.000000000000001, -3.2)\n",
      "Steps 4751/250000 (1.9%) - Rewards: (-2.750000000000001, 0.6999999999999997, -4.199999999999993, -1.0000000000000004)\n",
      "Steps 5234/250000 (2.094%) - Rewards: (-2.7999999999999945, 1.95, -3.699999999999999, -5.449999999999986)\n",
      "Steps 5734/250000 (2.294%) - Rewards: (-0.6500000000000012, 2.799999999999999, -0.550000000000001, -5.799999999999983)\n",
      "Steps 5977/250000 (2.391%) - Rewards: (-2.3500000000000005, 1.6000000000000014, -5.499999999999992, -3.349999999999996)\n",
      "Steps 6340/250000 (2.536%) - Rewards: (-2.650000000000001, 0.09999999999999964, -6.349999999999983, -0.05000000000000093)\n",
      "Steps 6523/250000 (2.609%) - Rewards: (-0.8999999999999999, 0.14999999999999925, -3.5499999999999954, -3.8999999999999977)\n",
      "Steps 7023/250000 (2.809%) - Rewards: (1.900000000000004, -0.05000000000000032, -6.049999999999972, -6.049999999999972)\n",
      "Steps 7206/250000 (2.882%) - Rewards: (-1.0, -2.500000000000001, -4.649999999999996, -3.7499999999999982)\n",
      "Steps 7409/250000 (2.964%) - Rewards: (-0.9999999999999999, -5.299999999999993, -3.5499999999999976, -3.8499999999999908)\n",
      "Steps 7612/250000 (3.045%) - Rewards: (-0.2000000000000004, -3.1999999999999993, -2.2000000000000006, -2.5500000000000007)\n",
      "Steps 7775/250000 (3.11%) - Rewards: (0.6499999999999999, -2.0, -4.399999999999996, -1.4500000000000002)\n",
      "Steps 8275/250000 (3.31%) - Rewards: (2.000000000000001, 0.6999999999999994, -6.899999999999969, -0.35000000000000075)\n",
      "Steps 8458/250000 (3.383%) - Rewards: (-1.1500000000000004, -2.1500000000000004, -5.3999999999999915, -3.6499999999999986)\n",
      "Steps 8661/250000 (3.464%) - Rewards: (0.050000000000000044, -3.0500000000000007, -3.2, -2.5499999999999994)\n",
      "Steps 8944/250000 (3.578%) - Rewards: (-1.1500000000000001, -6.249999999999986, -5.0499999999999945, -6.249999999999989)\n",
      "Steps 9307/250000 (3.723%) - Rewards: (2.0000000000000013, -9.700000000000003, -6.34999999999999, -5.349999999999988)\n",
      "Steps 9510/250000 (3.804%) - Rewards: (-1.7000000000000002, -5.499999999999992, -5.949999999999989, -1.7500000000000013)\n",
      "Steps 9673/250000 (3.869%) - Rewards: (-1.45, -5.449999999999992, -5.249999999999993, -2.6000000000000014)\n",
      "Steps 9936/250000 (3.974%) - Rewards: (-0.8999999999999999, -3.3999999999999995, -9.14999999999998, -4.449999999999994)\n",
      "Steps 10159/250000 (4.064%) - Rewards: (0.3500000000000001, -3.3999999999999995, -2.850000000000002, -6.149999999999986)\n",
      "Steps 10342/250000 (4.137%) - Rewards: (-0.8000000000000002, -1.3500000000000005, -2.0000000000000013, -6.649999999999988)\n",
      "Steps 10545/250000 (4.218%) - Rewards: (-1.6, -3.000000000000001, 0.19999999999999907, -6.449999999999989)\n",
      "Steps 10708/250000 (4.283%) - Rewards: (-2.8000000000000007, -2.3000000000000007, -0.7000000000000002, -4.949999999999994)\n",
      "Steps 10951/250000 (4.38%) - Rewards: (-5.849999999999991, -1.9000000000000004, -4.399999999999996, -3.8499999999999908)\n",
      "Steps 11134/250000 (4.454%) - Rewards: (-3.7499999999999947, -2.200000000000001, -3.7499999999999982, -3.6499999999999986)\n",
      "Steps 11337/250000 (4.535%) - Rewards: (-0.7000000000000001, -6.599999999999988, -3.3500000000000005, -5.149999999999993)\n",
      "Steps 11660/250000 (4.664%) - Rewards: (-4.4999999999999964, -8.549999999999981, -5.699999999999984, -4.549999999999994)\n",
      "Steps 11843/250000 (4.737%) - Rewards: (-1.85, -7.8999999999999835, -4.449999999999992, -5.799999999999991)\n",
      "Steps 12006/250000 (4.802%) - Rewards: (-1.35, -4.699999999999995, -3.2999999999999976, -5.099999999999993)\n",
      "Steps 12169/250000 (4.868%) - Rewards: (-0.6000000000000001, -3.8999999999999977, -3.5499999999999936, -5.399999999999992)\n",
      "Steps 12472/250000 (4.989%) - Rewards: (-1.1000000000000005, -4.549999999999992, -5.449999999999989, -3.8499999999999988)\n",
      "Steps 12635/250000 (5.054%) - Rewards: (-3.1000000000000014, -3.5999999999999988, -2.3000000000000016, -3.3499999999999996)\n",
      "Steps 12878/250000 (5.151%) - Rewards: (-1.8000000000000018, -4.299999999999993, -4.749999999999991, -3.999999999999994)\n",
      "Steps 13181/250000 (5.272%) - Rewards: (-1.4000000000000004, -7.399999999999985, -10.399999999999999, -1.7000000000000008)\n",
      "Steps 13364/250000 (5.346%) - Rewards: (-1.75, -5.849999999999991, -7.099999999999986, -3.8499999999999965)\n",
      "Steps 13527/250000 (5.411%) - Rewards: (-1.3, -4.649999999999995, -5.04999999999999, -4.049999999999997)\n",
      "Steps 13690/250000 (5.476%) - Rewards: (-0.30000000000000016, -2.950000000000001, -4.449999999999992, -4.499999999999996)\n",
      "Steps 13853/250000 (5.541%) - Rewards: (-0.4, -4.399999999999996, -4.2499999999999964, -6.04999999999999)\n",
      "Steps 14016/250000 (5.606%) - Rewards: (-3.1000000000000005, -2.950000000000001, -5.299999999999993, -3.849999999999998)\n",
      "Steps 14179/250000 (5.672%) - Rewards: (-6.749999999999988, -2.0500000000000003, -3.3000000000000007, -3.9499999999999975)\n",
      "Steps 14362/250000 (5.745%) - Rewards: (-4.549999999999992, -2.9000000000000012, -4.149999999999997, -4.499999999999996)\n",
      "Steps 14545/250000 (5.818%) - Rewards: (-3.9499999999999975, -2.450000000000001, -5.249999999999989, -4.049999999999997)\n",
      "Steps 14708/250000 (5.883%) - Rewards: (-3.149999999999997, -4.199999999999997, -2.650000000000001, -3.1999999999999966)\n",
      "Steps 14891/250000 (5.956%) - Rewards: (-1.85, -3.7499999999999982, -5.149999999999993, -4.649999999999995)\n",
      "Steps 15154/250000 (6.062%) - Rewards: (-5.549999999999989, -6.749999999999988, -8.84999999999998, -7.849999999999984)\n",
      "Steps 15357/250000 (6.143%) - Rewards: (-5.049999999999994, -4.949999999999994, -6.399999999999989, -5.049999999999994)\n",
      "Steps 15520/250000 (6.208%) - Rewards: (-3.799999999999998, -3.0500000000000016, -4.199999999999997, -4.599999999999995)\n",
      "Steps 15683/250000 (6.273%) - Rewards: (-5.549999999999988, -3.9499999999999975, -2.3000000000000003, -4.199999999999997)\n",
      "Steps 15866/250000 (6.346%) - Rewards: (-4.699999999999995, -4.199999999999997, -4.499999999999996, -7.549999999999985)\n",
      "Steps 16029/250000 (6.412%) - Rewards: (-5.6499999999999915, -5.099999999999993, -2.1500000000000004, -4.749999999999995)\n",
      "Steps 16232/250000 (6.493%) - Rewards: (-3.3499999999999996, -5.849999999999991, -4.099999999999998, -7.249999999999986)\n",
      "Steps 16415/250000 (6.566%) - Rewards: (-6.14999999999999, -4.599999999999995, -4.899999999999994, -5.699999999999991)\n",
      "Steps 16578/250000 (6.631%) - Rewards: (-2.550000000000001, -3.25, -4.999999999999994, -2.5500000000000016)\n",
      "Steps 16761/250000 (6.704%) - Rewards: (-3.9499999999999904, -3.6499999999999986, -4.699999999999995, -3.499999999999999)\n",
      "Steps 17004/250000 (6.802%) - Rewards: (-5.549999999999988, -3.3, -7.549999999999985, -2.799999999999999)\n",
      "Steps 17267/250000 (6.907%) - Rewards: (-7.84999999999998, -4.599999999999995, -4.8999999999999835, -3.6499999999999995)\n",
      "Steps 17510/250000 (7.004%) - Rewards: (-3.599999999999995, -5.749999999999988, -4.799999999999991, -6.599999999999988)\n",
      "Steps 17693/250000 (7.077%) - Rewards: (-3.1500000000000004, -3.9499999999999975, -2.200000000000001, -3.6999999999999984)\n",
      "Steps 17856/250000 (7.142%) - Rewards: (-2.350000000000001, -4.199999999999997, -1.4000000000000004, -3.25)\n",
      "Steps 18039/250000 (7.216%) - Rewards: (-6.14999999999999, -4.299999999999996, -2.9000000000000012, -2.100000000000001)\n",
      "Steps 18302/250000 (7.321%) - Rewards: (-3.849999999999998, -6.649999999999987, -6.449999999999989, -7.549999999999985)\n",
      "Steps 18485/250000 (7.394%) - Rewards: (-3.7499999999999982, -3.1500000000000012, -3.7499999999999982, -3.5999999999999988)\n",
      "Steps 18688/250000 (7.475%) - Rewards: (-6.449999999999989, -4.899999999999994, -4.2499999999999964, -3.799999999999998)\n",
      "Steps 18871/250000 (7.548%) - Rewards: (-7.299999999999986, -4.899999999999994, -3.5999999999999988, -3.1500000000000004)\n",
      "Steps 19054/250000 (7.622%) - Rewards: (-3.8499999999999908, -4.699999999999995, -4.199999999999997, -2.300000000000001)\n",
      "Steps 19217/250000 (7.687%) - Rewards: (-7.149999999999986, -4.2499999999999964, -4.549999999999995, -1.5500000000000007)\n",
      "Steps 19380/250000 (7.752%) - Rewards: (-7.449999999999985, -3.5999999999999988, -1.7000000000000008, -2.0500000000000003)\n",
      "Steps 19623/250000 (7.849%) - Rewards: (-9.999999999999993, -6.799999999999987, -5.3499999999999925, -3.2499999999999933)\n",
      "Steps 19786/250000 (7.914%) - Rewards: (-5.599999999999992, -4.2499999999999964, -2.6000000000000005, -3.0500000000000007)\n",
      "Steps 19949/250000 (7.98%) - Rewards: (-6.449999999999989, -1.7500000000000004, -2.700000000000001, -3.849999999999998)\n",
      "Steps 20132/250000 (8.053%) - Rewards: (-4.849999999999994, -2.4499999999999993, -2.850000000000001, -1.6000000000000003)\n",
      "Steps 20295/250000 (8.118%) - Rewards: (-4.599999999999995, -3.2, -3.1500000000000004, -2.2)\n",
      "Steps 20458/250000 (8.183%) - Rewards: (-6.14999999999999, -5.199999999999993, -3.499999999999999, -2.3000000000000007)\n",
      "Steps 20621/250000 (8.248%) - Rewards: (-3.0500000000000016, -4.299999999999996, -1.3000000000000003, -2.0)\n",
      "Steps 20784/250000 (8.314%) - Rewards: (-2.4000000000000004, -3.0500000000000007, -2.1000000000000005, -2.1000000000000005)\n",
      "Steps 20987/250000 (8.395%) - Rewards: (-2.550000000000001, -3.3499999999999996, -2.750000000000001, -2.500000000000001)\n",
      "Steps 21190/250000 (8.476%) - Rewards: (-4.349999999999996, -3.6999999999999984, -2.500000000000001, -0.44999999999999996)\n",
      "Steps 21393/250000 (8.557%) - Rewards: (-3.599999999999999, -1.85, -1.85, -2.0500000000000003)\n",
      "Steps 21556/250000 (8.622%) - Rewards: (-4.049999999999997, -1.1000000000000003, -2.0500000000000003, -0.7500000000000001)\n",
      "Steps 21779/250000 (8.712%) - Rewards: (-3.499999999999999, -3.549999999999999, -2.950000000000001, -0.9500000000000003)\n",
      "Steps 21962/250000 (8.785%) - Rewards: (-1.9000000000000004, -1.8000000000000003, -2.2500000000000004, -1.7000000000000002)\n",
      "Steps 22125/250000 (8.85%) - Rewards: (-2.2500000000000018, -2.3500000000000005, -2.3500000000000005, -1.75)\n",
      "Steps 22348/250000 (8.939%) - Rewards: (-0.950000000000001, -4.549999999999995, -1.4000000000000001, -1.0)\n",
      "Steps 22571/250000 (9.028%) - Rewards: (-1.4500000000000008, -3.1500000000000004, -1.9500000000000002, -0.6499999999999999)\n",
      "Steps 22774/250000 (9.11%) - Rewards: (-3.3999999999999995, -2.3000000000000016, -1.9500000000000002, -0.55)\n",
      "Steps 22937/250000 (9.175%) - Rewards: (-4.2499999999999964, -3.9999999999999973, -2.1000000000000005, -1.5499999999999998)\n",
      "Steps 23100/250000 (9.24%) - Rewards: (-4.449999999999996, -2.3500000000000005, -1.4, -1.5)\n",
      "Steps 23283/250000 (9.313%) - Rewards: (-4.849999999999994, -1.5500000000000007, -0.44999999999999996, -1.6)\n",
      "Steps 23446/250000 (9.378%) - Rewards: (-5.449999999999992, -2.2, -1.45, -1.3)\n",
      "Steps 23609/250000 (9.444%) - Rewards: (-6.849999999999987, -2.500000000000001, -1.35, -1.5)\n",
      "Steps 23772/250000 (9.509%) - Rewards: (-6.599999999999988, -2.3500000000000005, -0.6, -1.5)\n",
      "Steps 23935/250000 (9.574%) - Rewards: (-5.049999999999994, -2.9000000000000012, -1.35, -1.5)\n",
      "Steps 24098/250000 (9.639%) - Rewards: (-6.14999999999999, -2.750000000000001, -1.5499999999999998, -0.9500000000000003)\n",
      "Steps 24281/250000 (9.712%) - Rewards: (-5.749999999999991, -3.3, -1.6, -1.0500000000000003)\n",
      "Steps 24444/250000 (9.778%) - Rewards: (-5.399999999999992, -2.3500000000000005, -1.5499999999999998, -1.3)\n",
      "Steps 24607/250000 (9.843%) - Rewards: (-5.499999999999992, -2.2, -1.1500000000000004, -0.6499999999999999)\n",
      "Steps 24810/250000 (9.924%) - Rewards: (-4.699999999999995, -0.7000000000000008, -2.700000000000001, -1.5499999999999998)\n",
      "Steps 25013/250000 (10.005%) - Rewards: (-5.899999999999991, -1.7000000000000002, -2.0, 0.7)\n",
      "Steps 25176/250000 (10.07%) - Rewards: (-3.499999999999996, -1.2, -1.5499999999999998, -1.3)\n",
      "Steps 25339/250000 (10.136%) - Rewards: (-3.4499999999999993, -1.5499999999999998, -1.65, -1.25)\n",
      "Steps 25502/250000 (10.201%) - Rewards: (-1.4000000000000001, -1.8000000000000003, -0.7500000000000001, -0.7000000000000002)\n",
      "Steps 25685/250000 (10.274%) - Rewards: (-2.8000000000000007, -1.4, -0.7, -0.55)\n",
      "Steps 25908/250000 (10.363%) - Rewards: (-0.3500000000000014, -0.6499999999999999, -2.0500000000000003, -2.1500000000000004)\n",
      "Steps 26071/250000 (10.428%) - Rewards: (0.5500000000000005, -1.3, -1.5499999999999998, -2.700000000000001)\n",
      "Steps 26254/250000 (10.502%) - Rewards: (-2.900000000000002, -1.45, -1.25, -0.7499999999999999)\n",
      "Steps 26437/250000 (10.575%) - Rewards: (-2.5000000000000013, -1.45, -1.4, -2.0)\n",
      "Steps 26600/250000 (10.64%) - Rewards: (-3.799999999999998, -1.5, -1.6, -1.6)\n",
      "Steps 26783/250000 (10.713%) - Rewards: (-2.3000000000000007, -0.5, -1.6, -1.0)\n",
      "Steps 26966/250000 (10.786%) - Rewards: (-3.7499999999999982, -1.35, -1.1500000000000004, -1.0500000000000003)\n",
      "Steps 27129/250000 (10.852%) - Rewards: (-2.6000000000000005, -0.30000000000000004, -0.75, -3.1500000000000004)\n",
      "Steps 27292/250000 (10.917%) - Rewards: (-1.2500000000000004, -1.3, -1.05, -3.5999999999999988)\n",
      "Steps 27455/250000 (10.982%) - Rewards: (-2.0500000000000003, -0.3500000000000001, -1.2, -3.499999999999999)\n",
      "Steps 27618/250000 (11.047%) - Rewards: (-1.5499999999999998, -0.35, -1.35, -3.1500000000000004)\n",
      "Steps 27781/250000 (11.112%) - Rewards: (-2.0, -1.7000000000000002, -1.6, -5.749999999999991)\n",
      "Steps 27944/250000 (11.178%) - Rewards: (-2.8000000000000007, -1.1, -1.05, -1.4000000000000004)\n",
      "Steps 28127/250000 (11.251%) - Rewards: (-0.9500000000000001, -1.35, -1.15, -1.05)\n",
      "Steps 28290/250000 (11.316%) - Rewards: (-3.25, -1.3, -1.05, -1.45)\n",
      "Steps 28493/250000 (11.397%) - Rewards: (-2.2500000000000018, -1.3, -1.15, -1.1)\n",
      "Steps 28656/250000 (11.462%) - Rewards: (-2.599999999999999, 0.7, -1.45, -1.5)\n",
      "Steps 28819/250000 (11.528%) - Rewards: (-2.600000000000001, -1.2, -1.5, -1.4)\n",
      "Steps 29002/250000 (11.601%) - Rewards: (-2.6000000000000005, -1.2, -1.15, -0.30000000000000016)\n",
      "Steps 29165/250000 (11.666%) - Rewards: (-1.5499999999999998, -1.4, -0.65, -0.8499999999999999)\n",
      "Steps 29368/250000 (11.747%) - Rewards: (0.44999999999999973, -0.5500000000000003, -0.20000000000000007, -2.000000000000001)\n",
      "Steps 29531/250000 (11.812%) - Rewards: (-1.9500000000000006, -0.25, -1.25, -0.2500000000000001)\n",
      "Steps 29754/250000 (11.902%) - Rewards: (-0.5500000000000004, -1.35, -0.2500000000000002, -0.20000000000000018)\n",
      "Steps 29937/250000 (11.975%) - Rewards: (-0.8999999999999999, -1.3, -1.75, -1.3)\n",
      "Steps 30160/250000 (12.064%) - Rewards: (-0.7500000000000002, -0.09999999999999998, -0.20000000000000007, -1.25)\n",
      "Steps 30343/250000 (12.137%) - Rewards: (-1.6500000000000004, -1.3, -1.5, -0.3500000000000002)\n",
      "Steps 30506/250000 (12.202%) - Rewards: (-0.8, -1.2, -2.3000000000000016, -1.1)\n",
      "Steps 30669/250000 (12.268%) - Rewards: (-1.6, -1.2, -1.9000000000000004, -1.1)\n",
      "Steps 30832/250000 (12.333%) - Rewards: (-1.0000000000000002, -1.25, -1.65, -0.20000000000000007)\n",
      "Steps 30995/250000 (12.398%) - Rewards: (-1.65, -1.4, -1.8000000000000003, -1.1000000000000003)\n",
      "Steps 31158/250000 (12.463%) - Rewards: (-2.3500000000000005, -0.30000000000000004, -1.25, -1.3)\n",
      "Steps 31341/250000 (12.536%) - Rewards: (-1.9000000000000004, -1.2, -0.6000000000000003, -1.9000000000000004)\n",
      "Steps 31504/250000 (12.602%) - Rewards: (-1.9500000000000002, -1.25, -1.1, -0.5499999999999999)\n",
      "Steps 31667/250000 (12.667%) - Rewards: (-2.1500000000000004, -1.15, -1.1, -1.1500000000000001)\n",
      "Steps 31870/250000 (12.748%) - Rewards: (-2.850000000000001, -1.5, -1.4, -0.5000000000000002)\n",
      "Steps 32033/250000 (12.813%) - Rewards: (-3.4499999999999993, -2.2, -1.05, -1.2)\n",
      "Steps 32196/250000 (12.878%) - Rewards: (-0.5000000000000002, -1.05, -1, -1.1)\n",
      "Steps 32359/250000 (12.944%) - Rewards: (-1.3, -0.3500000000000001, -0.40000000000000013, -1.05)\n",
      "Steps 32522/250000 (13.009%) - Rewards: (-3.3999999999999995, -1.3, -1.75, -1.05)\n",
      "Steps 32685/250000 (13.074%) - Rewards: (-1, -1.1, -2.2500000000000004, -0.19999999999999996)\n",
      "Steps 32868/250000 (13.147%) - Rewards: (-1.05, -1.25, -2.0, -0.15000000000000013)\n",
      "Steps 33111/250000 (13.244%) - Rewards: (-1.25, 1.2999999999999998, -1.7000000000000002, -1.3)\n",
      "Steps 33354/250000 (13.342%) - Rewards: (-1.25, -0.4500000000000002, -2.7500000000000018, -1.3)\n",
      "Steps 33517/250000 (13.407%) - Rewards: (-0.19999999999999996, -1.5, -1.75, -1.35)\n",
      "Steps 33700/250000 (13.48%) - Rewards: (-1.65, -1.2, -1.15, -1.15)\n",
      "Steps 33863/250000 (13.545%) - Rewards: (-0.5, -1.25, -0.5000000000000002, -0.15000000000000002)\n",
      "Steps 34066/250000 (13.626%) - Rewards: (0.7999999999999996, -1.2, -1.35, -1.1)\n",
      "Steps 34229/250000 (13.692%) - Rewards: (-1.4, -1.35, -1.25, -1.15)\n",
      "Steps 34392/250000 (13.757%) - Rewards: (-0.6, -1.15, -1.05, -1)\n",
      "Steps 34555/250000 (13.822%) - Rewards: (-0.30000000000000004, -1, -1.1, -0.050000000000000044)\n",
      "Steps 34758/250000 (13.903%) - Rewards: (-0.8499999999999999, -0.6000000000000003, -1, -0.10000000000000009)\n",
      "Steps 34921/250000 (13.968%) - Rewards: (-1.3, -1.05, -1, -0.2500000000000001)\n",
      "Steps 35084/250000 (14.034%) - Rewards: (-1.15, -1, -0.050000000000000044, -0.10000000000000009)\n",
      "Steps 35247/250000 (14.099%) - Rewards: (-1.05, -1.05, -1, -1.5499999999999998)\n",
      "Steps 35410/250000 (14.164%) - Rewards: (-1.05, -1.1, -1, -1.05)\n",
      "Steps 35573/250000 (14.229%) - Rewards: (-0.09999999999999998, -0.20000000000000018, -1, -1.1)\n",
      "Steps 35736/250000 (14.294%) - Rewards: (-1.2, -1.9000000000000004, -1.65, -1.05)\n",
      "Steps 35899/250000 (14.36%) - Rewards: (-1.25, -1, -1.05, -1.15)\n",
      "Steps 36062/250000 (14.425%) - Rewards: (-1.3, -1, -1, -1.2)\n",
      "Steps 36225/250000 (14.49%) - Rewards: (-1.3, -1.15, -0.09999999999999998, -0.30000000000000004)\n",
      "Steps 36388/250000 (14.555%) - Rewards: (-0.15000000000000002, -1, -1.1, -0.050000000000000044)\n",
      "Steps 36551/250000 (14.62%) - Rewards: (-1.15, -1.2, -0.30000000000000004, -1.7000000000000002)\n",
      "Steps 36714/250000 (14.686%) - Rewards: (-1, -1.1, -1.6, -1.1)\n",
      "Steps 36897/250000 (14.759%) - Rewards: (-1, -2.2, -0.20000000000000007, -0.3500000000000001)\n",
      "Steps 37060/250000 (14.824%) - Rewards: (-1.05, -2.0, -1.9000000000000004, -1.05)\n",
      "Steps 37223/250000 (14.889%) - Rewards: (-1.1, -1.85, -1.7000000000000002, -1)\n",
      "Steps 37426/250000 (14.97%) - Rewards: (-0.09999999999999998, -1.850000000000001, -1.05, -1)\n",
      "Steps 37609/250000 (15.044%) - Rewards: (-1.05, -2.0000000000000004, -1.9000000000000004, -1.2)\n",
      "Steps 37772/250000 (15.109%) - Rewards: (-1.1, -1.2, -2.750000000000001, -1.9500000000000002)\n",
      "Steps 37975/250000 (15.19%) - Rewards: (-1.2, 0.6999999999999997, -1.45, -1.9500000000000002)\n",
      "Steps 38138/250000 (15.255%) - Rewards: (-0.19999999999999996, -1, -2.3000000000000007, -1.1)\n",
      "Steps 38301/250000 (15.32%) - Rewards: (-1, -1.05, 0.8499999999999999, -1.35)\n",
      "Steps 38464/250000 (15.386%) - Rewards: (-1.15, -1.1, -1.15, -1.25)\n",
      "Steps 38627/250000 (15.451%) - Rewards: (-1.05, -1.05, -1.7000000000000002, -1.15)\n",
      "Steps 38790/250000 (15.516%) - Rewards: (-1.1, -1.05, -1.2, -1.1)\n",
      "Steps 38953/250000 (15.581%) - Rewards: (-1.05, -1, -1.05, -1.05)\n",
      "Steps 39116/250000 (15.646%) - Rewards: (-0.09999999999999998, -1, -1, -1.05)\n",
      "Steps 39319/250000 (15.728%) - Rewards: (-1, -1.05, -0.10000000000000009, -1)\n",
      "Steps 39482/250000 (15.793%) - Rewards: (-1.05, -1.1, -1, -1)\n",
      "Steps 39645/250000 (15.858%) - Rewards: (-1.1, -1.1, -1.05, -1)\n",
      "Steps 39808/250000 (15.923%) - Rewards: (-1, -4.649999999999995, -1.6, -1.1)\n",
      "Steps 40011/250000 (16.004%) - Rewards: (-0.15000000000000002, -0.050000000000000044, -0.10000000000000009, -1)\n",
      "Steps 40174/250000 (16.07%) - Rewards: (-1.15, -2.1500000000000004, -1, -1.05)\n",
      "Steps 40377/250000 (16.151%) - Rewards: (-0.050000000000000044, -0.2500000000000002, -1.15, -1.5000000000000002)\n",
      "Steps 40540/250000 (16.216%) - Rewards: (-1.15, -1, -1.45, -1.05)\n",
      "Steps 40703/250000 (16.281%) - Rewards: (-1.1, -1.1, -2.2500000000000004, -1.3)\n",
      "Steps 40866/250000 (16.346%) - Rewards: (-1.1, -1.1, -1.05, -0.050000000000000044)\n",
      "Steps 41029/250000 (16.412%) - Rewards: (-1.05, -1.05, -2.2, -1.2)\n",
      "Steps 41192/250000 (16.477%) - Rewards: (-1.05, -0.30000000000000004, -1.15, -1.15)\n",
      "Steps 41355/250000 (16.542%) - Rewards: (-1.05, -1.5, -1.3, -1)\n",
      "Steps 41538/250000 (16.615%) - Rewards: (-1.2, -1.05, -0.15000000000000002, -1)\n",
      "Steps 41701/250000 (16.68%) - Rewards: (-1.05, -1.1, -1.1, -1.15)\n",
      "Steps 41864/250000 (16.746%) - Rewards: (-1.2, -1.05, -1, -1.35)\n",
      "Steps 42027/250000 (16.811%) - Rewards: (-1.05, -1.7000000000000002, -1.05, -1.05)\n",
      "Steps 42190/250000 (16.876%) - Rewards: (-1, -3.849999999999998, -1.15, -1)\n",
      "Steps 42373/250000 (16.949%) - Rewards: (-1.1, -1.35, -1.05, -0.40000000000000013)\n",
      "Steps 42576/250000 (17.03%) - Rewards: (-1.4, -0.2500000000000001, -0.09999999999999998, -1.05)\n",
      "Steps 42739/250000 (17.096%) - Rewards: (-1, -1.15, -1.15, -1.05)\n",
      "Steps 42902/250000 (17.161%) - Rewards: (-1, -1.1, -1.05, -1)\n",
      "Steps 43125/250000 (17.25%) - Rewards: (-1.1, -1.2, -0.050000000000000044, 0.8999999999999999)\n",
      "Steps 43308/250000 (17.323%) - Rewards: (-1.15, -0.19999999999999996, -1, -1)\n",
      "Steps 43471/250000 (17.388%) - Rewards: (-1.1, -1.05, -1.3, -1)\n",
      "Steps 43674/250000 (17.47%) - Rewards: (-1, -0.4, -1.05, -0.050000000000000044)\n",
      "Steps 43837/250000 (17.535%) - Rewards: (-0.19999999999999996, -1.05, -1.1, -1)\n",
      "Steps 44020/250000 (17.608%) - Rewards: (-1.1, -1.2, -1.05, -4.349999999999996)\n",
      "Steps 44183/250000 (17.673%) - Rewards: (-1, -1.05, -1.1, -2.3500000000000005)\n",
      "Steps 44346/250000 (17.738%) - Rewards: (-1, -1.3, -1.1, -1.1)\n",
      "Steps 44509/250000 (17.804%) - Rewards: (-0.15000000000000002, -1.05, -1.1, -1.15)\n",
      "Steps 44672/250000 (17.869%) - Rewards: (-1.05, -1, -1.05, -1.05)\n",
      "Steps 44875/250000 (17.95%) - Rewards: (0.8999999999999999, -1, -1.1, -0.050000000000000044)\n",
      "Steps 45058/250000 (18.023%) - Rewards: (-0.15000000000000002, -2.3500000000000005, -1, -1)\n",
      "Steps 45241/250000 (18.096%) - Rewards: (-0.09999999999999998, -1.05, -1.1, -1.05)\n",
      "Steps 45404/250000 (18.162%) - Rewards: (-0.15000000000000002, -1, -1, -1)\n",
      "Steps 45587/250000 (18.235%) - Rewards: (-0.09999999999999998, -1.1, -0.050000000000000044, -1.1)\n",
      "Steps 45790/250000 (18.316%) - Rewards: (-0.10000000000000009, -1.35, -1.15, -1.0000000000000002)\n",
      "Steps 45953/250000 (18.381%) - Rewards: (-1.15, -1, -1, -1.15)\n",
      "Steps 46116/250000 (18.446%) - Rewards: (-1.05, -1.05, -1.15, -1.15)\n",
      "Steps 46279/250000 (18.512%) - Rewards: (-1.05, -1.15, -1.4, -1.3)\n",
      "Steps 46522/250000 (18.609%) - Rewards: (-1.1, -1.0499999999999998, 0.6499999999999999, 0.5999999999999999)\n",
      "Steps 46705/250000 (18.682%) - Rewards: (-1.25, -1.25, -1.0, -1.5)\n",
      "Steps 46888/250000 (18.755%) - Rewards: (-0.30000000000000004, -1, -1, -0.3500000000000002)\n",
      "Steps 47051/250000 (18.82%) - Rewards: (-1.1, -1.1, -1.05, -1.05)\n",
      "Steps 47214/250000 (18.886%) - Rewards: (-1, -1.05, -1.3, -1.25)\n",
      "Steps 47377/250000 (18.951%) - Rewards: (-0.15000000000000013, -1.15, -1.15, -1.05)\n",
      "Steps 47540/250000 (19.016%) - Rewards: (-1, -1.1, -0.15000000000000002, -1.15)\n",
      "Steps 47743/250000 (19.097%) - Rewards: (-0.10000000000000009, -1.15, -1.4500000000000006, -0.25)\n",
      "Steps 47946/250000 (19.178%) - Rewards: (-0.25, -1.05, -0.3500000000000003, -1.1)\n",
      "Steps 48109/250000 (19.244%) - Rewards: (-1.15, -1, -1.05, -1.05)\n",
      "Steps 48272/250000 (19.309%) - Rewards: (-1, -1.2, -1.1, -1.15)\n",
      "Steps 48475/250000 (19.39%) - Rewards: (-1.15, -1.05, -1.15, 0.7499999999999998)\n",
      "Steps 48638/250000 (19.455%) - Rewards: (-1.1, -1.5499999999999998, -1.05, -1.2)\n",
      "Steps 48801/250000 (19.52%) - Rewards: (-1.1, -1.05, -1.7000000000000002, -0.050000000000000044)\n",
      "Steps 48964/250000 (19.586%) - Rewards: (-1.1, -1.1, -1, -1.15)\n",
      "Steps 49127/250000 (19.651%) - Rewards: (-1, -1, -1.5, -1.1)\n",
      "Steps 49290/250000 (19.716%) - Rewards: (-0.050000000000000044, -1.1, -1.6, -0.20000000000000007)\n",
      "Steps 49473/250000 (19.789%) - Rewards: (-1.1, -1.15, -0.20000000000000007, -1)\n",
      "Steps 49636/250000 (19.854%) - Rewards: (-1.15, -1.1, -1.35, -1.35)\n",
      "Steps 49799/250000 (19.92%) - Rewards: (-1.1, -2.950000000000001, -1, -1.05)\n",
      "Steps 49962/250000 (19.985%) - Rewards: (-1, -1.25, -1.05, -0.09999999999999998)\n",
      "Steps 50125/250000 (20.05%) - Rewards: (-1, -1.25, -1.1, -1.35)\n",
      "Steps 50288/250000 (20.115%) - Rewards: (-0.09999999999999998, -0.5, -1, 0.6499999999999999)\n",
      "Steps 50471/250000 (20.188%) - Rewards: (-1.55, -1.15, -1.25, -0.20000000000000018)\n",
      "Steps 50714/250000 (20.286%) - Rewards: (-0.09999999999999998, -1.4, -0.15000000000000002, 1.2999999999999998)\n",
      "Steps 50877/250000 (20.351%) - Rewards: (-1.1, -1.05, -1.45, -0.25)\n",
      "Steps 51060/250000 (20.424%) - Rewards: (-1.1, -1.1, -1.15, -1.3000000000000003)\n",
      "Steps 51263/250000 (20.505%) - Rewards: (-0.15000000000000002, -1.05, -1.0000000000000002, -1.25)\n",
      "Steps 51426/250000 (20.57%) - Rewards: (-1, -1, -1.1, -1.9500000000000002)\n",
      "Steps 51589/250000 (20.636%) - Rewards: (-1.1, -1, -1.05, -1.1)\n",
      "Steps 51772/250000 (20.709%) - Rewards: (-1.05, -1.1, -1.05, -1.4000000000000006)\n",
      "Steps 51935/250000 (20.774%) - Rewards: (-1.05, -1.2, -2.0, -1.35)\n",
      "Steps 52098/250000 (20.839%) - Rewards: (-1.05, -0.20000000000000007, -1, -1.2)\n",
      "Steps 52261/250000 (20.904%) - Rewards: (-1, -1.9000000000000004, -1, -1.4)\n",
      "Steps 52424/250000 (20.97%) - Rewards: (-0.050000000000000044, -1.15, -1, -1.45)\n",
      "Steps 52587/250000 (21.035%) - Rewards: (-1.05, -1.1, -0.35, -1.05)\n",
      "Steps 52750/250000 (21.1%) - Rewards: (-1.05, -1, -1.2, -1.2)\n",
      "Steps 52933/250000 (21.173%) - Rewards: (-1, -1.15, -1, -0.44999999999999996)\n",
      "Steps 53116/250000 (21.246%) - Rewards: (-1.15, -0.25, -1.1, -1.15)\n",
      "Steps 53279/250000 (21.312%) - Rewards: (-1, -1.2, -1, -0.09999999999999998)\n",
      "Steps 53442/250000 (21.377%) - Rewards: (-1.45, -1.4, -1.05, -0.09999999999999998)\n",
      "Steps 53605/250000 (21.442%) - Rewards: (-1, -1.05, -1, -0.09999999999999998)\n",
      "Steps 53768/250000 (21.507%) - Rewards: (-1.1, -0.15000000000000013, -1.15, -1.15)\n",
      "Steps 54011/250000 (21.604%) - Rewards: (-0.20000000000000007, 0.7999999999999998, 0.6499999999999999, -0.2500000000000001)\n",
      "Steps 54174/250000 (21.67%) - Rewards: (-1.05, -1, -1.05, -1.15)\n",
      "Steps 54357/250000 (21.743%) - Rewards: (-1.2, -1.05, -1.15, -0.30000000000000016)\n",
      "Steps 54560/250000 (21.824%) - Rewards: (-0.9000000000000002, -0.15000000000000002, 1.4, -1.15)\n",
      "Steps 54723/250000 (21.889%) - Rewards: (-1, -1.2, -1.4, -1.05)\n",
      "Steps 54886/250000 (21.954%) - Rewards: (-1.05, -0.25, -1.15, -1.1)\n",
      "Steps 55069/250000 (22.028%) - Rewards: (-1.25, -0.4500000000000002, -1.1500000000000004, -1.05)\n",
      "Steps 55232/250000 (22.093%) - Rewards: (-1, -1.25, -1.25, -1.1)\n",
      "Steps 55395/250000 (22.158%) - Rewards: (-1.1, -1.1, -1.2, -1)\n",
      "Steps 55558/250000 (22.223%) - Rewards: (-1, -1.35, -1.05, -0.25)\n",
      "Steps 55721/250000 (22.288%) - Rewards: (-1, -1.25, -1.45, -1.3)\n",
      "Steps 55984/250000 (22.394%) - Rewards: (1.25, -1.45, -1.15, -0.9)\n",
      "Steps 56147/250000 (22.459%) - Rewards: (-1.05, -1.2, -1.05, -1.45)\n",
      "Steps 56310/250000 (22.524%) - Rewards: (-1, -1.15, -1.6, -1.1)\n",
      "Steps 56473/250000 (22.589%) - Rewards: (-1.1, -0.25, -1.2, -2.0)\n",
      "Steps 56676/250000 (22.67%) - Rewards: (-1.1, 1.5999999999999996, -1.05, -1.5499999999999998)\n",
      "Steps 56839/250000 (22.736%) - Rewards: (-1.2, -1.3, -0.15000000000000002, -0.5500000000000002)\n",
      "Steps 57042/250000 (22.817%) - Rewards: (-0.050000000000000044, -1.0000000000000002, -0.30000000000000004, -1.15)\n",
      "Steps 57205/250000 (22.882%) - Rewards: (-1, -1.25, -1, -1.05)\n",
      "Steps 57368/250000 (22.947%) - Rewards: (-3.0500000000000007, -1, -1.1, -1)\n",
      "Steps 57531/250000 (23.012%) - Rewards: (-1, -1, -1.05, -1.1)\n",
      "Steps 57694/250000 (23.078%) - Rewards: (-1.1, -1.05, -1.2, -1.05)\n",
      "Steps 57897/250000 (23.159%) - Rewards: (-1.05, -1, 0.5499999999999998, -1.1)\n",
      "Steps 58060/250000 (23.224%) - Rewards: (-1.05, -1, -1, -1)\n",
      "Steps 58243/250000 (23.297%) - Rewards: (-0.25, -1.2, -1.2, -1.1)\n",
      "Steps 58406/250000 (23.362%) - Rewards: (-1.1, -1.15, -0.15000000000000002, -1.05)\n",
      "Steps 58569/250000 (23.428%) - Rewards: (-1, -1.15, -1, -1.1)\n",
      "Steps 58752/250000 (23.501%) - Rewards: (-1.05, -0.25, -1.1, -1.1)\n",
      "Steps 58935/250000 (23.574%) - Rewards: (-1.05, -1.25, -1.05, -0.09999999999999998)\n",
      "Steps 59098/250000 (23.639%) - Rewards: (-0.09999999999999998, -1.5, -1.25, -0.050000000000000044)\n",
      "Steps 59261/250000 (23.704%) - Rewards: (-1.1, -1.15, -1, -1.15)\n",
      "Steps 59484/250000 (23.794%) - Rewards: (-1.45, -1.25, -1.05, 0.6499999999999999)\n",
      "Steps 59647/250000 (23.859%) - Rewards: (-2.2, -1.2, -1.05, -1)\n",
      "Steps 59810/250000 (23.924%) - Rewards: (-0.25, -1.15, -1.15, -1.25)\n",
      "Steps 59993/250000 (23.997%) - Rewards: (-0.9500000000000003, -1.05, -1.1, -2.2)\n",
      "Steps 60156/250000 (24.062%) - Rewards: (-1.05, -1, -1.8000000000000003, -1.3500000000000005)\n",
      "Steps 60319/250000 (24.128%) - Rewards: (-1.1, -0.44999999999999996, -1.1, -1.4)\n",
      "Steps 60502/250000 (24.201%) - Rewards: (-0.30000000000000004, -1.2, -1.05, -0.15000000000000002)\n",
      "Steps 60665/250000 (24.266%) - Rewards: (-1.25, -1, -1.05, -1.2)\n",
      "Steps 60828/250000 (24.331%) - Rewards: (-1.05, -1.1, -1.15, -1.1)\n",
      "Steps 60991/250000 (24.396%) - Rewards: (-1.05, -1.1, -0.050000000000000044, -0.10000000000000009)\n",
      "Steps 61154/250000 (24.462%) - Rewards: (-1.1, -1.1, -1, -0.44999999999999996)\n",
      "Steps 61317/250000 (24.527%) - Rewards: (-1.15, -0.09999999999999998, -1.5, -1)\n",
      "Steps 61500/250000 (24.6%) - Rewards: (-1.05, -1.05, -1.85, -0.10000000000000009)\n",
      "Steps 61683/250000 (24.673%) - Rewards: (-0.20000000000000007, -1.25, -1.5499999999999998, -1.1)\n",
      "Steps 61846/250000 (24.738%) - Rewards: (-1.2, -1.05, -1.05, -1.05)\n",
      "Steps 62029/250000 (24.812%) - Rewards: (-1, -1.1, -1, 0.8999999999999999)\n",
      "Steps 62192/250000 (24.877%) - Rewards: (-1.1, -1.25, -1.05, -0.050000000000000044)\n",
      "Steps 62375/250000 (24.95%) - Rewards: (-1.25, -1.2, -1.05, -0.09999999999999998)\n",
      "Steps 62558/250000 (25.023%) - Rewards: (-0.20000000000000007, -0.09999999999999998, -1.3, -1.05)\n",
      "Steps 62741/250000 (25.096%) - Rewards: (1.75, -1.1, -1.1, -1.05)\n",
      "Steps 62904/250000 (25.162%) - Rewards: (-0.20000000000000007, -1.05, -0.09999999999999998, -1.05)\n",
      "Steps 63067/250000 (25.227%) - Rewards: (-1.1, -1.25, -1, -1.15)\n",
      "Steps 63250/250000 (25.3%) - Rewards: (-1.2, -1.2, -0.4, -1.1)\n",
      "Steps 63433/250000 (25.373%) - Rewards: (-0.15000000000000002, -0.30000000000000004, -1.1, -1)\n",
      "Steps 63596/250000 (25.438%) - Rewards: (-1.15, 0.75, -1.05, -1.05)\n",
      "Steps 63759/250000 (25.504%) - Rewards: (-0.05000000000000049, -1.05, -1, -1.1)\n",
      "Steps 63922/250000 (25.569%) - Rewards: (-1.1, -1, -1.05, -1.05)\n",
      "Steps 64085/250000 (25.634%) - Rewards: (-1.05, -1.1, -1.15, -1.05)\n",
      "Steps 64248/250000 (25.699%) - Rewards: (-1.05, -1.1, -1.2, -1.05)\n",
      "Steps 64411/250000 (25.764%) - Rewards: (-1.05, -1, -1.9000000000000004, -0.050000000000000044)\n",
      "Steps 64574/250000 (25.83%) - Rewards: (-1.1, -1.15, -1, -1.05)\n",
      "Steps 64757/250000 (25.903%) - Rewards: (-1.05, -1.15, -0.09999999999999998, -0.3500000000000001)\n",
      "Steps 64920/250000 (25.968%) - Rewards: (-1.1, -1.1, -1.1, -1)\n",
      "Steps 65083/250000 (26.033%) - Rewards: (-1.1, -1.1, -1, -1)\n",
      "Steps 65286/250000 (26.114%) - Rewards: (-1.35, -0.09999999999999998, -0.10000000000000009, -0.15000000000000002)\n",
      "Steps 65489/250000 (26.196%) - Rewards: (-0.2500000000000002, -1, -1.05, -0.15000000000000002)\n",
      "Steps 65652/250000 (26.261%) - Rewards: (-1.25, -1.05, -1.15, -1.05)\n",
      "Steps 65855/250000 (26.342%) - Rewards: (-1.25, -0.10000000000000009, -1, -0.050000000000000044)\n",
      "Steps 66078/250000 (26.431%) - Rewards: (-1.9000000000000004, 0.7999999999999998, -0.050000000000000044, -1.05)\n",
      "Steps 66241/250000 (26.496%) - Rewards: (-1.1, -1.45, -1, -1.15)\n",
      "Steps 66424/250000 (26.57%) - Rewards: (-1.05, -1.15, -0.10000000000000009, 0.8999999999999999)\n",
      "Steps 66587/250000 (26.635%) - Rewards: (-1, -1.1, -1.05, -1)\n",
      "Steps 66750/250000 (26.7%) - Rewards: (-1.05, -1.2, -1.3, -0.050000000000000044)\n",
      "Steps 66913/250000 (26.765%) - Rewards: (-2.0, 0.7999999999999998, -1, -0.10000000000000009)\n",
      "Steps 67076/250000 (26.83%) - Rewards: (-1.1, -1.2, -1.05, -1)\n",
      "Steps 67239/250000 (26.896%) - Rewards: (-1.15, -1.15, -1, -1.1)\n",
      "Steps 67422/250000 (26.969%) - Rewards: (-0.15000000000000002, -1.35, -1.1, -1.05)\n",
      "Steps 67585/250000 (27.034%) - Rewards: (-1.1, -1.05, -1.1, -1.1)\n",
      "Steps 67748/250000 (27.099%) - Rewards: (-1.1, -1.05, -1.2, -1)\n",
      "Steps 67951/250000 (27.18%) - Rewards: (-1.1, -1.05, 0.19999999999999996, 0.19999999999999996)\n",
      "Steps 68174/250000 (27.27%) - Rewards: (-1.2, 0.6999999999999997, -0.4, -1.1)\n",
      "Steps 68377/250000 (27.351%) - Rewards: (-0.19999999999999996, -1.25, -1.15, 0.5499999999999996)\n",
      "Steps 68580/250000 (27.432%) - Rewards: (-1.05, -0.25, -1.05, -1.35)\n",
      "Steps 68763/250000 (27.505%) - Rewards: (-1, -0.20000000000000007, -1.5499999999999998, -1.35)\n",
      "Steps 68926/250000 (27.57%) - Rewards: (-1.2, -1.2, -0.44999999999999996, -1.35)\n",
      "Steps 69089/250000 (27.636%) - Rewards: (-1.15, -0.25, -0.09999999999999998, -0.7)\n",
      "Steps 69272/250000 (27.709%) - Rewards: (-1.15, -1.05, -1.15, -0.09999999999999998)\n",
      "Steps 69435/250000 (27.774%) - Rewards: (-1.05, -0.050000000000000044, -1, -1.6)\n",
      "Steps 69598/250000 (27.839%) - Rewards: (-1.05, -1.15, -1.1, -1.4)\n",
      "Steps 69821/250000 (27.928%) - Rewards: (0.8499999999999999, -0.30000000000000004, -1, -0.09999999999999998)\n",
      "Steps 70004/250000 (28.002%) - Rewards: (-1.1, -0.15000000000000002, -1.1, -1.25)\n",
      "Steps 70187/250000 (28.075%) - Rewards: (-1.05, -1.15, -0.3500000000000001, -1.1)\n",
      "Steps 70350/250000 (28.14%) - Rewards: (-1, -1.05, -1.15, -1.15)\n",
      "Steps 70513/250000 (28.205%) - Rewards: (-1.05, -1, -1.3, -0.10000000000000009)\n",
      "Steps 70676/250000 (28.27%) - Rewards: (-1.1, -1.05, -0.5, -1.1)\n",
      "Steps 70859/250000 (28.344%) - Rewards: (-1.05, 1.75, -1.05, -1.25)\n",
      "Steps 71042/250000 (28.417%) - Rewards: (-1.15, -1.2, -1.05, -0.30000000000000004)\n",
      "Steps 71205/250000 (28.482%) - Rewards: (-1.05, -1, -1.3, -1.05)\n",
      "Steps 71388/250000 (28.555%) - Rewards: (-1.05, -1.05, -1.1, 0.5999999999999999)\n",
      "Steps 71551/250000 (28.62%) - Rewards: (-1.1, -1.2, -1.1, -1.25)\n",
      "Steps 71734/250000 (28.694%) - Rewards: (-1.25, -1.05, -1.1, -0.10000000000000009)\n",
      "Steps 71897/250000 (28.759%) - Rewards: (-1.05, -1.05, -1.1, -1.05)\n",
      "Steps 72060/250000 (28.824%) - Rewards: (-1.15, -1.1500000000000001, -1, -1.15)\n",
      "Steps 72223/250000 (28.889%) - Rewards: (-1.2000000000000002, -1.45, -1, -1)\n",
      "Steps 72386/250000 (28.954%) - Rewards: (-1.1, -1, -1.05, -1)\n",
      "Steps 72569/250000 (29.028%) - Rewards: (-0.25, -0.20000000000000007, -1, -1)\n",
      "Steps 72752/250000 (29.101%) - Rewards: (-1.05, -1.1500000000000004, -1.1, -1.35)\n",
      "Steps 72915/250000 (29.166%) - Rewards: (-1.1, -0.15000000000000002, -1.1, -1.4)\n",
      "Steps 73158/250000 (29.263%) - Rewards: (0.4999999999999998, -0.30000000000000004, -2.0500000000000003, -0.10000000000000009)\n",
      "Steps 73321/250000 (29.328%) - Rewards: (-1.05, -0.3500000000000001, -1.5499999999999998, -1.05)\n",
      "Steps 73484/250000 (29.394%) - Rewards: (-1.1, -1.3, -1, -1.2)\n",
      "Steps 73667/250000 (29.467%) - Rewards: (-1.05, -1.4, -1.1, -0.15000000000000013)\n",
      "Steps 73830/250000 (29.532%) - Rewards: (-1.2, -0.25, -1, -1)\n",
      "Steps 73993/250000 (29.597%) - Rewards: (-1.45, -1.5, -1.05, -1.15)\n",
      "Steps 74156/250000 (29.662%) - Rewards: (-1.45, -1.3, -1.05, -1.1)\n",
      "Steps 74319/250000 (29.728%) - Rewards: (-1.45, -1.25, -1, -1.05)\n",
      "Steps 74502/250000 (29.801%) - Rewards: (-1.05, -1.05, -1.15, -0.050000000000000044)\n",
      "Steps 74665/250000 (29.866%) - Rewards: (-1.05, -1.15, -1.05, -1.05)\n",
      "Steps 74868/250000 (29.947%) - Rewards: (-1.05, -0.30000000000000004, -0.30000000000000004, -0.15000000000000013)\n",
      "Steps 75031/250000 (30.012%) - Rewards: (-1.05, -1.05, -1.2, -1.05)\n",
      "Steps 75194/250000 (30.078%) - Rewards: (-1, -1.15, -1.1, -1.1)\n",
      "Steps 75377/250000 (30.151%) - Rewards: (-1.1, -0.09999999999999998, -1.05, -0.050000000000000044)\n",
      "Steps 75540/250000 (30.216%) - Rewards: (-1.2, -1.05, -1, -1.05)\n",
      "Steps 75703/250000 (30.281%) - Rewards: (-1.05, -1.1, -1.1, -1.05)\n",
      "Steps 75886/250000 (30.354%) - Rewards: (-1.05, -1.05, -0.09999999999999998, -1.05)\n",
      "Steps 76129/250000 (30.452%) - Rewards: (0.6999999999999997, -1.1, -1.5000000000000002, -0.2500000000000002)\n",
      "Steps 76292/250000 (30.517%) - Rewards: (-1.1, -1, -2.3999999999999995, 0.8500000000000001)\n",
      "Steps 76455/250000 (30.582%) - Rewards: (-1.15, -1.05, -1.1, -1.25)\n",
      "Steps 76638/250000 (30.655%) - Rewards: (-1.1, -1, -0.6, -1.5)\n",
      "Steps 76801/250000 (30.72%) - Rewards: (-1.1, -1.2, -1.5, -1.5499999999999998)\n",
      "Steps 76964/250000 (30.786%) - Rewards: (-1.1, -1.15, -1.8000000000000003, -1.3)\n",
      "Steps 77147/250000 (30.859%) - Rewards: (-1.1, -1.3, -0.2500000000000002, -1.5)\n",
      "Steps 77310/250000 (30.924%) - Rewards: (-1, -0.15000000000000002, -0.3999999999999999, -1.85)\n",
      "Steps 77533/250000 (31.013%) - Rewards: (-1.15, 0.6499999999999999, 0.1999999999999995, -0.30000000000000004)\n",
      "Steps 77696/250000 (31.078%) - Rewards: (-1.1, -1.05, -1.2, -1.05)\n",
      "Steps 77859/250000 (31.144%) - Rewards: (-1.05, -1.1, -1.85, -1)\n",
      "Steps 78042/250000 (31.217%) - Rewards: (-1.25, -1.25, -0.5, -1.05)\n",
      "Steps 78225/250000 (31.29%) - Rewards: (-1, -1.25, -0.25, -0.7000000000000001)\n",
      "Steps 78408/250000 (31.363%) - Rewards: (-1.1, -1.15, -1.35, -1.65)\n",
      "Steps 78611/250000 (31.444%) - Rewards: (-1, -0.20000000000000007, -1.35, -0.10000000000000009)\n",
      "Steps 78774/250000 (31.51%) - Rewards: (-1.1, -2.1000000000000005, -1.05, -1.3)\n",
      "Steps 78957/250000 (31.583%) - Rewards: (-1.15, 0.6499999999999999, -1, -1.5499999999999998)\n",
      "Steps 79120/250000 (31.648%) - Rewards: (-0.2500000000000001, -1.1, -1.05, -1.5499999999999998)\n",
      "Steps 79303/250000 (31.721%) - Rewards: (-0.2500000000000001, -1, -1.45, -1.4)\n",
      "Steps 79466/250000 (31.786%) - Rewards: (-1.1, -1.1, -0.30000000000000004, -1.5499999999999998)\n",
      "Steps 79629/250000 (31.852%) - Rewards: (-1.05, -0.20000000000000007, -1.15, -1.65)\n",
      "Steps 79792/250000 (31.917%) - Rewards: (-1.1, -1.05, -1.6, -2.2)\n",
      "Steps 79975/250000 (31.99%) - Rewards: (-1.15, -1.1, -0.7, -1.75)\n",
      "Steps 80138/250000 (32.055%) - Rewards: (-1.05, -1.05, -1.35, -1.35)\n",
      "Steps 80301/250000 (32.12%) - Rewards: (-1.15, -1.45, -0.15000000000000002, -0.15000000000000002)\n",
      "Steps 80464/250000 (32.186%) - Rewards: (-1.1, -1.15, -1.05, -1.45)\n",
      "Steps 80647/250000 (32.259%) - Rewards: (-1.1, -1.25, -0.10000000000000009, -1.25)\n",
      "Steps 80810/250000 (32.324%) - Rewards: (-1.1, 0.5499999999999998, -0.050000000000000044, -0.3500000000000001)\n",
      "Steps 80973/250000 (32.389%) - Rewards: (-0.15000000000000002, -1.1, -1.05, -1.05)\n",
      "Steps 81136/250000 (32.454%) - Rewards: (-1.05, -1.15, -1.05, -1.15)\n",
      "Steps 81299/250000 (32.52%) - Rewards: (-1.1, -1.25, -1, -1.1)\n",
      "Steps 81462/250000 (32.585%) - Rewards: (-1.1, -1, -1.05, -1.15)\n",
      "Steps 81645/250000 (32.658%) - Rewards: (-1.05, -0.050000000000000044, -0.55, -1)\n",
      "Steps 81828/250000 (32.731%) - Rewards: (-1.05, -0.09999999999999998, -0.30000000000000016, -1.1)\n",
      "Steps 81991/250000 (32.796%) - Rewards: (-1.45, -1, 0.8499999999999999, -1)\n",
      "Steps 82194/250000 (32.878%) - Rewards: (-1, -1.05, -0.20000000000000018, -0.4500000000000002)\n",
      "Steps 82357/250000 (32.943%) - Rewards: (-1, -1.05, -1.05, -1.05)\n",
      "Steps 82520/250000 (33.008%) - Rewards: (-1, -1.05, -1.05, -1)\n",
      "Steps 82683/250000 (33.073%) - Rewards: (-1.15, -1.15, -1.05, -1.05)\n",
      "Steps 82846/250000 (33.138%) - Rewards: (-1, -2.5500000000000007, -0.15000000000000002, -1.05)\n",
      "Steps 83009/250000 (33.204%) - Rewards: (-1.05, -0.15000000000000013, -1.1, -1)\n",
      "Steps 83172/250000 (33.269%) - Rewards: (-0.09999999999999998, -1.3, -1.1, -1)\n",
      "Steps 83335/250000 (33.334%) - Rewards: (-1.05, -1.1, -0.5499999999999999, -1.1)\n",
      "Steps 83558/250000 (33.423%) - Rewards: (-0.09999999999999998, -1.3499999999999999, -2.0500000000000003, 1.0499999999999998)\n",
      "Steps 83761/250000 (33.504%) - Rewards: (-1.1, -1, -1.05, 0.7499999999999998)\n",
      "Steps 83944/250000 (33.578%) - Rewards: (-1.15, -1.1, -2.000000000000001, -1.9000000000000004)\n",
      "Steps 84127/250000 (33.651%) - Rewards: (-2.700000000000001, -1.2, -2.150000000000001, -2.1500000000000004)\n",
      "Steps 84290/250000 (33.716%) - Rewards: (-1.15, -2.3500000000000005, -1.45, 0.09999999999999987)\n",
      "Steps 84453/250000 (33.781%) - Rewards: (-0.10000000000000009, -1.2, -1.15, -1.2500000000000002)\n",
      "Steps 84616/250000 (33.846%) - Rewards: (-1.1, -1.65, -1.1, -1.8000000000000003)\n",
      "Steps 84799/250000 (33.92%) - Rewards: (-0.15000000000000013, -1.4, -1.45, -3.000000000000001)\n",
      "Steps 84962/250000 (33.985%) - Rewards: (-0.20000000000000007, -1.6, -1.85, -1.45)\n",
      "Steps 85125/250000 (34.05%) - Rewards: (-1.15, -1.35, -1.85, -0.3500000000000001)\n",
      "Steps 85388/250000 (34.155%) - Rewards: (-0.3999999999999999, -0.5, -1.2000000000000002, 1.25)\n",
      "Steps 85551/250000 (34.22%) - Rewards: (-1.15, -1.1, -1.35, -1.85)\n",
      "Steps 85714/250000 (34.286%) - Rewards: (-0.050000000000000044, -1.3, -1.5499999999999998, -1.6)\n",
      "Steps 85877/250000 (34.351%) - Rewards: (-1.2, -1.1, -0.7, -1.5499999999999998)\n",
      "Steps 86040/250000 (34.416%) - Rewards: (-1.25, -1.35, -1.15, -2.0500000000000003)\n",
      "Steps 86203/250000 (34.481%) - Rewards: (-1.35, -0.9500000000000001, -1.1, -1.1)\n",
      "Steps 86386/250000 (34.554%) - Rewards: (-1.3, -0.19999999999999996, -1.1, -0.25)\n",
      "Steps 86549/250000 (34.62%) - Rewards: (-1.2, -1.05, -1.4, -1.05)\n",
      "Steps 86752/250000 (34.701%) - Rewards: (-1.15, -1.1, -1.65, 0.75)\n",
      "Steps 86935/250000 (34.774%) - Rewards: (-1.15, -1.1, -0.30000000000000004, 0.44999999999999973)\n",
      "Steps 87098/250000 (34.839%) - Rewards: (-1.2, -1.3, -1.1, -1.05)\n",
      "Steps 87281/250000 (34.912%) - Rewards: (-1.1, -1.2, -0.30000000000000016, -1.1)\n",
      "Steps 87444/250000 (34.978%) - Rewards: (-0.15000000000000002, -1.8000000000000003, -1.05, -1.05)\n",
      "Steps 87607/250000 (35.043%) - Rewards: (-1.1, -1.05, -1.05, -1.1)\n",
      "Steps 87790/250000 (35.116%) - Rewards: (-0.25, -1, -0.050000000000000044, -1.1)\n",
      "Steps 87993/250000 (35.197%) - Rewards: (-1.75, -1.3, -0.10000000000000009, -0.30000000000000027)\n",
      "Steps 88156/250000 (35.262%) - Rewards: (-1.1, -1.05, -1.2, -1.5)\n",
      "Steps 88319/250000 (35.328%) - Rewards: (-1.1, -1.05, -1.05, -1.2)\n",
      "Steps 88482/250000 (35.393%) - Rewards: (-1.1, -1.1, -1, -1.3)\n",
      "Steps 88645/250000 (35.458%) - Rewards: (-1.2000000000000002, -1.1, -1.2, -0.09999999999999998)\n",
      "Steps 88828/250000 (35.531%) - Rewards: (-1, -1, -0.30000000000000093, -1)\n",
      "Steps 88991/250000 (35.596%) - Rewards: (-1.1, -1.05, -0.15000000000000002, -1.1)\n",
      "Steps 89154/250000 (35.662%) - Rewards: (-1, -1, -1.25, -1)\n",
      "Steps 89317/250000 (35.727%) - Rewards: (-1.1, -1.2, -1.3, -1.05)\n",
      "Steps 89480/250000 (35.792%) - Rewards: (-1, -1.1, -1.05, -1.1)\n",
      "Steps 89643/250000 (35.857%) - Rewards: (-0.09999999999999998, -1, -1.1, -1.05)\n",
      "Steps 89806/250000 (35.922%) - Rewards: (-1.05, -1, -1, -0.050000000000000044)\n",
      "Steps 89969/250000 (35.988%) - Rewards: (-1, -1, -1.1, -1)\n",
      "Steps 90132/250000 (36.053%) - Rewards: (-1.15, -1, -1.1, -1.05)\n",
      "Steps 90295/250000 (36.118%) - Rewards: (-1, -1.1, -1.1, -1.15)\n",
      "Steps 90458/250000 (36.183%) - Rewards: (-1.05, -1.15, -1.05, -1)\n",
      "Steps 90621/250000 (36.248%) - Rewards: (-1.5, -0.5000000000000002, -1.5, -1)\n",
      "Steps 90784/250000 (36.314%) - Rewards: (-1.2, -1.45, -1.05, -1.15)\n",
      "Steps 90947/250000 (36.379%) - Rewards: (-1.2, -1.85, -1.05, -1.15)\n",
      "Steps 91110/250000 (36.444%) - Rewards: (-1, -1.05, -1.25, -1.05)\n",
      "Steps 91273/250000 (36.509%) - Rewards: (-1, -1.1, -1, -1.05)\n",
      "Steps 91436/250000 (36.574%) - Rewards: (-1.1, -1.35, -1.05, -1.05)\n",
      "Steps 91599/250000 (36.64%) - Rewards: (-1.05, -1, -1.05, -1.05)\n",
      "Steps 91762/250000 (36.705%) - Rewards: (-1.05, -1.1, -1.2, -1)\n",
      "Steps 91925/250000 (36.77%) - Rewards: (-1.05, -1.05, -1.05, -1.2)\n",
      "Steps 92148/250000 (36.859%) - Rewards: (-0.09999999999999998, -0.15000000000000013, -1.1, -0.30000000000000004)\n",
      "Steps 92311/250000 (36.924%) - Rewards: (-1.05, -1, -1, -1.05)\n",
      "Steps 92494/250000 (36.998%) - Rewards: (-1, -0.4500000000000003, -1.45, -1.2)\n",
      "Steps 92657/250000 (37.063%) - Rewards: (-1.05, -1, -1.1, -1)\n",
      "Steps 92840/250000 (37.136%) - Rewards: (-1, -1, -1.1, -0.050000000000000044)\n",
      "Steps 93003/250000 (37.201%) - Rewards: (-1.05, -1, -1.1, -1.05)\n",
      "Steps 93166/250000 (37.266%) - Rewards: (-1, -1.05, -1.05, -1)\n",
      "Steps 93329/250000 (37.332%) - Rewards: (-1.05, -1.1, -1.05, -1.15)\n",
      "Steps 93492/250000 (37.397%) - Rewards: (-1.05, -1, -1, -1.35)\n",
      "Steps 93675/250000 (37.47%) - Rewards: (-0.050000000000000044, -1.2, -1.05, -1.05)\n",
      "Steps 93858/250000 (37.543%) - Rewards: (-1.1, -1.1, -0.20000000000000007, -1.05)\n",
      "Steps 94021/250000 (37.608%) - Rewards: (-1.1, -1.1, -1.45, -1)\n",
      "Steps 94184/250000 (37.674%) - Rewards: (-0.050000000000000044, -1.1, -1.05, -1.15)\n",
      "Steps 94407/250000 (37.763%) - Rewards: (0.7, -1.15, 0.8999999999999999, -1.1)\n",
      "Steps 94570/250000 (37.828%) - Rewards: (-1, -1, -1, -1.2)\n",
      "Steps 94733/250000 (37.893%) - Rewards: (-1.2, -1.1, -1.1, -1.3)\n",
      "Steps 94896/250000 (37.958%) - Rewards: (-1, -1, -1, -1.25)\n",
      "Steps 95059/250000 (38.024%) - Rewards: (-1, -1.1, -1.2, -1.25)\n",
      "Steps 95222/250000 (38.089%) - Rewards: (-1.05, -1.05, -1.05, -1.35)\n",
      "Steps 95385/250000 (38.154%) - Rewards: (-0.09999999999999998, -1.65, -1, -0.09999999999999998)\n",
      "Steps 95568/250000 (38.227%) - Rewards: (-1.15, -1.1, -1.1, -0.30000000000000004)\n",
      "Steps 95731/250000 (38.292%) - Rewards: (-1, -1.2, -1.05, -1.05)\n",
      "Steps 95894/250000 (38.358%) - Rewards: (-1.2, -1.35, -0.050000000000000044, -1.2)\n",
      "Steps 96097/250000 (38.439%) - Rewards: (-1, -0.09999999999999998, -0.10000000000000009, -1.05)\n",
      "Steps 96260/250000 (38.504%) - Rewards: (-1.1, -1.1, -1.1, -1)\n",
      "Steps 96423/250000 (38.569%) - Rewards: (-1, -1.05, -0.20000000000000007, -1.1)\n",
      "Steps 96586/250000 (38.634%) - Rewards: (-1.05, -1.05, -0.20000000000000007, -0.6)\n",
      "Steps 96749/250000 (38.7%) - Rewards: (-1.1, -1.1, -1.15, -1.1)\n",
      "Steps 96912/250000 (38.765%) - Rewards: (-1.2000000000000002, -1.25, -1, -1.15)\n",
      "Steps 97095/250000 (38.838%) - Rewards: (-1.05, -1, -0.10000000000000009, -1.2)\n",
      "Steps 97278/250000 (38.911%) - Rewards: (-0.050000000000000044, -1, -1.1, -1.05)\n",
      "Steps 97441/250000 (38.976%) - Rewards: (-1.05, -1.15, -1.15, -1.05)\n",
      "Steps 97604/250000 (39.042%) - Rewards: (-1.05, -1, -1, -1.15)\n",
      "Steps 97787/250000 (39.115%) - Rewards: (-1, -1, -1.1, -0.6499999999999999)\n",
      "Steps 97970/250000 (39.188%) - Rewards: (-1.05, -0.050000000000000044, -1.8000000000000003, -1.15)\n",
      "Steps 98133/250000 (39.253%) - Rewards: (-1.05, -1, -1.1, -1)\n",
      "Steps 98296/250000 (39.318%) - Rewards: (-1.05, -1, -1.45, -0.09999999999999998)\n",
      "Steps 98459/250000 (39.384%) - Rewards: (-1.15, -1.05, -1.05, -1.2)\n",
      "Steps 98702/250000 (39.481%) - Rewards: (-0.10000000000000009, -0.09999999999999998, -0.09999999999999998, 0.8999999999999999)\n",
      "Steps 98865/250000 (39.546%) - Rewards: (-1.25, -1.1, -1.05, -1.15)\n",
      "Steps 99028/250000 (39.611%) - Rewards: (-1.05, -1.05, -1.1, -0.19999999999999996)\n",
      "Steps 99191/250000 (39.676%) - Rewards: (-2.2, -1.1, -1.05, -1.65)\n",
      "Steps 99374/250000 (39.75%) - Rewards: (-1.1, -1.05, 0.8500000000000001, -1.1)\n",
      "Steps 99577/250000 (39.831%) - Rewards: (-1.4, -0.6, -1, -1.0500000000000003)\n",
      "Steps 99780/250000 (39.912%) - Rewards: (-1.15, -1.2, 1.6499999999999995, -0.09999999999999998)\n",
      "Steps 99943/250000 (39.977%) - Rewards: (-1.05, -1.2, -1.05, -1.1)\n",
      "Steps 100106/250000 (40.042%) - Rewards: (-0.09999999999999998, -0.35, -0.050000000000000044, -1.1)\n",
      "Steps 100269/250000 (40.108%) - Rewards: (-1.2000000000000002, -1.35, -1.05, -1.15)\n",
      "Steps 100432/250000 (40.173%) - Rewards: (-1.1, -1.15, -1.1, -1.05)\n",
      "Steps 100635/250000 (40.254%) - Rewards: (-0.050000000000000044, -1.1, -1.15, -0.15000000000000002)\n",
      "Steps 100798/250000 (40.319%) - Rewards: (-1.05, -1.1, 0.1499999999999999, -1)\n",
      "Steps 101001/250000 (40.4%) - Rewards: (-1.05, -0.15000000000000013, -0.15000000000000013, -1.05)\n",
      "Steps 101184/250000 (40.474%) - Rewards: (-1, -0.050000000000000044, -1.1, -1)\n",
      "Steps 101347/250000 (40.539%) - Rewards: (-1.05, -1.15, -1.1, -1.15)\n",
      "Steps 101550/250000 (40.62%) - Rewards: (-1.1, 0.6999999999999997, -1.3, -1.3)\n",
      "Steps 101753/250000 (40.701%) - Rewards: (-1, 0.34999999999999987, -1.65, -0.30000000000000004)\n",
      "Steps 101916/250000 (40.766%) - Rewards: (-1, -1.35, -1, -1.05)\n",
      "Steps 102079/250000 (40.832%) - Rewards: (-1.1, -1.1, -1, -0.050000000000000044)\n",
      "Steps 102242/250000 (40.897%) - Rewards: (-1.05, -1.05, -0.09999999999999998, -2.1000000000000005)\n",
      "Steps 102405/250000 (40.962%) - Rewards: (-1.15, -1.2, -1.1, -1.15)\n",
      "Steps 102568/250000 (41.027%) - Rewards: (-1, -1.35, -1.15, -1)\n",
      "Steps 102791/250000 (41.116%) - Rewards: (-0.20000000000000007, -1.2, -1.25, 0.6999999999999997)\n",
      "Steps 102974/250000 (41.19%) - Rewards: (-1, -0.20000000000000007, -1.15, -1.15)\n",
      "Steps 103157/250000 (41.263%) - Rewards: (-1, -1.2, -0.30000000000000004, -1)\n",
      "Steps 103340/250000 (41.336%) - Rewards: (-1.1, -1.15, -1.1, -0.19999999999999996)\n",
      "Steps 103503/250000 (41.401%) - Rewards: (-1.1, -1.2, -0.25, -1.15)\n",
      "Steps 103666/250000 (41.466%) - Rewards: (-1, -1.25, -1.2, -1.05)\n",
      "Steps 103849/250000 (41.54%) - Rewards: (-1.15, -0.4500000000000002, -1.05, -1.25)\n",
      "Steps 104012/250000 (41.605%) - Rewards: (-1.25, -0.44999999999999996, -0.15000000000000002, -1.1)\n",
      "Steps 104195/250000 (41.678%) - Rewards: (-0.19999999999999996, 1.6000000000000005, -1, -1.1500000000000001)\n",
      "Steps 104398/250000 (41.759%) - Rewards: (-1, -1.05, -0.09999999999999998, 0.7499999999999998)\n",
      "Steps 104561/250000 (41.824%) - Rewards: (-1.05, -1.2, -1.2, -1.15)\n",
      "Steps 104724/250000 (41.89%) - Rewards: (-1, -1.05, -1.1, -1.05)\n",
      "Steps 104887/250000 (41.955%) - Rewards: (-1.05, -1, -1.05, -1.15)\n",
      "Steps 105050/250000 (42.02%) - Rewards: (-1.05, -1.2, -1.2, -1.1)\n",
      "Steps 105213/250000 (42.085%) - Rewards: (-1.2, -1, -1.25, -1.05)\n",
      "Steps 105416/250000 (42.166%) - Rewards: (-0.10000000000000009, 0.5499999999999998, -1.1, -1.1)\n",
      "Steps 105599/250000 (42.24%) - Rewards: (-1.05, -1.15, -1.15, -0.25)\n",
      "Steps 105762/250000 (42.305%) - Rewards: (-1.15, -1.15, -0.09999999999999998, -0.050000000000000044)\n",
      "Steps 105925/250000 (42.37%) - Rewards: (-1.45, -1.05, -1.05, -1.1)\n",
      "Steps 106088/250000 (42.435%) - Rewards: (-1.2, -1.2, -0.050000000000000044, -1.1)\n",
      "Steps 106271/250000 (42.508%) - Rewards: (-1.25, -0.050000000000000044, -1.25, -1.15)\n",
      "Steps 106434/250000 (42.574%) - Rewards: (-1.05, -1.1, -1, -1.1)\n",
      "Steps 106637/250000 (42.655%) - Rewards: (-0.20000000000000007, -1.1, -0.15000000000000002, -1.15)\n",
      "Steps 106820/250000 (42.728%) - Rewards: (-1.25, -1.15, -1.05, -0.8)\n",
      "Steps 106983/250000 (42.793%) - Rewards: (-1.2, -1, -1.3, -1.15)\n",
      "Steps 107166/250000 (42.866%) - Rewards: (-1, -1.05, -1.35, -0.44999999999999996)\n",
      "Steps 107329/250000 (42.932%) - Rewards: (-1, -1, -1.05, -1.2500000000000004)\n",
      "Steps 107532/250000 (43.013%) - Rewards: (-1.05, -0.30000000000000016, -0.15000000000000002, -0.6000000000000004)\n",
      "Steps 107715/250000 (43.086%) - Rewards: (-1, -0.45000000000000007, -1.25, -1.4)\n",
      "Steps 107878/250000 (43.151%) - Rewards: (-1.05, -1, -0.2, -2.5500000000000007)\n",
      "Steps 108041/250000 (43.216%) - Rewards: (-1, -1.05, -1.05, -1.9000000000000004)\n",
      "Steps 108204/250000 (43.282%) - Rewards: (-1.05, -0.9000000000000002, 0.5499999999999998, -0.15000000000000002)\n",
      "Steps 108387/250000 (43.355%) - Rewards: (-0.050000000000000044, -1.5, -1.4, -1.1)\n",
      "Steps 108550/250000 (43.42%) - Rewards: (-1.2, -1.45, -1.5499999999999998, -0.15000000000000002)\n",
      "Steps 108733/250000 (43.493%) - Rewards: (-1.2, -1.5, -1.15, -1.05)\n",
      "Steps 108896/250000 (43.558%) - Rewards: (-1.05, -0.3500000000000001, -1.25, -1.1)\n",
      "Steps 109059/250000 (43.624%) - Rewards: (-1.05, -1.25, -0.09999999999999998, -1)\n",
      "Steps 109222/250000 (43.689%) - Rewards: (-0.3500000000000001, -1.35, -0.15000000000000002, -1.5)\n",
      "Steps 109385/250000 (43.754%) - Rewards: (-1.2, -1.5499999999999998, -1.1, -1)\n",
      "Steps 109568/250000 (43.827%) - Rewards: (-1.05, -1.25, -0.15000000000000002, -0.19999999999999996)\n",
      "Steps 109731/250000 (43.892%) - Rewards: (-1.1, -1.6, -1.2, -0.25)\n",
      "Steps 109914/250000 (43.966%) - Rewards: (-1.6, -1.25, -1.15, -0.2500000000000002)\n",
      "Steps 110077/250000 (44.031%) - Rewards: (-1.1, -1.3, -1.05, -0.050000000000000044)\n",
      "Steps 110300/250000 (44.12%) - Rewards: (-0.09999999999999998, 1.7000000000000002, -1.3, -1)\n",
      "Steps 110483/250000 (44.193%) - Rewards: (-1.15, -0.40000000000000013, -0.3500000000000001, -1)\n",
      "Steps 110646/250000 (44.258%) - Rewards: (-1.05, -0.40000000000000013, -1.1, -1.05)\n",
      "Steps 110809/250000 (44.324%) - Rewards: (-1.3, -1.15, -1.05, -1.05)\n",
      "Steps 110972/250000 (44.389%) - Rewards: (-0.09999999999999998, -1.1, -1, -1.05)\n",
      "Steps 111135/250000 (44.454%) - Rewards: (-1.2, -1.1, -1.05, -1.05)\n",
      "Steps 111338/250000 (44.535%) - Rewards: (-0.2500000000000001, -1.05, -1.05, -1.25)\n",
      "Steps 111501/250000 (44.6%) - Rewards: (-1.15, -0.050000000000000044, -1.15, -1)\n",
      "Steps 111664/250000 (44.666%) - Rewards: (-1.1, -1.15, -1, -1)\n",
      "Steps 111867/250000 (44.747%) - Rewards: (-1.3, -0.30000000000000016, -0.09999999999999998, -0.3500000000000001)\n",
      "Steps 112030/250000 (44.812%) - Rewards: (-1.65, -1.35, -1.05, -1.05)\n",
      "Steps 112193/250000 (44.877%) - Rewards: (-1.1, -1.15, -1.05, -0.30000000000000004)\n",
      "Steps 112396/250000 (44.958%) - Rewards: (-1.5, -1.35, -0.25, -1.25)\n",
      "Steps 112599/250000 (45.04%) - Rewards: (-1.25, 0.5999999999999999, -1.15, -0.15000000000000013)\n",
      "Steps 112762/250000 (45.105%) - Rewards: (-1.1, -1.15, -1.05, -1.15)\n",
      "Steps 112945/250000 (45.178%) - Rewards: (-1.1, -0.25, -1.1, -1.05)\n",
      "Steps 113128/250000 (45.251%) - Rewards: (-1.25, -0.10000000000000009, -0.19999999999999996, -1.1)\n",
      "Steps 113291/250000 (45.316%) - Rewards: (-0.25, -0.20000000000000007, -0.2500000000000001, -1.1)\n",
      "Steps 113454/250000 (45.382%) - Rewards: (-1.3, 0.29999999999999916, -1, -1)\n",
      "Steps 113617/250000 (45.447%) - Rewards: (-1.3, -1.15, -1.05, -1)\n",
      "Steps 113780/250000 (45.512%) - Rewards: (-0.3500000000000001, -1.85, -1.05, -1)\n",
      "Steps 113943/250000 (45.577%) - Rewards: (-1.05, -1.1, -1.05, 0.6999999999999997)\n",
      "Steps 114106/250000 (45.642%) - Rewards: (-0.15000000000000002, -1.15, -1.25, -1.3)\n",
      "Steps 114265/250000 (45.706%) - Rewards: (-1.1, -1.15, -0.35, -1)\n",
      "Steps 114448/250000 (45.779%) - Rewards: (-0.09999999999999998, -0.75, -0.3500000000000001, -1)\n",
      "Steps 114611/250000 (45.844%) - Rewards: (-1.2, -0.30000000000000004, -1.1, -1.1)\n",
      "Steps 114774/250000 (45.91%) - Rewards: (-0.3500000000000001, -0.44999999999999996, -1.2, -1.05)\n",
      "Steps 114937/250000 (45.975%) - Rewards: (-1.4, -1.4, -1.1, -1.05)\n",
      "Steps 115140/250000 (46.056%) - Rewards: (-1.5499999999999998, -0.25, -0.15000000000000002, -1.2)\n",
      "Steps 115363/250000 (46.145%) - Rewards: (-1.25, -0.15000000000000013, -0.25, -0.20000000000000007)\n",
      "Steps 115526/250000 (46.21%) - Rewards: (-1.2, -1.15, -1.1, -1.05)\n",
      "Steps 115709/250000 (46.284%) - Rewards: (-1.2, -0.19999999999999996, -1.05, -0.09999999999999998)\n",
      "Steps 115912/250000 (46.365%) - Rewards: (-0.40000000000000013, -0.30000000000000004, -1, -1.1)\n",
      "Steps 116075/250000 (46.43%) - Rewards: (-1.15, -1.15, -1.15, -1.05)\n",
      "Steps 116238/250000 (46.495%) - Rewards: (-1.15, -1.45, -0.35, -1.05)\n",
      "Steps 116401/250000 (46.56%) - Rewards: (-1.25, -1.15, -1, -1)\n",
      "Steps 116564/250000 (46.626%) - Rewards: (-1.1, 0.6499999999999999, -1.25, -1.05)\n",
      "Steps 116727/250000 (46.691%) - Rewards: (-1.05, -1.05, -1.15, -1.05)\n",
      "Steps 116930/250000 (46.772%) - Rewards: (-1.1, -1.25, -1.2, 0.8499999999999999)\n",
      "Steps 117133/250000 (46.853%) - Rewards: (-0.10000000000000009, -1.1, -0.09999999999999998, -1.05)\n",
      "Steps 117296/250000 (46.918%) - Rewards: (-1, -1.1, -1.05, -1.1)\n",
      "Steps 117459/250000 (46.984%) - Rewards: (-1.05, -0.15000000000000002, -1.3, -1)\n",
      "Steps 117622/250000 (47.049%) - Rewards: (-1.1, -1.15, -1.1, -1.1)\n",
      "Steps 117785/250000 (47.114%) - Rewards: (-1.05, -1.05, -1, -1)\n",
      "Steps 117948/250000 (47.179%) - Rewards: (-1.05, -1.4, -0.050000000000000044, -0.4500000000000003)\n",
      "Steps 118131/250000 (47.252%) - Rewards: (-1.1, -0.20000000000000007, -1.05, -1)\n",
      "Steps 118294/250000 (47.318%) - Rewards: (-1, -1, -1.2, -1.1)\n",
      "Steps 118457/250000 (47.383%) - Rewards: (-1.15, -1, -1, -0.09999999999999998)\n",
      "Steps 118740/250000 (47.496%) - Rewards: (-1.05, 1.7000000000000002, -1.05, 1.25)\n",
      "Steps 118943/250000 (47.577%) - Rewards: (-1, -0.10000000000000009, -1, -0.09999999999999998)\n",
      "Steps 119106/250000 (47.642%) - Rewards: (-1.1, -0.050000000000000044, -0.20000000000000018, -1)\n",
      "Steps 119269/250000 (47.708%) - Rewards: (-0.09999999999999998, -1, -1, -1)\n",
      "Steps 119452/250000 (47.781%) - Rewards: (-0.050000000000000044, -1.1, -1.1, -1.15)\n",
      "Steps 119635/250000 (47.854%) - Rewards: (-0.10000000000000009, -1.15, -1.05, -0.20000000000000007)\n",
      "Steps 119798/250000 (47.919%) - Rewards: (-1, -1, -1.2, -1.05)\n",
      "Steps 119961/250000 (47.984%) - Rewards: (-1, -1.1, -1.05, -1)\n",
      "Steps 120144/250000 (48.058%) - Rewards: (-1.1, -1.05, -0.09999999999999998, -0.20000000000000007)\n",
      "Steps 120307/250000 (48.123%) - Rewards: (-1.1, -1, -1.05, -1)\n",
      "Steps 120470/250000 (48.188%) - Rewards: (-1.05, -1.15, -1, -0.050000000000000044)\n",
      "Steps 120633/250000 (48.253%) - Rewards: (-0.09999999999999998, -1, -1, -1.15)\n",
      "Steps 120796/250000 (48.318%) - Rewards: (-1, -0.3500000000000001, -1.15, -1.25)\n",
      "Steps 120959/250000 (48.384%) - Rewards: (-1.05, -1.2, -1.5, -1.05)\n",
      "Steps 121142/250000 (48.457%) - Rewards: (-0.15000000000000002, 0.6999999999999997, -1.3, -1.1)\n",
      "Steps 121305/250000 (48.522%) - Rewards: (-1, -0.09999999999999998, -1.05, -1.1)\n",
      "Steps 121488/250000 (48.595%) - Rewards: (-1.05, 1.4499999999999997, -1.05, -1.05)\n",
      "Steps 121671/250000 (48.668%) - Rewards: (-1, 0.7499999999999998, -1.05, -1.05)\n",
      "Steps 121834/250000 (48.734%) - Rewards: (-1.05, -1.2, -1.1, -1)\n",
      "Steps 121997/250000 (48.799%) - Rewards: (-1.1, 0.49999999999999933, -1.05, -1.25)\n",
      "Steps 122160/250000 (48.864%) - Rewards: (-1, -1.4, -1.1, -1.05)\n",
      "Steps 122323/250000 (48.929%) - Rewards: (-1.15, -1.5, -1.05, -1.15)\n",
      "Steps 122486/250000 (48.994%) - Rewards: (-1.05, 0.3999999999999997, -1.05, -0.050000000000000044)\n",
      "Steps 122689/250000 (49.076%) - Rewards: (-1, -1.900000000000001, 0.8999999999999999, -0.050000000000000044)\n",
      "Steps 122852/250000 (49.141%) - Rewards: (-1.15, -2.500000000000001, -1.05, -1.1)\n",
      "Steps 123015/250000 (49.206%) - Rewards: (-1.1, -1.6, -1, -1.2)\n",
      "Steps 123198/250000 (49.279%) - Rewards: (-1.05, -1.05, -0.15000000000000002, -0.3500000000000001)\n",
      "Steps 123381/250000 (49.352%) - Rewards: (-1.15, -0.8500000000000001, -1, -1.45)\n",
      "Steps 123544/250000 (49.418%) - Rewards: (-1.05, -1.1, -1.05, -1)\n",
      "Steps 123707/250000 (49.483%) - Rewards: (-1.25, -0.40000000000000013, -1, -1.1)\n",
      "Steps 123930/250000 (49.572%) - Rewards: (-1, 1.4, -1.1, -1.1)\n",
      "Steps 124093/250000 (49.637%) - Rewards: (-1.25, -0.3500000000000001, -1.05, -1.2)\n",
      "Steps 124276/250000 (49.71%) - Rewards: (-1.1, -1.05, -1.1, 0.4999999999999998)\n",
      "Steps 124439/250000 (49.776%) - Rewards: (-1.15, -1.15, -0.3999999999999999, -1.05)\n",
      "Steps 124602/250000 (49.841%) - Rewards: (-1.05, -0.3500000000000001, 1.7999999999999998, -1.1)\n",
      "Steps 124765/250000 (49.906%) - Rewards: (-1.05, -1.2, -1.05, -1)\n",
      "Steps 124928/250000 (49.971%) - Rewards: (-1.05, -1.6, -1.05, -1.1)\n",
      "Steps 125111/250000 (50.044%) - Rewards: (-1.1, -1.2, 0.7999999999999998, -0.09999999999999998)\n",
      "Steps 125274/250000 (50.11%) - Rewards: (-1.15, -1.35, -1.05, -1.1)\n",
      "Steps 125437/250000 (50.175%) - Rewards: (-1.05, -1.3, -1.1, -1.05)\n",
      "Steps 125600/250000 (50.24%) - Rewards: (-1.05, -0.09999999999999998, -0.09999999999999998, -1.1)\n",
      "Steps 125763/250000 (50.305%) - Rewards: (-1, 0.7, -1, -1.1)\n",
      "Steps 125926/250000 (50.37%) - Rewards: (-1.1, -1.05, -1.1, -1.05)\n",
      "Steps 126089/250000 (50.436%) - Rewards: (-1.15, -1, -1, -1.1)\n",
      "Steps 126252/250000 (50.501%) - Rewards: (-1.05, -1, -1.05, -1.2)\n",
      "Steps 126435/250000 (50.574%) - Rewards: (-1, -1.05, -0.09999999999999998, -1)\n",
      "Steps 126598/250000 (50.639%) - Rewards: (-1, -0.7, -0.15000000000000002, -1.1)\n",
      "Steps 126761/250000 (50.704%) - Rewards: (-0.09999999999999998, 0.1499999999999999, -1.05, -1.15)\n",
      "Steps 126924/250000 (50.77%) - Rewards: (-1.05, -1.55, -1.15, -1.05)\n",
      "Steps 127107/250000 (50.843%) - Rewards: (-1, -2.1000000000000005, -1, -0.20000000000000007)\n",
      "Steps 127290/250000 (50.916%) - Rewards: (-1.1, -0.5000000000000004, -1.05, -1.05)\n",
      "Steps 127453/250000 (50.981%) - Rewards: (-1.05, -1.5499999999999998, -1.15, -1.05)\n",
      "Steps 127696/250000 (51.078%) - Rewards: (-1.3, 0.5499999999999996, -1.15, -1.05)\n",
      "Steps 127859/250000 (51.144%) - Rewards: (-1, -1.1, -1.05, -1)\n",
      "Steps 128022/250000 (51.209%) - Rewards: (-1.05, -0.2500000000000002, -0.15000000000000002, -1)\n",
      "Steps 128185/250000 (51.274%) - Rewards: (-1.15, -1.1, -1.05, -1)\n",
      "Steps 128348/250000 (51.339%) - Rewards: (-1, -1, -1.05, -0.15000000000000002)\n",
      "Steps 128511/250000 (51.404%) - Rewards: (-1.05, -1.1, -0.09999999999999998, -1.05)\n",
      "Steps 128694/250000 (51.478%) - Rewards: (-1.2, 0.49999999999999956, -1.05, -0.050000000000000044)\n",
      "Steps 128877/250000 (51.551%) - Rewards: (-1.2, -0.3500000000000002, -1.15, -1.1)\n",
      "Steps 129040/250000 (51.616%) - Rewards: (-1, 0.75, -1.05, -1.05)\n",
      "Steps 129243/250000 (51.697%) - Rewards: (-1.1, 0.24999999999999978, -0.25, -1.1)\n",
      "Steps 129426/250000 (51.77%) - Rewards: (-0.20000000000000007, -0.75, -1.1, -1.05)\n",
      "Steps 129649/250000 (51.86%) - Rewards: (-1.05, -1.45, -0.050000000000000044, -0.15000000000000002)\n",
      "Steps 129812/250000 (51.925%) - Rewards: (-1.05, -1, -1, -1.05)\n",
      "Steps 129995/250000 (51.998%) - Rewards: (0.6499999999999997, -1.25, -1, -1.3)\n",
      "Steps 130178/250000 (52.071%) - Rewards: (-1.2, -0.7, -0.3500000000000001, -1.05)\n",
      "Steps 130361/250000 (52.144%) - Rewards: (-0.44999999999999996, -0.75, -0.7000000000000002, -1.3)\n",
      "Steps 130524/250000 (52.21%) - Rewards: (-1.05, -1.1, -1.05, -1.15)\n",
      "Steps 130707/250000 (52.283%) - Rewards: (-1.15, -1.3, -1.0000000000000004, -1)\n",
      "Steps 130870/250000 (52.348%) - Rewards: (0.75, -1.15, -0.15000000000000002, -1.3)\n",
      "Steps 131053/250000 (52.421%) - Rewards: (-1.5, -0.40000000000000024, -1.1, -0.30000000000000004)\n",
      "Steps 131216/250000 (52.486%) - Rewards: (-1.2, -1.15, -0.19999999999999996, -1.35)\n",
      "Steps 131379/250000 (52.552%) - Rewards: (-1.85, -1.15, -1.15, -1.05)\n",
      "Steps 131562/250000 (52.625%) - Rewards: (-0.3500000000000001, -1.4000000000000004, 0.6999999999999997, -1.05)\n",
      "Steps 131745/250000 (52.698%) - Rewards: (0.5999999999999999, -1.15, -1.1, -1.3)\n",
      "Steps 131928/250000 (52.771%) - Rewards: (-1.35, -1.15, -0.6500000000000001, -1)\n",
      "Steps 132091/250000 (52.836%) - Rewards: (-1.15, -1.2, -1.15, -1.05)\n",
      "Steps 132254/250000 (52.902%) - Rewards: (-1.35, -1.25, -1.05, -1.1)\n",
      "Steps 132417/250000 (52.967%) - Rewards: (-1.15, -1.15, -1.15, -1.2)\n",
      "Steps 132580/250000 (53.032%) - Rewards: (-1.4, -1.45, -1, -1.1)\n",
      "Steps 132743/250000 (53.097%) - Rewards: (-0.3999999999999999, -0.30000000000000004, -0.20000000000000007, -1.05)\n",
      "Steps 132906/250000 (53.162%) - Rewards: (-1.15, -1.1, -1.5, -1.25)\n",
      "Steps 133069/250000 (53.228%) - Rewards: (-1.25, -1.2, -1.6, -1.05)\n",
      "Steps 133232/250000 (53.293%) - Rewards: (-1.05, -1.05, -1.5, -1.1)\n",
      "Steps 133395/250000 (53.358%) - Rewards: (-1.05, -1.3, 2.7, -1)\n",
      "Steps 133558/250000 (53.423%) - Rewards: (-1.05, -1.4, -0.15000000000000002, -1)\n",
      "Steps 133721/250000 (53.488%) - Rewards: (-1, -1.15, -1, -1)\n",
      "Steps 133884/250000 (53.554%) - Rewards: (-1.1, -1.15, -1.25, -1)\n",
      "Steps 134127/250000 (53.651%) - Rewards: (-0.40000000000000036, 0.7999999999999998, -0.09999999999999998, -1.25)\n",
      "Steps 134310/250000 (53.724%) - Rewards: (-1.3, -0.20000000000000007, -1.05, -1.05)\n",
      "Steps 134533/250000 (53.813%) - Rewards: (-1.45, 0.8, 0.30000000000000004, -1)\n",
      "Steps 134736/250000 (53.894%) - Rewards: (0.6499999999999999, -1.15, -1.05, -1.05)\n",
      "Steps 134919/250000 (53.968%) - Rewards: (-0.3500000000000001, -1.1, -1.05, -1.05)\n",
      "Steps 135082/250000 (54.033%) - Rewards: (-1.2, -1, -1.05, -1)\n",
      "Steps 135245/250000 (54.098%) - Rewards: (-1.15, -1.05, -1.05, -1.05)\n",
      "Steps 135428/250000 (54.171%) - Rewards: (-1.1, -0.19999999999999996, -1.05, -1.15)\n",
      "Steps 135611/250000 (54.244%) - Rewards: (-1.5, -1, -1.05, -0.09999999999999998)\n",
      "Steps 135774/250000 (54.31%) - Rewards: (-1.05, -1, -1.2, -1.1)\n",
      "Steps 135957/250000 (54.383%) - Rewards: (-1.15, -0.3999999999999999, -1, -1.1)\n",
      "Steps 136140/250000 (54.456%) - Rewards: (-1.05, -0.09999999999999998, -1.05, -1)\n",
      "Steps 136323/250000 (54.529%) - Rewards: (-1.1, -1.05, 0.8499999999999999, -1.15)\n",
      "Steps 136486/250000 (54.594%) - Rewards: (-1, -1, -1, -1.05)\n",
      "Steps 136649/250000 (54.66%) - Rewards: (-1, -1.1, -1.15, -1.25)\n",
      "Steps 136812/250000 (54.725%) - Rewards: (-1.1, -1.05, -1.15, -1.15)\n",
      "Steps 136975/250000 (54.79%) - Rewards: (-1.05, -1, -1.2, -1.05)\n",
      "Steps 137158/250000 (54.863%) - Rewards: (-1.05, -1.1, -1.2, -1.05)\n",
      "Steps 137321/250000 (54.928%) - Rewards: (-1.1, -1.25, -1.15, -1)\n",
      "Steps 137504/250000 (55.002%) - Rewards: (-1.05, -2.1500000000000004, -0.30000000000000004, -1.05)\n",
      "Steps 137667/250000 (55.067%) - Rewards: (-1.15, -1.1, -1, -1)\n",
      "Steps 137830/250000 (55.132%) - Rewards: (-1, -1.05, -1.1, -1)\n",
      "Steps 137993/250000 (55.197%) - Rewards: (-1.05, -1, -0.050000000000000044, -1.1)\n",
      "Steps 138156/250000 (55.262%) - Rewards: (-1.2, -0.15000000000000002, -1.05, -1.05)\n",
      "Steps 138319/250000 (55.328%) - Rewards: (-1.1, -1.25, -1, -1.1)\n",
      "Steps 138482/250000 (55.393%) - Rewards: (-1.15, -1.15, -1, -1.05)\n",
      "Steps 138685/250000 (55.474%) - Rewards: (-1.1500000000000001, 0.5999999999999996, -1.15, -1.25)\n",
      "Steps 138848/250000 (55.539%) - Rewards: (-1.05, -1.3, -1.1, -1.1)\n",
      "Steps 139031/250000 (55.612%) - Rewards: (-1.15, -1.15, -0.3999999999999999, -1.1)\n",
      "Steps 139194/250000 (55.678%) - Rewards: (-1.05, -1.15, -1, -1.05)\n",
      "Steps 139357/250000 (55.743%) - Rewards: (-0.20000000000000007, -1.05, -1.1, -1)\n",
      "Steps 139520/250000 (55.808%) - Rewards: (-1, -1.1, -1.05, -1.05)\n",
      "Steps 139683/250000 (55.873%) - Rewards: (-1.05, -1.1, -1.1, -1.15)\n",
      "Steps 139866/250000 (55.946%) - Rewards: (-1, -1.15, -0.2500000000000001, -1.05)\n",
      "Steps 140069/250000 (56.028%) - Rewards: (-1, 0.3500000000000001, -1.1, -1.05)\n",
      "Steps 140232/250000 (56.093%) - Rewards: (-1, -1.15, -0.050000000000000044, -1)\n",
      "Steps 140395/250000 (56.158%) - Rewards: (-1, -1.05, -1.05, -1)\n",
      "Steps 140558/250000 (56.223%) - Rewards: (-1.05, -1.05, -1.1, -1)\n",
      "Steps 140721/250000 (56.288%) - Rewards: (-1, -1.1, -1.05, -1)\n",
      "Steps 140884/250000 (56.354%) - Rewards: (-1, -1.05, -1, -1.1)\n",
      "Steps 141047/250000 (56.419%) - Rewards: (-1.15, -1.1, -0.5499999999999999, -1.1)\n",
      "Steps 141210/250000 (56.484%) - Rewards: (-1, -1.1, -1.1, -1.05)\n",
      "Steps 141373/250000 (56.549%) - Rewards: (-1.05, -1.05, -1.1, -1)\n",
      "Steps 141536/250000 (56.614%) - Rewards: (-1, -1.05, -0.09999999999999998, -1.05)\n",
      "Steps 141699/250000 (56.68%) - Rewards: (-1.1, -1.05, -1, -1.1)\n",
      "Steps 141862/250000 (56.745%) - Rewards: (-1.2, -1.25, -1, -0.09999999999999998)\n",
      "Steps 142025/250000 (56.81%) - Rewards: (-1.05, -1.1, -1, -1.1)\n",
      "Steps 142188/250000 (56.875%) - Rewards: (-1.1, -1.05, -1.05, -1)\n",
      "Steps 142371/250000 (56.948%) - Rewards: (-1.1, -0.19999999999999996, -1, -1.05)\n",
      "Steps 142574/250000 (57.03%) - Rewards: (-0.15000000000000013, -0.10000000000000009, -1, -1.1)\n",
      "Steps 142757/250000 (57.103%) - Rewards: (-1.05, -1, -0.15000000000000002, -1.05)\n",
      "Steps 142901/250000 (57.16%) - Rewards: (-1.05, -1, -1, -1.05)\n",
      "Steps 143084/250000 (57.234%) - Rewards: (-1, -1.15, -1.05, -0.09999999999999998)\n",
      "Steps 143247/250000 (57.299%) - Rewards: (-1.15, -1.15, -1, -1.05)\n",
      "Steps 143410/250000 (57.364%) - Rewards: (-1.1, -1.1, -1, -1.1)\n",
      "Steps 143613/250000 (57.445%) - Rewards: (-1.1, -1.4, -0.20000000000000007, -0.5500000000000004)\n",
      "Steps 143796/250000 (57.518%) - Rewards: (-1.05, -0.10000000000000009, -0.050000000000000044, -1.4)\n",
      "Steps 143959/250000 (57.584%) - Rewards: (-1.15, -1.1, -1, -1)\n",
      "Steps 144122/250000 (57.649%) - Rewards: (-1.1, -1.1, -1.05, -1.05)\n",
      "Steps 144285/250000 (57.714%) - Rewards: (-1, -1, -1.05, -1.05)\n",
      "Steps 144468/250000 (57.787%) - Rewards: (-1, -1.2, -1.1, -0.050000000000000044)\n",
      "Steps 144631/250000 (57.852%) - Rewards: (-1.1, -1, -1.1, -0.10000000000000009)\n",
      "Steps 144814/250000 (57.926%) - Rewards: (-1, -1.15, -1.1, -0.15000000000000002)\n",
      "Steps 144977/250000 (57.991%) - Rewards: (-1, -1.9000000000000004, -1.05, -1)\n",
      "Steps 145140/250000 (58.056%) - Rewards: (-1, -1.3, -0.15000000000000002, -1)\n",
      "Steps 145303/250000 (58.121%) - Rewards: (-1, -1.35, -1.15, -1.1)\n",
      "Steps 145506/250000 (58.202%) - Rewards: (-0.5, -1.35, 0.8999999999999999, -0.15000000000000002)\n",
      "Steps 145689/250000 (58.276%) - Rewards: (-1.2, -1, -1.05, -0.10000000000000009)\n",
      "Steps 145852/250000 (58.341%) - Rewards: (-0.050000000000000044, -1.05, -1.05, -1.05)\n",
      "Steps 146015/250000 (58.406%) - Rewards: (-1.15, -1.1, -0.15000000000000002, -1.05)\n",
      "Steps 146178/250000 (58.471%) - Rewards: (-1.2, -1.05, -1.05, -1.05)\n",
      "Steps 146341/250000 (58.536%) - Rewards: (-0.30000000000000004, -1.1, -1, -1.1)\n",
      "Steps 146504/250000 (58.602%) - Rewards: (-1.25, -1.05, -1.15, -1.05)\n",
      "Steps 146667/250000 (58.667%) - Rewards: (-1.15, -1.05, -1.1, -1)\n",
      "Steps 146890/250000 (58.756%) - Rewards: (0.5999999999999999, -0.5, -1.15, -1.15)\n",
      "Steps 147053/250000 (58.821%) - Rewards: (0.34999999999999987, -1, -1, -1.1)\n",
      "Steps 147216/250000 (58.886%) - Rewards: (-1.1, -1.1, -0.19999999999999996, -1.45)\n",
      "Steps 147399/250000 (58.96%) - Rewards: (-1.6500000000000004, -1.05, -1.05, -0.050000000000000044)\n",
      "Steps 147562/250000 (59.025%) - Rewards: (-1.15, -1.15, -1.05, -1.05)\n",
      "Steps 147745/250000 (59.098%) - Rewards: (-0.6000000000000002, -1, -1.15, -1)\n",
      "Steps 147928/250000 (59.171%) - Rewards: (-0.25, -1.1, -1.1, -1.05)\n",
      "Steps 148091/250000 (59.236%) - Rewards: (-1.1, -1.05, -1.05, -1)\n",
      "Steps 148254/250000 (59.302%) - Rewards: (-1.15, -0.2500000000000001, -0.19999999999999996, -1)\n",
      "Steps 148417/250000 (59.367%) - Rewards: (-1.3, -1.05, -1.05, -1.1)\n",
      "Steps 148580/250000 (59.432%) - Rewards: (-1.3, -1.05, -1.05, -1.15)\n",
      "Steps 148763/250000 (59.505%) - Rewards: (-0.44999999999999996, -1.05, -1, -1.25)\n",
      "Steps 148946/250000 (59.578%) - Rewards: (-1.35, -1, -0.25, -1.05)\n",
      "Steps 149109/250000 (59.644%) - Rewards: (-1.15, -1.05, -1.25, -1.1)\n",
      "Steps 149272/250000 (59.709%) - Rewards: (-1.1, -1.1, -0.10000000000000009, -0.09999999999999998)\n",
      "Steps 149450/250000 (59.78%) - Rewards: (-0.09999999999999998, -1, -1.05, -1.1)\n",
      "Steps 149633/250000 (59.853%) - Rewards: (-1.25, -1.05, -0.15000000000000002, -1.05)\n",
      "Steps 149796/250000 (59.918%) - Rewards: (-1.05, -1.3, -1.1, -1.05)\n",
      "Steps 149959/250000 (59.984%) - Rewards: (-1.1, -1.1, -1.1, -1.15)\n",
      "Steps 150122/250000 (60.049%) - Rewards: (-0.6, -1.05, -0.050000000000000044, -1)\n",
      "Steps 150285/250000 (60.114%) - Rewards: (-1.8000000000000003, -1.1, -1.05, -1)\n",
      "Steps 150448/250000 (60.179%) - Rewards: (-0.09999999999999998, -1.15, -1.1, -0.15000000000000002)\n",
      "Steps 150651/250000 (60.26%) - Rewards: (-1.2, -0.10000000000000009, -1.1, 0.8500000000000001)\n",
      "Steps 150814/250000 (60.326%) - Rewards: (-1, -1.05, -1.15, -1)\n",
      "Steps 151017/250000 (60.407%) - Rewards: (-1.2, -1.1, -0.5500000000000003, -0.2500000000000001)\n",
      "Steps 151180/250000 (60.472%) - Rewards: (-0.19999999999999996, -1, -1.05, -0.050000000000000044)\n",
      "Steps 151343/250000 (60.537%) - Rewards: (-0.09999999999999998, -1.05, -1.1, -1.05)\n",
      "Steps 151506/250000 (60.602%) - Rewards: (-1.05, -1, -1, -1.05)\n",
      "Steps 151689/250000 (60.676%) - Rewards: (-1.2, -1.05, -0.050000000000000044, -1)\n",
      "Steps 151852/250000 (60.741%) - Rewards: (-0.09999999999999998, -1, -1.05, -1.05)\n",
      "Steps 152015/250000 (60.806%) - Rewards: (-1.15, -0.15000000000000002, -1.1, -1)\n",
      "Steps 152178/250000 (60.871%) - Rewards: (-1.1, -1.05, -1, -1.05)\n",
      "Steps 152341/250000 (60.936%) - Rewards: (-1, -1.75, -1.05, -1.5)\n",
      "Steps 152504/250000 (61.002%) - Rewards: (-1.15, -1.05, -0.09999999999999998, -0.09999999999999998)\n",
      "Steps 152667/250000 (61.067%) - Rewards: (-1.05, -1.05, -1.05, -1.25)\n",
      "Steps 152830/250000 (61.132%) - Rewards: (-1.1, -1, -1.05, -1.15)\n",
      "Steps 153033/250000 (61.213%) - Rewards: (-1.1, -0.050000000000000044, -1.1, -1.15)\n",
      "Steps 153196/250000 (61.278%) - Rewards: (-0.15000000000000002, -1.15, -1.1, -1.2)\n",
      "Steps 153419/250000 (61.368%) - Rewards: (-0.20000000000000007, -1, -1.65, -0.09999999999999998)\n",
      "Steps 153582/250000 (61.433%) - Rewards: (-1.1, -1, -0.20000000000000007, -1)\n",
      "Steps 153745/250000 (61.498%) - Rewards: (-1.05, -1, -1, -1)\n",
      "Steps 153908/250000 (61.563%) - Rewards: (-1.1, -1, -1.1, -1)\n",
      "Steps 154071/250000 (61.628%) - Rewards: (-1.05, -1.05, -1.25, -1.2)\n",
      "Steps 154234/250000 (61.694%) - Rewards: (-1.25, -1.15, -0.10000000000000009, -1.1)\n",
      "Steps 154397/250000 (61.759%) - Rewards: (-1.05, -1.05, -1.45, -1.35)\n",
      "Steps 154560/250000 (61.824%) - Rewards: (-1, -1.1, -1.05, -1.05)\n",
      "Steps 154743/250000 (61.897%) - Rewards: (-1.15, -0.050000000000000044, -1.05, 0.4999999999999998)\n",
      "Steps 154906/250000 (61.962%) - Rewards: (-1, -1, -0.25, -1)\n",
      "Steps 155069/250000 (62.028%) - Rewards: (-1.1, -1.1, -1, -1.05)\n",
      "Steps 155232/250000 (62.093%) - Rewards: (-0.15000000000000002, -1.05, -1.05, -1)\n",
      "Steps 155395/250000 (62.158%) - Rewards: (-1, -1.05, -1.05, -0.09999999999999998)\n",
      "Steps 155558/250000 (62.223%) - Rewards: (-0.050000000000000044, -0.050000000000000044, -1.05, -1)\n",
      "Steps 155761/250000 (62.304%) - Rewards: (-1.05, 0.8499999999999999, -1.05, -1.1)\n",
      "Steps 155924/250000 (62.37%) - Rewards: (-0.25, -1.05, -1, -1.05)\n",
      "Steps 156107/250000 (62.443%) - Rewards: (-0.09999999999999998, 0.5, -1, -1.05)\n",
      "Steps 156310/250000 (62.524%) - Rewards: (-0.25, -1.15, -1, -0.44999999999999996)\n",
      "Steps 156473/250000 (62.589%) - Rewards: (-1, -1.05, -1.1, -1)\n",
      "Steps 156636/250000 (62.654%) - Rewards: (-1.05, -0.050000000000000044, -1.1, -1.1)\n",
      "Steps 156819/250000 (62.728%) - Rewards: (-0.09999999999999998, -1, -1.15, -1.1)\n",
      "Steps 156982/250000 (62.793%) - Rewards: (-1.05, -1.1, -1, -1)\n",
      "Steps 157165/250000 (62.866%) - Rewards: (-1.15, -1.2, -0.19999999999999996, -1.05)\n",
      "Steps 157328/250000 (62.931%) - Rewards: (-1, -1.4, -1.2, -1)\n",
      "Steps 157491/250000 (62.996%) - Rewards: (-1, -1.05, -1, -0.7000000000000002)\n",
      "Steps 157654/250000 (63.062%) - Rewards: (-1, -0.09999999999999998, -1.1, -1.05)\n",
      "Steps 157817/250000 (63.127%) - Rewards: (-1.05, -1.05, -1, -0.15000000000000002)\n",
      "Steps 157980/250000 (63.192%) - Rewards: (-1.1, -1.35, -0.10000000000000009, -1)\n",
      "Steps 158143/250000 (63.257%) - Rewards: (-0.3500000000000001, -1.2, -1.05, -1.15)\n",
      "Steps 158306/250000 (63.322%) - Rewards: (-1.05, -1.1, -1.05, -1.1)\n",
      "Steps 158529/250000 (63.412%) - Rewards: (0.7999999999999998, -1.15, -1, 0.8500000000000001)\n",
      "Steps 158712/250000 (63.485%) - Rewards: (-0.050000000000000044, -0.20000000000000007, -1.15, -1.1)\n",
      "Steps 158875/250000 (63.55%) - Rewards: (-1, -1, -1.05, -1.05)\n",
      "Steps 159058/250000 (63.623%) - Rewards: (-1.05, -1.25, -1.1, -0.7500000000000001)\n",
      "Steps 159221/250000 (63.688%) - Rewards: (-1, -1.1, -1.05, -1)\n",
      "Steps 159404/250000 (63.762%) - Rewards: (-1.05, -1, -1.1, 0.8499999999999999)\n",
      "Steps 159587/250000 (63.835%) - Rewards: (-1.1, -1.1, -1.1, -0.10000000000000009)\n",
      "Steps 159750/250000 (63.9%) - Rewards: (-1, -1.05, -1.15, -1)\n",
      "Steps 159973/250000 (63.989%) - Rewards: (-0.3500000000000001, -1.35, -0.2500000000000001, 0.8999999999999999)\n",
      "Steps 160156/250000 (64.062%) - Rewards: (-1.15, -1.1, -0.15000000000000002, -1)\n",
      "Steps 160319/250000 (64.128%) - Rewards: (-1, -1.05, -0.09999999999999998, -1.05)\n",
      "Steps 160482/250000 (64.193%) - Rewards: (-2.0, -1.1, -1.3, -1.15)\n",
      "Steps 160665/250000 (64.266%) - Rewards: (-0.09999999999999998, -1.2, -1, -1.05)\n",
      "Steps 160828/250000 (64.331%) - Rewards: (-1.05, -1.05, -1.05, -1.25)\n",
      "Steps 160991/250000 (64.396%) - Rewards: (-1.15, -1.15, -1.1, -1)\n",
      "Steps 161154/250000 (64.462%) - Rewards: (-0.6000000000000001, -1, -1.15, -1.2)\n",
      "Steps 161317/250000 (64.527%) - Rewards: (-1, -1.05, -1.05, -1.05)\n",
      "Steps 161500/250000 (64.6%) - Rewards: (-1.15, -0.050000000000000044, -1.05, -1)\n",
      "Steps 161663/250000 (64.665%) - Rewards: (-1.2, -0.09999999999999998, -1.05, -1.05)\n",
      "Steps 161826/250000 (64.73%) - Rewards: (-1.1, -1.1, -1.05, -1)\n",
      "Steps 161989/250000 (64.796%) - Rewards: (-1.05, -1.1500000000000001, -1.05, -1.05)\n",
      "Steps 162152/250000 (64.861%) - Rewards: (-1.25, -1.1, -1.15, -0.09999999999999998)\n",
      "Steps 162375/250000 (64.95%) - Rewards: (-0.15000000000000013, -0.10000000000000009, -0.050000000000000044, -1.1)\n",
      "Steps 162538/250000 (65.015%) - Rewards: (-1, -1, -1.1, -1.1)\n",
      "Steps 162701/250000 (65.08%) - Rewards: (-1.1, -1.1, -1.05, -1)\n",
      "Steps 162924/250000 (65.17%) - Rewards: (0.8999999999999999, -1.15, -1.05, 0.7999999999999998)\n",
      "Steps 163087/250000 (65.235%) - Rewards: (-1.05, -1.05, -1.05, -1.1)\n",
      "Steps 163250/250000 (65.3%) - Rewards: (-1.05, -1, -1.15, -1)\n",
      "Steps 163433/250000 (65.373%) - Rewards: (-1, -1.05, -0.15000000000000002, -1.05)\n",
      "Steps 163616/250000 (65.446%) - Rewards: (-1, -1, -1.1, -0.40000000000000013)\n",
      "Steps 163799/250000 (65.52%) - Rewards: (-0.30000000000000004, -1.1, -1, 0.6999999999999997)\n",
      "Steps 163982/250000 (65.593%) - Rewards: (-1.2, -1.75, -1.05, -0.30000000000000004)\n",
      "Steps 164145/250000 (65.658%) - Rewards: (-0.3500000000000002, -1.15, -1.05, -1.25)\n",
      "Steps 164328/250000 (65.731%) - Rewards: (-1.05, -1, -0.050000000000000044, -1.3)\n",
      "Steps 164491/250000 (65.796%) - Rewards: (-1.1, -1.05, -1.05, -1.15)\n",
      "Steps 164674/250000 (65.87%) - Rewards: (-1.05, -1.05, -1.05, -0.25)\n",
      "Steps 164857/250000 (65.943%) - Rewards: (-0.20000000000000007, -1.05, -1.25, -1.1)\n",
      "Steps 165040/250000 (66.016%) - Rewards: (-0.10000000000000009, -1.05, -1.05, -0.7)\n",
      "Steps 165203/250000 (66.081%) - Rewards: (-1.1, -1.05, -1.15, -1.05)\n",
      "Steps 165366/250000 (66.146%) - Rewards: (-1.25, -1.05, -1.1, -1.1)\n",
      "Steps 165529/250000 (66.212%) - Rewards: (-1.5000000000000007, -1.35, -1, -1.05)\n",
      "Steps 165692/250000 (66.277%) - Rewards: (-1.1, -1.05, -1, -0.2500000000000001)\n",
      "Steps 165855/250000 (66.342%) - Rewards: (-1.05, -1.05, -1, -1.2)\n",
      "Steps 166038/250000 (66.415%) - Rewards: (-1.1, -1.1, -1.1, -0.3500000000000001)\n",
      "Steps 166241/250000 (66.496%) - Rewards: (-1.15, -1.05, -1, 0.7499999999999998)\n",
      "Steps 166404/250000 (66.562%) - Rewards: (-1, -1.05, -1, -1.1)\n",
      "Steps 166567/250000 (66.627%) - Rewards: (-1.2, -1, -1.05, -1.05)\n",
      "Steps 166730/250000 (66.692%) - Rewards: (-1.05, -1, -1.1, -1.15)\n",
      "Steps 166893/250000 (66.757%) - Rewards: (-1.15, -1.05, -1, -1.4)\n",
      "Steps 167056/250000 (66.822%) - Rewards: (-1.1, -1.15, -1.1, -1.05)\n",
      "Steps 167219/250000 (66.888%) - Rewards: (-1.05, -1.1, -1.05, -1.05)\n",
      "Steps 167382/250000 (66.953%) - Rewards: (-1.1, -1, -1.1, -1.1)\n",
      "Steps 167565/250000 (67.026%) - Rewards: (-1.05, -1.15, -0.050000000000000044, -1.05)\n",
      "Steps 167728/250000 (67.091%) - Rewards: (-1.15, -1.05, -1.05, -1.05)\n",
      "Steps 167891/250000 (67.156%) - Rewards: (-1.1, -1.05, -1.15, -0.09999999999999998)\n",
      "Steps 168074/250000 (67.23%) - Rewards: (-1.1, -1.2, -1, -0.3999999999999999)\n",
      "Steps 168237/250000 (67.295%) - Rewards: (-0.10000000000000009, -1.25, -1.1, -1.1)\n",
      "Steps 168400/250000 (67.36%) - Rewards: (-1.05, -1.1, -1.05, -1.05)\n",
      "Steps 168563/250000 (67.425%) - Rewards: (-1.15, -1.15, -1, -1)\n",
      "Steps 168726/250000 (67.49%) - Rewards: (-1.1, -1.15, -1, -1)\n",
      "Steps 168889/250000 (67.556%) - Rewards: (-1.05, -0.19999999999999996, -1.2, -1.1500000000000001)\n",
      "Steps 169052/250000 (67.621%) - Rewards: (-1.1, -1.2, -1.05, -1.1)\n",
      "Steps 169215/250000 (67.686%) - Rewards: (-1, -1, -1, -1)\n",
      "Steps 169398/250000 (67.759%) - Rewards: (-0.19999999999999996, -1, -1.15, -1.2)\n",
      "Steps 169581/250000 (67.832%) - Rewards: (-0.3500000000000002, -1.05, -1.05, -0.6500000000000002)\n",
      "Steps 169744/250000 (67.898%) - Rewards: (-1, -1.05, -1.05, -1.1)\n",
      "Steps 169907/250000 (67.963%) - Rewards: (-1.05, -1.15, -1.1, -1.1)\n",
      "Steps 170090/250000 (68.036%) - Rewards: (1.7999999999999998, -1.1, -1.1, -1.1)\n",
      "Steps 170253/250000 (68.101%) - Rewards: (-1.2, -1.05, -0.10000000000000009, -1.2)\n",
      "Steps 170416/250000 (68.166%) - Rewards: (-1.1, -1.1, -1.05, -1.05)\n",
      "Steps 170579/250000 (68.232%) - Rewards: (-1.05, -1.05, -1.1, -1.2)\n",
      "Steps 170742/250000 (68.297%) - Rewards: (-1.25, -1.05, -1.05, -0.4)\n",
      "Steps 170905/250000 (68.362%) - Rewards: (-0.050000000000000044, -1.05, -1, -1.1)\n",
      "Steps 171088/250000 (68.435%) - Rewards: (-1.2, -0.050000000000000044, -1, -0.15000000000000002)\n",
      "Steps 171271/250000 (68.508%) - Rewards: (-1.05, -1.3, -1.1, -1.1)\n",
      "Steps 171454/250000 (68.582%) - Rewards: (-0.35, -1.1, -1, -1.15)\n",
      "Steps 171617/250000 (68.647%) - Rewards: (-1.05, -1.05, -1, -1.1)\n",
      "Steps 171800/250000 (68.72%) - Rewards: (-0.8500000000000001, -1.1, -1.15, -1.3)\n",
      "Steps 171983/250000 (68.793%) - Rewards: (-0.15000000000000002, -1.1, -1.1, -0.40000000000000013)\n",
      "Steps 172146/250000 (68.858%) - Rewards: (-1.1500000000000001, -1, -1, -1.25)\n",
      "Steps 172309/250000 (68.924%) - Rewards: (-1.3, -1, -1.1, -1.05)\n",
      "Steps 172492/250000 (68.997%) - Rewards: (-1.15, -1.2, -0.8499999999999999, -1.9000000000000004)\n",
      "Steps 172655/250000 (69.062%) - Rewards: (-1.1, -2.0, -1, -1)\n",
      "Steps 172838/250000 (69.135%) - Rewards: (-1.2, -1.1, -1.05, -0.15000000000000002)\n",
      "Steps 173001/250000 (69.2%) - Rewards: (-1, -1.15, -1.15, -1.25)\n",
      "Steps 173184/250000 (69.274%) - Rewards: (-0.09999999999999998, -1.05, -2.750000000000001, -2.1000000000000005)\n",
      "Steps 173347/250000 (69.339%) - Rewards: (-0.25, -1.1, -1.15, -1.1)\n",
      "Steps 173510/250000 (69.404%) - Rewards: (-1, -1.1, -1.2, -1.35)\n",
      "Steps 173693/250000 (69.477%) - Rewards: (-0.30000000000000016, -0.09999999999999998, -1, -0.19999999999999996)\n",
      "Steps 173856/250000 (69.542%) - Rewards: (-1.05, -1.05, -1.85, -1.05)\n",
      "Steps 174019/250000 (69.608%) - Rewards: (-1.05, -1.05, -1.05, -1.1)\n",
      "Steps 174182/250000 (69.673%) - Rewards: (-1.05, -1.1, -1.1, -0.050000000000000044)\n",
      "Steps 174365/250000 (69.746%) - Rewards: (-1.15, -1.2, -0.10000000000000009, -1.1)\n",
      "Steps 174528/250000 (69.811%) - Rewards: (-0.050000000000000044, -1.05, -1.05, -1.05)\n",
      "Steps 174731/250000 (69.892%) - Rewards: (-0.15000000000000002, -1, -1.1, -0.050000000000000044)\n",
      "Steps 174894/250000 (69.958%) - Rewards: (-1.05, -1.05, -1.3, -0.10000000000000009)\n",
      "Steps 175057/250000 (70.023%) - Rewards: (-1.05, -1, -1.1, -1.05)\n",
      "Steps 175260/250000 (70.104%) - Rewards: (-1.25, -1.1, 1.6999999999999997, -1.1)\n",
      "Steps 175423/250000 (70.169%) - Rewards: (-1.05, -1, -1.15, -1)\n",
      "Steps 175586/250000 (70.234%) - Rewards: (-1, -1.05, -1.05, -1)\n",
      "Steps 175769/250000 (70.308%) - Rewards: (-1.2, -1, -1, 0.7999999999999998)\n",
      "Steps 175932/250000 (70.373%) - Rewards: (-1, -1.05, -1.2, -1.1)\n",
      "Steps 176095/250000 (70.438%) - Rewards: (-1.05, -1.2, -1.05, -1.1)\n",
      "Steps 176278/250000 (70.511%) - Rewards: (-1, -1, -0.6500000000000001, -0.09999999999999998)\n",
      "Steps 176461/250000 (70.584%) - Rewards: (-1.05, -1, -0.15000000000000002, -1.1)\n",
      "Steps 176624/250000 (70.65%) - Rewards: (-1.2000000000000002, -1.15, -1.05, -1.05)\n",
      "Steps 176787/250000 (70.715%) - Rewards: (-1.05, -1.1, -1.05, -1.1)\n",
      "Steps 176950/250000 (70.78%) - Rewards: (-1.2, -1, -1.1, -1.2)\n",
      "Steps 177113/250000 (70.845%) - Rewards: (-1.1, -1.05, -0.2500000000000001, -1.05)\n",
      "Steps 177276/250000 (70.91%) - Rewards: (-1.05, -1, -1.1, -1.05)\n",
      "Steps 177439/250000 (70.976%) - Rewards: (-1.1, -1.1, -1.1, -1.1)\n",
      "Steps 177642/250000 (71.057%) - Rewards: (-1.05, -1.1, -1.25, -0.10000000000000009)\n",
      "Steps 177825/250000 (71.13%) - Rewards: (-1.25, -1, -0.40000000000000013, -1.05)\n",
      "Steps 177988/250000 (71.195%) - Rewards: (-1.1, -1.15, -1.2, -1.05)\n",
      "Steps 178191/250000 (71.276%) - Rewards: (-0.050000000000000044, -1.15, -0.19999999999999996, -1)\n",
      "Steps 178354/250000 (71.342%) - Rewards: (-1, -1.05, -1, -1.05)\n",
      "Steps 178517/250000 (71.407%) - Rewards: (-1.25, -1, -1.15, -1.05)\n",
      "Steps 178680/250000 (71.472%) - Rewards: (-1.05, -1.3, -1.4, -1.05)\n",
      "Steps 178883/250000 (71.553%) - Rewards: (-0.30000000000000004, -0.050000000000000044, -1, -1.05)\n",
      "Steps 179066/250000 (71.626%) - Rewards: (-0.050000000000000044, -1.2, -0.050000000000000044, -1.05)\n",
      "Steps 179229/250000 (71.692%) - Rewards: (-1.1, -1, -1, -1.1)\n",
      "Steps 179392/250000 (71.757%) - Rewards: (-1.4, -1.05, -1.1, -1.15)\n",
      "Steps 179575/250000 (71.83%) - Rewards: (-1.9500000000000002, -1, -1.1, -1.1)\n",
      "Steps 179758/250000 (71.903%) - Rewards: (-0.09999999999999998, -1.15, -1.7000000000000002, -1.05)\n",
      "Steps 179941/250000 (71.976%) - Rewards: (-0.2500000000000001, -1.25, -1.1, -1.1)\n",
      "Steps 180104/250000 (72.042%) - Rewards: (-1.3, -1.2, -1.15, -1.1)\n",
      "Steps 180267/250000 (72.107%) - Rewards: (-1.45, -1, -0.3500000000000001, -1)\n",
      "Steps 180430/250000 (72.172%) - Rewards: (-1.2, -0.050000000000000044, -1.15, -1)\n",
      "Steps 180613/250000 (72.245%) - Rewards: (-1.1, -1.05, -1.25, -1)\n",
      "Steps 180776/250000 (72.31%) - Rewards: (-0.20000000000000007, -1.1, -1, -1)\n",
      "Steps 180959/250000 (72.384%) - Rewards: (-0.10000000000000009, -1.15, -0.050000000000000044, -0.55)\n",
      "Steps 181122/250000 (72.449%) - Rewards: (-1, -1, -1.1, -1.05)\n",
      "Steps 181305/250000 (72.522%) - Rewards: (-1.3, -1.1, -0.050000000000000044, -0.09999999999999998)\n",
      "Steps 181468/250000 (72.587%) - Rewards: (-1.4, -1.1, -2.1500000000000004, -1.05)\n",
      "Steps 181631/250000 (72.652%) - Rewards: (-1, -1, -1.05, -1)\n",
      "Steps 181794/250000 (72.718%) - Rewards: (-1, -1, -1.1, -0.15000000000000002)\n",
      "Steps 181977/250000 (72.791%) - Rewards: (-1.8000000000000003, -1.4, -1.1, -0.09999999999999998)\n",
      "Steps 182160/250000 (72.864%) - Rewards: (-1.5, -0.25, -1, -1.05)\n",
      "Steps 182323/250000 (72.929%) - Rewards: (-1.3, -1.05, -1.05, -1.05)\n",
      "Steps 182486/250000 (72.994%) - Rewards: (-1.1, -1.6, -0.3500000000000001, -0.15000000000000002)\n",
      "Steps 182649/250000 (73.06%) - Rewards: (-1, -1.05, -1.5499999999999998, -1)\n",
      "Steps 182812/250000 (73.125%) - Rewards: (-1.15, -1, -1.25, -1.05)\n",
      "Steps 182975/250000 (73.19%) - Rewards: (-1.1, -1, -1.15, -1.1)\n",
      "Steps 183138/250000 (73.255%) - Rewards: (-1.05, -1.05, -0.20000000000000007, -1.05)\n",
      "Steps 183321/250000 (73.328%) - Rewards: (-1.05, -0.050000000000000044, -1.1, -0.050000000000000044)\n",
      "Steps 183504/250000 (73.402%) - Rewards: (-0.19999999999999996, -1.2, -1.05, -1)\n",
      "Steps 183687/250000 (73.475%) - Rewards: (-1.1, -1.15, -0.15000000000000002, -0.10000000000000009)\n",
      "Steps 183850/250000 (73.54%) - Rewards: (-0.25, -1.1, -1.2, -1)\n",
      "Steps 184013/250000 (73.605%) - Rewards: (-1.3, -1.05, -1, -1.05)\n",
      "Steps 184176/250000 (73.67%) - Rewards: (-0.15000000000000002, -1.15, -1.1, -1.05)\n",
      "Steps 184339/250000 (73.736%) - Rewards: (-0.25, -1.1, -1.15, -1.25)\n",
      "Steps 184522/250000 (73.809%) - Rewards: (-1.15, -0.10000000000000009, -1.1, -1.35)\n",
      "Steps 184685/250000 (73.874%) - Rewards: (-1.15, -1.35, -1.05, -1.05)\n",
      "Steps 184868/250000 (73.947%) - Rewards: (-0.10000000000000009, -1.15, -1.1, -1.05)\n",
      "Steps 185031/250000 (74.012%) - Rewards: (-1.1, -1.05, -1.65, -1.05)\n",
      "Steps 185194/250000 (74.078%) - Rewards: (-1, -1.2, -1, -1)\n",
      "Steps 185357/250000 (74.143%) - Rewards: (-0.10000000000000009, -1.05, -1.05, -1.1)\n",
      "Steps 185520/250000 (74.208%) - Rewards: (-1.2, -1.05, -1.15, -0.09999999999999998)\n",
      "Steps 185683/250000 (74.273%) - Rewards: (-1.15, -1.05, -0.15000000000000002, -1.05)\n",
      "Steps 185846/250000 (74.338%) - Rewards: (-1.1, -1.05, -0.050000000000000044, -1.1)\n",
      "Steps 186009/250000 (74.404%) - Rewards: (-1.05, -1.1, -0.15000000000000002, -1.05)\n",
      "Steps 186192/250000 (74.477%) - Rewards: (-0.25, -1.1, -1.15, -1.15)\n",
      "Steps 186347/250000 (74.539%) - Rewards: (-1.4, -1.1, -1.05, -1.1)\n",
      "Steps 186510/250000 (74.604%) - Rewards: (-1.45, -1, -0.19999999999999996, -1.05)\n",
      "Steps 186693/250000 (74.677%) - Rewards: (-1.05, -0.2500000000000001, -1.1, -1.05)\n",
      "Steps 186876/250000 (74.75%) - Rewards: (0.6999999999999997, -1, -1.05, -1.1)\n",
      "Steps 187039/250000 (74.816%) - Rewards: (-1.4, -0.6500000000000001, -1.15, -1.1)\n",
      "Steps 187202/250000 (74.881%) - Rewards: (-1.2, -1.05, -1.05, -1.25)\n",
      "Steps 187385/250000 (74.954%) - Rewards: (-0.3500000000000001, -1.05, -1.05, -1.05)\n",
      "Steps 187568/250000 (75.027%) - Rewards: (-0.25, -1.2, -1.1, -1.2)\n",
      "Steps 187731/250000 (75.092%) - Rewards: (0.6499999999999999, -0.4, -1.15, -1.1)\n",
      "Steps 187894/250000 (75.158%) - Rewards: (-1.05, -1.6, -1.3, -1.1)\n",
      "Steps 188057/250000 (75.223%) - Rewards: (-0.25, -0.15000000000000013, -1.1, -1.05)\n",
      "Steps 188260/250000 (75.304%) - Rewards: (0.30000000000000004, -1.1, -1.1, -1.1)\n",
      "Steps 188423/250000 (75.369%) - Rewards: (-1.2, -1, -1.15, -1)\n",
      "Steps 188646/250000 (75.458%) - Rewards: (-1.15, -1.1, 0.7999999999999998, 0.7)\n",
      "Steps 188829/250000 (75.532%) - Rewards: (-1, -1, -0.15000000000000002, -1.1)\n",
      "Steps 188992/250000 (75.597%) - Rewards: (-1.05, -1.05, -1.15, -1.2)\n",
      "Steps 189155/250000 (75.662%) - Rewards: (-1, -1, -1.05, -1.05)\n",
      "Steps 189338/250000 (75.735%) - Rewards: (-1.1, -0.15000000000000002, -1, -1.15)\n",
      "Steps 189501/250000 (75.8%) - Rewards: (-1.15, -1.15, -0.15000000000000002, -1.05)\n",
      "Steps 189684/250000 (75.874%) - Rewards: (-1.05, -1.1, -0.15000000000000002, -0.050000000000000044)\n",
      "Steps 189867/250000 (75.947%) - Rewards: (-0.4500000000000003, -1.1, -1.15, -1)\n",
      "Steps 190070/250000 (76.028%) - Rewards: (0.3999999999999997, -1, -1.15, -1.05)\n",
      "Steps 190233/250000 (76.093%) - Rewards: (-1.05, -1.05, -1.05, -1.05)\n",
      "Steps 190396/250000 (76.158%) - Rewards: (-1.25, -0.09999999999999998, -1, -1)\n",
      "Steps 190559/250000 (76.224%) - Rewards: (-0.1, -1.1, -1.05, -1.1)\n",
      "Steps 190722/250000 (76.289%) - Rewards: (-1.05, -1.05, -1, -0.09999999999999998)\n",
      "Steps 190945/250000 (76.378%) - Rewards: (0.4499999999999995, -0.20000000000000007, -1.7000000000000002, -1.3)\n",
      "Steps 191168/250000 (76.467%) - Rewards: (-0.75, -0.65, -1, -1.05)\n",
      "Steps 191331/250000 (76.532%) - Rewards: (-1.3, -3.9499999999999975, -1, -1.05)\n",
      "Steps 191494/250000 (76.598%) - Rewards: (-1.25, -0.09999999999999998, -0.050000000000000044, -1.05)\n",
      "Steps 191657/250000 (76.663%) - Rewards: (-0.7500000000000001, -1.05, -1.1, -1.15)\n",
      "Steps 191820/250000 (76.728%) - Rewards: (-1.2, -0.25, -1.05, -1.05)\n",
      "Steps 192003/250000 (76.801%) - Rewards: (-0.25, -1.2, -1.15, -1.05)\n",
      "Steps 192186/250000 (76.874%) - Rewards: (-0.5, 0.8999999999999999, -1.1, -1.15)\n",
      "Steps 192349/250000 (76.94%) - Rewards: (-1.1, -0.15000000000000002, -1.05, -1.05)\n",
      "Steps 192572/250000 (77.029%) - Rewards: (0.25, -1.1, -1.15, -0.09999999999999998)\n",
      "Steps 192735/250000 (77.094%) - Rewards: (-1.25, -1.05, -1, -1.15)\n",
      "Steps 192898/250000 (77.159%) - Rewards: (-1.15, -0.20000000000000007, -1.1, -1.05)\n",
      "Steps 193061/250000 (77.224%) - Rewards: (-0.25, -1.05, -0.15000000000000002, -1)\n",
      "Steps 193224/250000 (77.29%) - Rewards: (-1.2, -0.09999999999999998, -0.050000000000000044, -1.05)\n",
      "Steps 193387/250000 (77.355%) - Rewards: (-1.1, -1.1, -1, -1.2)\n",
      "Steps 193550/250000 (77.42%) - Rewards: (-1.2, -1, -1, -1.05)\n",
      "Steps 193733/250000 (77.493%) - Rewards: (-1.7000000000000004, 0.4999999999999998, -1, -1)\n",
      "Steps 193896/250000 (77.558%) - Rewards: (-1.3, -1, -1.1, -1.1)\n",
      "Steps 194059/250000 (77.624%) - Rewards: (-1.1, -0.050000000000000044, -1.05, -1.1)\n",
      "Steps 194222/250000 (77.689%) - Rewards: (-1.15, -1.05, -1, -1.2)\n",
      "Steps 194385/250000 (77.754%) - Rewards: (-1.05, -1, -1.05, -1.05)\n",
      "Steps 194548/250000 (77.819%) - Rewards: (-0.25, -1.1, -1, -1.05)\n",
      "Steps 194731/250000 (77.892%) - Rewards: (-1.2, -0.09999999999999998, -1.15, -0.09999999999999998)\n",
      "Steps 194934/250000 (77.974%) - Rewards: (-1.35, -0.20000000000000007, -0.050000000000000044, -1.15)\n",
      "Steps 195117/250000 (78.047%) - Rewards: (0.7, -1.05, -0.050000000000000044, -1.1)\n",
      "Steps 195280/250000 (78.112%) - Rewards: (-0.40000000000000013, -1.1, -1.1, -1.1)\n",
      "Steps 195443/250000 (78.177%) - Rewards: (-1.1, -1.15, -1.1, -1)\n",
      "Steps 195626/250000 (78.25%) - Rewards: (-1.3, -1, -0.20000000000000007, -1.1)\n",
      "Steps 195789/250000 (78.316%) - Rewards: (-1.1, -0.050000000000000044, -1.3, -1.2)\n",
      "Steps 195952/250000 (78.381%) - Rewards: (-1.2, -0.09999999999999998, -1.45, -1)\n",
      "Steps 196115/250000 (78.446%) - Rewards: (-0.15000000000000002, -0.09999999999999998, -1.05, -1.15)\n",
      "Steps 196278/250000 (78.511%) - Rewards: (-1.15, -1.25, -1.15, -1)\n",
      "Steps 196461/250000 (78.584%) - Rewards: (-1.1, -0.15000000000000002, -1.2, -0.050000000000000044)\n",
      "Steps 196644/250000 (78.658%) - Rewards: (-0.09999999999999998, -1.5, -0.30000000000000016, -0.09999999999999998)\n",
      "Steps 196807/250000 (78.723%) - Rewards: (-0.09999999999999998, -1.05, -1.1, -1.05)\n",
      "Steps 196990/250000 (78.796%) - Rewards: (-1.15, -1.6, -1.05, -1)\n",
      "Steps 197173/250000 (78.869%) - Rewards: (-1.1, -0.050000000000000044, -1.05, -1)\n",
      "Steps 197336/250000 (78.934%) - Rewards: (-1.3, -0.050000000000000044, -0.050000000000000044, -1.15)\n",
      "Steps 197499/250000 (79.0%) - Rewards: (-1.15, -1, -1.05, -1)\n",
      "Steps 197662/250000 (79.065%) - Rewards: (-0.15000000000000002, -1.2, -1.1, -1.05)\n",
      "Steps 197825/250000 (79.13%) - Rewards: (-1.75, -1.25, -1.25, -1)\n",
      "Steps 197988/250000 (79.195%) - Rewards: (-1.15, -1.1, -1.2, -1.15)\n",
      "Steps 198151/250000 (79.26%) - Rewards: (-1.15, -1.05, -0.19999999999999996, -1)\n",
      "Steps 198314/250000 (79.326%) - Rewards: (-1.1500000000000001, -1.1, -1.05, -1.05)\n",
      "Steps 198477/250000 (79.391%) - Rewards: (-0.09999999999999998, -2.5500000000000007, -1.05, -1.1)\n",
      "Steps 198660/250000 (79.464%) - Rewards: (-0.20000000000000007, -0.30000000000000004, -1.05, -1.05)\n",
      "Steps 198843/250000 (79.537%) - Rewards: (-1.15, -0.09999999999999998, -1.05, -1)\n",
      "Steps 199006/250000 (79.602%) - Rewards: (-1.35, -0.09999999999999998, -1, -1)\n",
      "Steps 199169/250000 (79.668%) - Rewards: (-1, -1, -1, -0.09999999999999998)\n",
      "Steps 199332/250000 (79.733%) - Rewards: (-1.05, -1.15, -1.1, -1)\n",
      "Steps 199535/250000 (79.814%) - Rewards: (-1.9000000000000012, -1.2, -1.05, -1)\n",
      "Steps 199698/250000 (79.879%) - Rewards: (-1.1, -1.05, -1, -1.05)\n",
      "Steps 199861/250000 (79.944%) - Rewards: (-1.1, -1, -1, -1.05)\n",
      "Steps 200044/250000 (80.018%) - Rewards: (-1.1, -0.10000000000000009, -0.050000000000000044, -1)\n",
      "Steps 200207/250000 (80.083%) - Rewards: (-0.050000000000000044, -1.15, -1.1, -1.1)\n",
      "Steps 200370/250000 (80.148%) - Rewards: (-1.05, -1.15, -1.15, -1.1)\n",
      "Steps 200533/250000 (80.213%) - Rewards: (-1.1, -1.05, -1.05, -1)\n",
      "Steps 200696/250000 (80.278%) - Rewards: (-1.05, -1, -1.1, -1.2)\n",
      "Steps 200859/250000 (80.344%) - Rewards: (-1, -0.09999999999999998, -1.1, -1)\n",
      "Steps 201022/250000 (80.409%) - Rewards: (-1.3500000000000005, -0.15000000000000002, -1.3, -1)\n",
      "Steps 201185/250000 (80.474%) - Rewards: (-0.9000000000000002, -0.050000000000000044, -1.05, -1.05)\n",
      "Steps 201348/250000 (80.539%) - Rewards: (-1.15, -1, -1.05, -0.050000000000000044)\n",
      "Steps 201511/250000 (80.604%) - Rewards: (-0.10000000000000009, -1.1, -1.15, -1)\n",
      "Steps 201674/250000 (80.67%) - Rewards: (-1, -0.20000000000000007, -1.2, -1.05)\n",
      "Steps 201837/250000 (80.735%) - Rewards: (-0.050000000000000044, -0.050000000000000044, -1, -1.05)\n",
      "Steps 202000/250000 (80.8%) - Rewards: (-0.09999999999999998, -1.1, -0.050000000000000044, -1.1)\n",
      "Steps 202163/250000 (80.865%) - Rewards: (-1.1, -1.15, -1, -1.15)\n",
      "Steps 202326/250000 (80.93%) - Rewards: (-0.20000000000000007, -1.05, -1.1, -1)\n",
      "Steps 202489/250000 (80.996%) - Rewards: (-0.55, -1.05, -1, -1.1)\n",
      "Steps 202652/250000 (81.061%) - Rewards: (-1.05, -1.05, -1.2, -1.1)\n",
      "Steps 202815/250000 (81.126%) - Rewards: (-1.1, -0.050000000000000044, -1.05, -1.1)\n",
      "Steps 202978/250000 (81.191%) - Rewards: (-0.20000000000000018, -1.4, -1.05, -1)\n",
      "Steps 203141/250000 (81.256%) - Rewards: (-1.15, -1.15, -1.1, -1.05)\n",
      "Steps 203304/250000 (81.322%) - Rewards: (-1, -1.05, -1.15, -1.1)\n",
      "Steps 203467/250000 (81.387%) - Rewards: (-1.1, -1, -1.1, -1.15)\n",
      "Steps 203630/250000 (81.452%) - Rewards: (-0.7500000000000002, -1, -0.09999999999999998, -1.1)\n",
      "Steps 203793/250000 (81.517%) - Rewards: (-1.4, -1.05, -0.15000000000000002, -1.05)\n",
      "Steps 203956/250000 (81.582%) - Rewards: (-1.05, -1, -1.3, -1)\n",
      "Steps 204159/250000 (81.664%) - Rewards: (0.7499999999999998, -0.10000000000000009, -1.2, -1)\n",
      "Steps 204322/250000 (81.729%) - Rewards: (-1.05, -0.050000000000000044, -1.1, -1.05)\n",
      "Steps 204485/250000 (81.794%) - Rewards: (-1, -1.05, -1.1, -1.05)\n",
      "Steps 204668/250000 (81.867%) - Rewards: (-0.20000000000000007, -1, -1.15, -1.05)\n",
      "Steps 204851/250000 (81.94%) - Rewards: (-1.3, -0.050000000000000044, -1, -1.05)\n",
      "Steps 205014/250000 (82.006%) - Rewards: (-1, -1.15, -1.05, -1.05)\n",
      "Steps 205177/250000 (82.071%) - Rewards: (-1.2, -0.050000000000000044, -1.05, -1.05)\n",
      "Steps 205360/250000 (82.144%) - Rewards: (-2.4000000000000004, -0.20000000000000007, -1.1, -1.05)\n",
      "Steps 205523/250000 (82.209%) - Rewards: (-2.2, -1.05, -1, -1.05)\n",
      "Steps 205686/250000 (82.274%) - Rewards: (-1.35, -0.050000000000000044, -1, -1)\n",
      "Steps 205849/250000 (82.34%) - Rewards: (-1.15, -1.2, -1.15, -1.05)\n",
      "Steps 206012/250000 (82.405%) - Rewards: (-1.15, 0.7999999999999998, -1.1, -1.15)\n",
      "Steps 206195/250000 (82.478%) - Rewards: (-1.15, -1.05, 0.8999999999999999, -1)\n",
      "Steps 206378/250000 (82.551%) - Rewards: (-1.2, -1.1, -1.05, -0.050000000000000044)\n",
      "Steps 206541/250000 (82.616%) - Rewards: (-1.15, -1, -1.25, -1.05)\n",
      "Steps 206704/250000 (82.682%) - Rewards: (-1.15, -1.05, -1.15, -1.15)\n",
      "Steps 206947/250000 (82.779%) - Rewards: (2.2500000000000004, -1.05, -0.19999999999999996, -0.19999999999999996)\n",
      "Steps 207130/250000 (82.852%) - Rewards: (-0.19999999999999996, -0.10000000000000009, -1.1, -1.1)\n",
      "Steps 207293/250000 (82.917%) - Rewards: (-1.2, -1, -1.15, -1.15)\n",
      "Steps 207476/250000 (82.99%) - Rewards: (-0.4500000000000002, -1, -1, -1)\n",
      "Steps 207639/250000 (83.056%) - Rewards: (-1.3, -1, -1.05, -1.1)\n",
      "Steps 207802/250000 (83.121%) - Rewards: (-0.30000000000000004, -1.1, -1.15, -1.05)\n",
      "Steps 207985/250000 (83.194%) - Rewards: (1.0999999999999996, -2.500000000000001, -1.05, -1.2)\n",
      "Steps 208208/250000 (83.283%) - Rewards: (1.1, -1.1, -1.05, -1.1)\n",
      "Steps 208371/250000 (83.348%) - Rewards: (-1.15, -1.05, -1.15, -0.050000000000000044)\n",
      "Steps 208534/250000 (83.414%) - Rewards: (-1.2, -1.05, -1.05, -1)\n",
      "Steps 208697/250000 (83.479%) - Rewards: (-1.3, -1.1, -1.1, -1.15)\n",
      "Steps 208860/250000 (83.544%) - Rewards: (-0.19999999999999996, -1.05, -1, -1.05)\n",
      "Steps 209023/250000 (83.609%) - Rewards: (-1.25, -1.15, -1, -1.05)\n",
      "Steps 209226/250000 (83.69%) - Rewards: (-0.2500000000000002, -1, -0.09999999999999998, -1.05)\n",
      "Steps 209409/250000 (83.764%) - Rewards: (-0.30000000000000004, -1.1, -0.050000000000000044, -0.09999999999999998)\n",
      "Steps 209592/250000 (83.837%) - Rewards: (-0.5000000000000002, -1, -0.20000000000000007, -1.1)\n",
      "Steps 209755/250000 (83.902%) - Rewards: (-1.3, -1.05, -1.2, -1.05)\n",
      "Steps 209958/250000 (83.983%) - Rewards: (0.49999999999999956, -0.2500000000000001, -1.05, -1.05)\n",
      "Steps 210121/250000 (84.048%) - Rewards: (-1.35, -1.05, -0.20000000000000007, -1)\n",
      "Steps 210304/250000 (84.122%) - Rewards: (-0.6500000000000002, -1.05, -1, -1.15)\n",
      "Steps 210467/250000 (84.187%) - Rewards: (-1.5499999999999998, -1, -1.1, -1.1)\n",
      "Steps 210630/250000 (84.252%) - Rewards: (-1.25, -1.05, -0.050000000000000044, -1.1)\n",
      "Steps 210793/250000 (84.317%) - Rewards: (-1.1, -1.15, -1, -1)\n",
      "Steps 210956/250000 (84.382%) - Rewards: (-1.45, -0.050000000000000044, -1.05, -1.1)\n",
      "Steps 211139/250000 (84.456%) - Rewards: (-0.35, -1.15, -1.1, -1)\n",
      "Steps 211302/250000 (84.521%) - Rewards: (-1.3, -1, -1.1, -1)\n",
      "Steps 211465/250000 (84.586%) - Rewards: (-1.2, -0.050000000000000044, -1.1, -1)\n",
      "Steps 211628/250000 (84.651%) - Rewards: (-1.5499999999999998, -0.050000000000000044, -0.09999999999999998, -1)\n",
      "Steps 211811/250000 (84.724%) - Rewards: (-1.5499999999999998, -0.10000000000000009, -1.1, -1.1)\n",
      "Steps 211974/250000 (84.79%) - Rewards: (-1.4, -1.1, -1.05, -1.05)\n",
      "Steps 212137/250000 (84.855%) - Rewards: (0.6499999999999999, -1.15, -1.05, -1.15)\n",
      "Steps 212320/250000 (84.928%) - Rewards: (-1.4, -1, -1.2, -1.0500000000000003)\n",
      "Steps 212503/250000 (85.001%) - Rewards: (-0.4500000000000003, -1.05, -0.050000000000000044, -1.05)\n",
      "Steps 212686/250000 (85.074%) - Rewards: (0.44999999999999973, -1.1, -1.05, -1.1)\n",
      "Steps 212849/250000 (85.14%) - Rewards: (-1.15, -1.05, -1.05, -1.4)\n",
      "Steps 213012/250000 (85.205%) - Rewards: (-1.25, -1.05, -1.05, -1.2)\n",
      "Steps 213175/250000 (85.27%) - Rewards: (-0.40000000000000024, -1, -1, -1.2)\n",
      "Steps 213338/250000 (85.335%) - Rewards: (-1.3, -1.05, -1, -1.65)\n",
      "Steps 213501/250000 (85.4%) - Rewards: (0.5999999999999999, -1.2, -1.05, -1.1)\n",
      "Steps 213664/250000 (85.466%) - Rewards: (-0.40000000000000013, -1.1, -1, -1.1)\n",
      "Steps 213827/250000 (85.531%) - Rewards: (-1.5499999999999998, -1, -1.1, -1)\n",
      "Steps 213990/250000 (85.596%) - Rewards: (-1.5, -1.05, -1.05, -1)\n",
      "Steps 214153/250000 (85.661%) - Rewards: (-2.0, -1.05, -1.3, -1.15)\n",
      "Steps 214316/250000 (85.726%) - Rewards: (-1.4, -1.1, -1, -1)\n",
      "Steps 214499/250000 (85.8%) - Rewards: (-0.20000000000000018, -1, -1.4, -1.05)\n",
      "Steps 214662/250000 (85.865%) - Rewards: (-1.25, -1.15, -0.050000000000000044, -1.1)\n",
      "Steps 214825/250000 (85.93%) - Rewards: (-1.35, -1, -1, -1)\n",
      "Steps 214988/250000 (85.995%) - Rewards: (-1.25, -1.1, -1.15, -1.1)\n",
      "Steps 215151/250000 (86.06%) - Rewards: (-1.2, -1.1, -0.15000000000000002, -1.15)\n",
      "Steps 215314/250000 (86.126%) - Rewards: (-0.09999999999999998, -1.05, -1.05, -1.05)\n",
      "Steps 215517/250000 (86.207%) - Rewards: (-1.25, -0.3500000000000001, -2.1500000000000004, -1.3)\n",
      "Steps 215700/250000 (86.28%) - Rewards: (-1.15, -1.3, -1.6, -1.05)\n",
      "Steps 215963/250000 (86.385%) - Rewards: (-1.35, -1.4500000000000006, -0.5, -0.10000000000000009)\n",
      "Steps 216166/250000 (86.466%) - Rewards: (0.6999999999999997, -1.7000000000000002, -1.05, -1)\n",
      "Steps 216329/250000 (86.532%) - Rewards: (-1.1, -1.6, -1.25, -1.1)\n",
      "Steps 216532/250000 (86.613%) - Rewards: (-1.85, -0.5, -0.8, -1)\n",
      "Steps 216695/250000 (86.678%) - Rewards: (-1.35, -1.2, -1.5499999999999998, -1)\n",
      "Steps 216878/250000 (86.751%) - Rewards: (-1.4, -1.4, -1.2, -1)\n",
      "Steps 217041/250000 (86.816%) - Rewards: (-1.45, -0.25, -1.1, -1)\n",
      "Steps 217244/250000 (86.898%) - Rewards: (-1.45, -0.050000000000000044, -0.3999999999999999, -1.05)\n",
      "Steps 217407/250000 (86.963%) - Rewards: (-1.25, -0.050000000000000044, -1, -1.05)\n",
      "Steps 217570/250000 (87.028%) - Rewards: (-1.3, -1, -0.050000000000000044, -1)\n",
      "Steps 217773/250000 (87.109%) - Rewards: (-1.25, -0.10000000000000009, -0.09999999999999998, -0.8500000000000001)\n",
      "Steps 217936/250000 (87.174%) - Rewards: (-0.2500000000000001, -0.050000000000000044, -1.15, -1.2)\n",
      "Steps 218159/250000 (87.264%) - Rewards: (0.5499999999999998, -0.19999999999999996, -0.050000000000000044, -1.3)\n",
      "Steps 218342/250000 (87.337%) - Rewards: (-0.30000000000000027, -1.05, -1.05, -0.30000000000000004)\n",
      "Steps 218505/250000 (87.402%) - Rewards: (-1.15, -1.1, -1, -1.5499999999999998)\n",
      "Steps 218668/250000 (87.467%) - Rewards: (-1.2, -1.1, -1.05, -1.1)\n",
      "Steps 218831/250000 (87.532%) - Rewards: (-0.3999999999999999, -1.2, -1.15, -1.05)\n",
      "Steps 218994/250000 (87.598%) - Rewards: (-1.6, -0.9999999999999999, -1.2, -1.25)\n",
      "Steps 219157/250000 (87.663%) - Rewards: (0.44999999999999973, -1, -0.050000000000000044, -1)\n",
      "Steps 219320/250000 (87.728%) - Rewards: (-1.2, -1.1, -1, -1.15)\n",
      "Steps 219479/250000 (87.792%) - Rewards: (-0.25, -0.09999999999999998, -1.05, -0.15000000000000002)\n",
      "Steps 219662/250000 (87.865%) - Rewards: (-1.15, -0.10000000000000009, -1.05, -1.1)\n",
      "Steps 219845/250000 (87.938%) - Rewards: (1.75, -1.1, -1.1, -1.05)\n",
      "Steps 220028/250000 (88.011%) - Rewards: (0.7999999999999998, -1, -1.05, -1.1)\n",
      "Steps 220211/250000 (88.084%) - Rewards: (-1.35, -0.15000000000000013, -0.2, -1.05)\n",
      "Steps 220374/250000 (88.15%) - Rewards: (-1.35, -1.05, -1.1, -1.05)\n",
      "Steps 220537/250000 (88.215%) - Rewards: (-0.3500000000000002, -1, -1.05, -0.15000000000000002)\n",
      "Steps 220720/250000 (88.288%) - Rewards: (-0.09999999999999998, -1.15, -0.10000000000000009, -1.1)\n",
      "Steps 220883/250000 (88.353%) - Rewards: (-1.3, -1.05, 0.8500000000000001, -1.05)\n",
      "Steps 221066/250000 (88.426%) - Rewards: (-1.25, -0.2500000000000001, -0.050000000000000044, -1.05)\n",
      "Steps 221229/250000 (88.492%) - Rewards: (-1.4, -1.1, -1, -1.1)\n",
      "Steps 221392/250000 (88.557%) - Rewards: (-1.25, -1, 0.8500000000000001, -1)\n",
      "Steps 221555/250000 (88.622%) - Rewards: (-1.6, -1.05, -1, -1)\n",
      "Steps 221718/250000 (88.687%) - Rewards: (-1.25, -0.15000000000000002, -1.1, -1.05)\n",
      "Steps 221881/250000 (88.752%) - Rewards: (0.2999999999999998, -1.2, -1, -1.05)\n",
      "Steps 222044/250000 (88.818%) - Rewards: (-1.6, -1.1, -1.05, -1)\n",
      "Steps 222207/250000 (88.883%) - Rewards: (-1.2, -1.05, -1.05, -1.2)\n",
      "Steps 222390/250000 (88.956%) - Rewards: (-1.15, -0.050000000000000044, -0.09999999999999998, -1)\n",
      "Steps 222553/250000 (89.021%) - Rewards: (-1.7000000000000002, -1.05, -1.05, -1.05)\n",
      "Steps 222756/250000 (89.102%) - Rewards: (0.7499999999999998, -1.1, -1.1, -1.05)\n",
      "Steps 222919/250000 (89.168%) - Rewards: (-1.1, -1.15, -0.10000000000000009, -1.2)\n",
      "Steps 223102/250000 (89.241%) - Rewards: (-1.35, 0.8499999999999999, -1, -1.15)\n",
      "Steps 223265/250000 (89.306%) - Rewards: (-1.25, -1, -1.05, -1.05)\n",
      "Steps 223448/250000 (89.379%) - Rewards: (-0.15000000000000002, -1.1, -1.05, -1.15)\n",
      "Steps 223611/250000 (89.444%) - Rewards: (-1.1, -1, -1, -1)\n",
      "Steps 223774/250000 (89.51%) - Rewards: (-1.1, -1, -0.09999999999999998, -1)\n",
      "Steps 223937/250000 (89.575%) - Rewards: (-1.3, -1.05, -1, -1)\n",
      "Steps 224100/250000 (89.64%) - Rewards: (-1.25, -1, -0.15000000000000002, -1)\n",
      "Steps 224263/250000 (89.705%) - Rewards: (-0.25, -1, -0.15000000000000002, -1)\n",
      "Steps 224426/250000 (89.77%) - Rewards: (-1.3, -1.05, -1, -1.05)\n",
      "Steps 224589/250000 (89.836%) - Rewards: (-1.15, -0.050000000000000044, -0.10000000000000009, -1.1)\n",
      "Steps 224772/250000 (89.909%) - Rewards: (0.7999999999999998, -0.15000000000000002, -1.1, -1.05)\n",
      "Steps 224975/250000 (89.99%) - Rewards: (-1.05, -1.05, -0.2500000000000001, -1)\n",
      "Steps 225138/250000 (90.055%) - Rewards: (-0.15000000000000002, -0.15000000000000002, -0.30000000000000004, -1)\n",
      "Steps 225321/250000 (90.128%) - Rewards: (-0.3500000000000002, -1, -1.15, -1.15)\n",
      "Steps 225504/250000 (90.202%) - Rewards: (-0.09999999999999998, -0.10000000000000009, 0.8500000000000001, -1.05)\n",
      "Steps 225667/250000 (90.267%) - Rewards: (-1.15, -1.2, -1, -1.05)\n",
      "Steps 225830/250000 (90.332%) - Rewards: (-0.25, -1.05, -0.050000000000000044, -1.05)\n",
      "Steps 225993/250000 (90.397%) - Rewards: (-1.1, -1.05, -1, -1.05)\n",
      "Steps 226176/250000 (90.47%) - Rewards: (-1.1, -1.1, -0.050000000000000044, -1)\n",
      "Steps 226339/250000 (90.536%) - Rewards: (-1.15, -1.15, -1.15, -1)\n",
      "Steps 226502/250000 (90.601%) - Rewards: (-0.20000000000000007, -1.15, -1.05, -1.1)\n",
      "Steps 226665/250000 (90.666%) - Rewards: (-1.15, -0.30000000000000004, -1.1, -1.1)\n",
      "Steps 226828/250000 (90.731%) - Rewards: (-1.15, -1.15, -1.05, -1)\n",
      "Steps 227011/250000 (90.804%) - Rewards: (-1.15, -0.30000000000000004, -0.15000000000000002, -1)\n",
      "Steps 227194/250000 (90.878%) - Rewards: (-1.3, -1.1, -0.20000000000000007, -1.05)\n",
      "Steps 227357/250000 (90.943%) - Rewards: (0.6499999999999997, -1.05, -1.05, -1.1)\n",
      "Steps 227520/250000 (91.008%) - Rewards: (-0.15000000000000002, -1.25, -1.05, -1)\n",
      "Steps 227703/250000 (91.081%) - Rewards: (-1.1, -1.1, -0.10000000000000009, -1.1)\n",
      "Steps 227886/250000 (91.154%) - Rewards: (-0.9500000000000003, -1.15, -1.05, -0.09999999999999998)\n",
      "Steps 228069/250000 (91.228%) - Rewards: (-0.7500000000000001, -1.15, -1, -1.05)\n",
      "Steps 228232/250000 (91.293%) - Rewards: (-1.25, -1.25, -1.05, -1)\n",
      "Steps 228395/250000 (91.358%) - Rewards: (-1.75, -1.05, -1.1, -1.05)\n",
      "Steps 228558/250000 (91.423%) - Rewards: (-1.45, -1.1, -1, -1.05)\n",
      "Steps 228721/250000 (91.488%) - Rewards: (-1, -1.05, -1.05, -0.050000000000000044)\n",
      "Steps 228924/250000 (91.57%) - Rewards: (1.5500000000000007, -1.1, -1.1, -1)\n",
      "Steps 229087/250000 (91.635%) - Rewards: (-0.35, -1, -1.05, -1.2)\n",
      "Steps 229250/250000 (91.7%) - Rewards: (-0.19999999999999996, -1, -1.05, -1.05)\n",
      "Steps 229413/250000 (91.765%) - Rewards: (-1, -1.5499999999999998, -0.30000000000000004, -1.15)\n",
      "Steps 229596/250000 (91.838%) - Rewards: (-1.1, -1.2, -1.05, 0.7)\n",
      "Steps 229759/250000 (91.904%) - Rewards: (-1.2, -1, -1.1, -1.05)\n",
      "Steps 229942/250000 (91.977%) - Rewards: (-0.15000000000000002, -1.05, -1.1, -1.05)\n",
      "Steps 230105/250000 (92.042%) - Rewards: (-1.15, -1.1, -1, -1.1)\n",
      "Steps 230268/250000 (92.107%) - Rewards: (-0.30000000000000004, -1, -1, -1.05)\n",
      "Steps 230451/250000 (92.18%) - Rewards: (-1.1, -1, -0.09999999999999998, -1.05)\n",
      "Steps 230614/250000 (92.246%) - Rewards: (-0.4500000000000002, -1, -1, -1.05)\n",
      "Steps 230777/250000 (92.311%) - Rewards: (-0.3500000000000002, -1.15, -1, -1)\n",
      "Steps 230940/250000 (92.376%) - Rewards: (-1.1, -1.15, -1.1, -1.05)\n",
      "Steps 231103/250000 (92.441%) - Rewards: (-1.35, -1.05, -1, -1.1)\n",
      "Steps 231266/250000 (92.506%) - Rewards: (-1.05, -1, -2.0, -1.05)\n",
      "Steps 231429/250000 (92.572%) - Rewards: (-0.3500000000000001, -1.05, -1.1, -1.05)\n",
      "Steps 231592/250000 (92.637%) - Rewards: (-1.65, -1.2, -1.05, -1.1)\n",
      "Steps 231775/250000 (92.71%) - Rewards: (-1.2, -1, -1.15, -1)\n",
      "Steps 231938/250000 (92.775%) - Rewards: (0.3999999999999999, -1, -1.05, -1)\n",
      "Steps 232101/250000 (92.84%) - Rewards: (-1.1, -1.05, -1.1, -1)\n",
      "Steps 232264/250000 (92.906%) - Rewards: (-1.3, -1.1, -1, -1.05)\n",
      "Steps 232427/250000 (92.971%) - Rewards: (-1.1, -1.1, -1, -1.1)\n",
      "Steps 232590/250000 (93.036%) - Rewards: (-1.05, -1.05, -1.05, -1.1)\n",
      "Steps 232753/250000 (93.101%) - Rewards: (-1.05, -1.15, -0.050000000000000044, -1.05)\n",
      "Steps 232936/250000 (93.174%) - Rewards: (-1.1, -1.1, -1.05, -0.15000000000000002)\n",
      "Steps 233099/250000 (93.24%) - Rewards: (-1.1, -1, -1.05, -1)\n",
      "Steps 233282/250000 (93.313%) - Rewards: (0.5999999999999999, -1.05, -0.20000000000000007, -1.05)\n",
      "Steps 233465/250000 (93.386%) - Rewards: (1.7999999999999998, -1.05, -1, -1.05)\n",
      "Steps 233648/250000 (93.459%) - Rewards: (-0.25, -0.10000000000000009, -1.15, -1)\n",
      "Steps 233851/250000 (93.54%) - Rewards: (-0.10000000000000009, -1.05, -1.25, -0.19999999999999996)\n",
      "Steps 234034/250000 (93.614%) - Rewards: (-0.65, -0.050000000000000044, -1.25, -1)\n",
      "Steps 234197/250000 (93.679%) - Rewards: (-0.30000000000000004, -1.1, -1.05, -1.05)\n",
      "Steps 234360/250000 (93.744%) - Rewards: (-0.44999999999999996, -1.05, -1.2, -1.15)\n",
      "Steps 234523/250000 (93.809%) - Rewards: (-0.7999999999999998, -1.2, -0.050000000000000044, -1)\n",
      "Steps 234726/250000 (93.89%) - Rewards: (0.39999999999999947, -1, -1.1, -1.05)\n",
      "Steps 234889/250000 (93.956%) - Rewards: (-3.6999999999999984, -1.05, -1, -1.15)\n",
      "Steps 235052/250000 (94.021%) - Rewards: (-0.30000000000000004, -1, -1, -0.25)\n",
      "Steps 235215/250000 (94.086%) - Rewards: (-1.45, -1.05, -1.15, -0.15000000000000002)\n",
      "Steps 235418/250000 (94.167%) - Rewards: (0.6499999999999997, -1.1500000000000001, -0.050000000000000044, -1)\n",
      "Steps 235581/250000 (94.232%) - Rewards: (-2.850000000000001, -1, -1, -1.15)\n",
      "Steps 235744/250000 (94.298%) - Rewards: (-2.4500000000000006, -1, -1, 0.8500000000000001)\n",
      "Steps 235927/250000 (94.371%) - Rewards: (-0.20000000000000018, -1.05, -1.1, -1.25)\n",
      "Steps 236090/250000 (94.436%) - Rewards: (0.8999999999999999, -1.1, -0.050000000000000044, -1.1)\n",
      "Steps 236253/250000 (94.501%) - Rewards: (-1.15, -1.05, -1.1, -1)\n",
      "Steps 236456/250000 (94.582%) - Rewards: (-0.25, -1.1, -1, -0.15000000000000002)\n",
      "Steps 236619/250000 (94.648%) - Rewards: (-1.05, -1.1, -1, -1.15)\n",
      "Steps 236782/250000 (94.713%) - Rewards: (-1, -1.1, -1.05, -0.09999999999999998)\n",
      "Steps 236965/250000 (94.786%) - Rewards: (-0.44999999999999996, -1, -1.1, -0.050000000000000044)\n",
      "Steps 237148/250000 (94.859%) - Rewards: (-0.5999999999999999, -1, -1.05, -1.05)\n",
      "Steps 237311/250000 (94.924%) - Rewards: (-1.3, -1.1, -1, -1.1)\n",
      "Steps 237494/250000 (94.998%) - Rewards: (-1.45, -1.1, -1, -0.09999999999999998)\n",
      "Steps 237657/250000 (95.063%) - Rewards: (-1.25, -1, -1.1, -1.1)\n",
      "Steps 237820/250000 (95.128%) - Rewards: (-1.1, -1.05, -1.05, -1)\n",
      "Steps 238003/250000 (95.201%) - Rewards: (-1.15, -1.05, -1.05, -0.15000000000000013)\n",
      "Steps 238186/250000 (95.274%) - Rewards: (-1.1, -0.10000000000000009, -1.15, -1.1500000000000001)\n",
      "Steps 238349/250000 (95.34%) - Rewards: (-1.15, -1, -1.15, -1)\n",
      "Steps 238512/250000 (95.405%) - Rewards: (-1.1, -1, -1, -1)\n",
      "Steps 238675/250000 (95.47%) - Rewards: (-1.1, -1.2, -1.05, -1.05)\n",
      "Steps 238858/250000 (95.543%) - Rewards: (-1.05, -1.05, -1.05, -0.050000000000000044)\n",
      "Steps 239021/250000 (95.608%) - Rewards: (-1.15, -1, -0.15000000000000002, -1.15)\n",
      "Steps 239204/250000 (95.682%) - Rewards: (-1.05, -0.09999999999999998, -0.10000000000000009, -1.05)\n",
      "Steps 239387/250000 (95.755%) - Rewards: (-0.15000000000000013, -1.05, -1.1, -1)\n",
      "Steps 239550/250000 (95.82%) - Rewards: (-1.05, -1, -1.15, -1.05)\n",
      "Steps 239713/250000 (95.885%) - Rewards: (-1.1, -1.05, -1, -1.15)\n",
      "Steps 239876/250000 (95.95%) - Rewards: (-1.05, -1.05, -1.05, -1.05)\n",
      "Steps 240039/250000 (96.016%) - Rewards: (-1.15, -1.05, -1.05, -1)\n",
      "Steps 240222/250000 (96.089%) - Rewards: (-0.10000000000000009, -1, -1, -1)\n",
      "Steps 240405/250000 (96.162%) - Rewards: (-0.09999999999999998, -1, -0.10000000000000009, -1)\n",
      "Steps 240588/250000 (96.235%) - Rewards: (-1.1, -1.1, -0.050000000000000044, -0.10000000000000009)\n",
      "Steps 240751/250000 (96.3%) - Rewards: (-1.05, -1, -1, -1.1)\n",
      "Steps 240914/250000 (96.366%) - Rewards: (-1.05, -1.1, -0.15000000000000002, -1.05)\n",
      "Steps 241077/250000 (96.431%) - Rewards: (-1.15, -1.15, -1.1, -1.05)\n",
      "Steps 241260/250000 (96.504%) - Rewards: (-0.25, -0.15000000000000013, -1.1, -1.1)\n",
      "Steps 241443/250000 (96.577%) - Rewards: (-1.1, -1.1, -1.05, -0.35)\n",
      "Steps 241606/250000 (96.642%) - Rewards: (-1.2, -1, -1.15, -1)\n",
      "Steps 241789/250000 (96.716%) - Rewards: (-1.2, -0.050000000000000044, -1.05, -1.05)\n",
      "Steps 241950/250000 (96.78%) - Rewards: (-1.05, -1.1, -1.05, -1.1)\n",
      "Steps 242113/250000 (96.845%) - Rewards: (-1.05, -1, -1, -0.050000000000000044)\n",
      "Steps 242296/250000 (96.918%) - Rewards: (-0.10000000000000009, -1, -1.1, -1.15)\n",
      "Steps 242499/250000 (97.0%) - Rewards: (-1.1, -0.15000000000000002, -1.15, -0.09999999999999998)\n",
      "Steps 242682/250000 (97.073%) - Rewards: (-0.050000000000000044, -1, -1, -1.05)\n",
      "Steps 242845/250000 (97.138%) - Rewards: (-1.1, -1.05, -1.1, -1.05)\n",
      "Steps 243008/250000 (97.203%) - Rewards: (-1.1, -1, -1.1, -1)\n",
      "Steps 243171/250000 (97.268%) - Rewards: (-1.05, -1.05, -1.05, -1.05)\n",
      "Steps 243354/250000 (97.342%) - Rewards: (-1.15, -1, -1, -0.15000000000000013)\n",
      "Steps 243517/250000 (97.407%) - Rewards: (-1.05, -1, -1, -1)\n",
      "Steps 243680/250000 (97.472%) - Rewards: (-1.15, -1.05, -1, -1)\n",
      "Steps 243843/250000 (97.537%) - Rewards: (-0.15000000000000002, -1.05, -1.6, -0.15000000000000002)\n",
      "Steps 244006/250000 (97.602%) - Rewards: (-1.15, -1.1, -1, -1.15)\n",
      "Steps 244169/250000 (97.668%) - Rewards: (-1.05, -0.050000000000000044, -1, -1.05)\n",
      "Steps 244332/250000 (97.733%) - Rewards: (-1.1, -1.1, -1.1, -1.05)\n",
      "Steps 244495/250000 (97.798%) - Rewards: (-1.25, -1.05, -0.09999999999999998, -1.05)\n",
      "Steps 244658/250000 (97.863%) - Rewards: (-0.09999999999999998, -1, -1.05, -1)\n",
      "Steps 244821/250000 (97.928%) - Rewards: (-1.15, -1.15, -1, -1)\n",
      "Steps 244984/250000 (97.994%) - Rewards: (-1.15, -1.05, -1.15, -1.05)\n",
      "Steps 245147/250000 (98.059%) - Rewards: (-1.05, -1, -1.05, -1.1)\n",
      "Steps 245330/250000 (98.132%) - Rewards: (-0.09999999999999998, -1, -1.05, -0.050000000000000044)\n",
      "Steps 245493/250000 (98.197%) - Rewards: (-1.15, -1.1, -1, -1)\n",
      "Steps 245656/250000 (98.262%) - Rewards: (-1.2, -1.05, -1.1, -1.1)\n",
      "Steps 245819/250000 (98.328%) - Rewards: (-0.25, -1.1, -1.1, -1)\n",
      "Steps 245982/250000 (98.393%) - Rewards: (-1.05, -1.2, -1.05, -1.1)\n",
      "Steps 246145/250000 (98.458%) - Rewards: (-1.05, -1.05, -1, -1.05)\n",
      "Steps 246308/250000 (98.523%) - Rewards: (-1, -1.05, -1.1, -1)\n",
      "Steps 246491/250000 (98.596%) - Rewards: (-1.15, -1, -0.050000000000000044, 0.8500000000000001)\n",
      "Steps 246654/250000 (98.662%) - Rewards: (-1.2, -1.05, -1.1, -1.15)\n",
      "Steps 246817/250000 (98.727%) - Rewards: (-1.1, -1.05, -1.05, -1)\n",
      "Steps 246980/250000 (98.792%) - Rewards: (-1.1, -1.1, -1, -0.10000000000000009)\n",
      "Steps 247143/250000 (98.857%) - Rewards: (-1.1, -1.15, -1.05, -1.05)\n",
      "Steps 247326/250000 (98.93%) - Rewards: (-0.2500000000000001, -1, -1.05, -1.05)\n",
      "Steps 247489/250000 (98.996%) - Rewards: (-1.05, -1.05, -1.1, -1.1)\n",
      "Steps 247692/250000 (99.077%) - Rewards: (-0.10000000000000009, -0.050000000000000044, -1.05, -1.05)\n",
      "Steps 247855/250000 (99.142%) - Rewards: (-1.35, 0.8500000000000001, -1.05, -1.05)\n",
      "Steps 248018/250000 (99.207%) - Rewards: (-1.5499999999999998, -1, -1.05, -1.2)\n",
      "Steps 248181/250000 (99.272%) - Rewards: (-1.25, -1, -1, -1.05)\n",
      "Steps 248344/250000 (99.338%) - Rewards: (-0.09999999999999998, -1.1, -1.1, -1.05)\n",
      "Steps 248507/250000 (99.403%) - Rewards: (-1.15, -1.05, -1, -1.05)\n",
      "Steps 248688/250000 (99.475%) - Rewards: (-1.15, -1.1, -1, -0.15000000000000002)\n",
      "Steps 248871/250000 (99.548%) - Rewards: (-1.1, -1.05, -1.05, -0.20000000000000018)\n",
      "Steps 249034/250000 (99.614%) - Rewards: (-1.15, -1.1, -1.05, -0.15000000000000013)\n",
      "Steps 249197/250000 (99.679%) - Rewards: (-1.1, -1, -1.1, -1.05)\n",
      "Steps 249360/250000 (99.744%) - Rewards: (-1.1, -0.050000000000000044, -1.05, -1.05)\n",
      "Steps 249523/250000 (99.809%) - Rewards: (-1.15, -0.050000000000000044, -1.05, -1.1)\n",
      "Steps 249686/250000 (99.874%) - Rewards: (-1.25, -1, -1.2, -1.05)\n",
      "Steps 249849/250000 (99.94%) - Rewards: (-1.45, -1.4, -0.050000000000000044, -1.15)\n",
      "Steps 250032/250000 (100.013%) - Rewards: (-1.1, -0.5499999999999999, -1.05, -1)\n",
      "Entraînement terminé!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# init de l'environnement et de chaque agent\n",
    "env = knights_archers_zombies_v10.env(\n",
    "  spawn_rate=20,\n",
    "  num_archers=4,\n",
    "  num_knights=0,\n",
    "  max_zombies=10,\n",
    "  max_arrows=20,\n",
    "  killable_knights=True,\n",
    "  killable_archers=True,\n",
    "  max_cycles=900,)\n",
    "env = aec_to_parallel(env)\n",
    "\n",
    "env.reset()\n",
    "agents = env.agents  # Obtenir la liste des agents apres reset\n",
    "num_agents = len(agents)\n",
    "state_dims = env.observation_space(agents[0]).shape[0] * 5\n",
    "action_dims = env.action_space(agents[0]).n\n",
    "\n",
    "# Hyperparams\n",
    "epsilon = 0.8\n",
    "epsilon_decay = 0.98\n",
    "epsilon_min = 0.05\n",
    "gamma = 0.999\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "buffer_capacity = 1024\n",
    "target_update_freq = 3\n",
    "num_episodes = 1_000_000 # Tres grand pour un arret via le nombre de pas  TODO\n",
    "reward_penalty = 1\n",
    "\n",
    "# init des réseaux Q et des buffers pour chaque agent\n",
    "q_networks =        {agent: QNetwork(state_dims, action_dims) for agent in agents}\n",
    "target_networks =   {agent: QNetwork(state_dims, action_dims) for agent in agents}\n",
    "optimizers =        {agent: optim.Adam(q_networks[agent].parameters(), lr=lr) for agent in agents}\n",
    "buffers =           {agent: ReplayBuffer(buffer_capacity) for agent in agents}\n",
    "\n",
    "# Gestion des agents inactifs / blackdeath a la main\n",
    "black_obs = {agent: np.zeros_like(env.observation_space(agent).low) for agent in env.agents}\n",
    "black_rewards = {agent: 0 for agent in env.agents}\n",
    "black_dones = {agent: True for agent in env.agents}\n",
    "\n",
    "# Nombre de pas maximum par épisode\n",
    "max_steps = 500\n",
    "nb_steps = 0\n",
    "total_nb_steps = 0\n",
    "max_total_steps = 250_000\n",
    "\n",
    "# Logs\n",
    "step_count = []\n",
    "episode_rewards = []\n",
    "agents_rewards = {agent: [] for agent in agents}\n",
    "intra_episode_rewards = []\n",
    "# Répertoire pour les logs de TensorBoard\n",
    "#log_dir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_\" + str(max_total_steps) +\"steps\"\n",
    "log_dir = \"logs/scalars/arrowpenalty_4_250000steps\" \n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "\n",
    "# Synchronisation initiale des réseaux cibles\n",
    "for agent in agents:\n",
    "    target_networks[agent].load_state_dict(q_networks[agent].state_dict())\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for episode in range(num_episodes):\n",
    "   # obs,_ = env.reset()\n",
    "   # state = {agent: env.observe(agent) for agent in agents}  # Initialiser l'état pour chaque agent\n",
    "    state,_ = env.reset()\n",
    "    done = {agent: False for agent in agents}\n",
    "    episode_reward = {agent: 0 for agent in agents}\n",
    "\n",
    "    # Init des rewards intrinseques par agent\n",
    "    for agent in agents:\n",
    "        agents_rewards[agent].append(0.0)\n",
    "    intra_episode_rewards.append(0.0)\n",
    "\n",
    "    while not (all(done.values()) or nb_steps >= max_steps):\n",
    "        actions = {}\n",
    "        \n",
    "        # Sélection d'actions pour chaque agent\n",
    "        for agent in agents:\n",
    "            if not done[agent]:\n",
    "                action = select_action(state[agent], q_networks[agent], epsilon, action_dims)\n",
    "                \n",
    "                # Vérification de l'action\n",
    "                assert 0 <= action < action_dims, f\"Action {action} pour {agent} n'est pas dans l'espace d'actions [0, {action_dims - 1}]\"\n",
    "                actions[agent] = action\n",
    "                #print(f\"Agent {agent} a choisi l'action {action}\")\n",
    "\n",
    "        # Environnement procède aux actions\n",
    "        \n",
    "        next_state, rewards, dones, _ ,_= env.step(actions)\n",
    "    \n",
    "\n",
    "        # Reward intrinseque\n",
    "        for agent in agents:\n",
    "            agents_rewards[agent][-1] += rewards[agent]\n",
    "            intra_episode_rewards[-1] += rewards[agent]\n",
    "\n",
    "        # Penalise le tire de fleche\n",
    "        for agent in actions:\n",
    "            if actions[agent] == 4:\n",
    "                rewards[agent] = -0.05\n",
    "\n",
    "        dones = {**black_dones, **dones}\n",
    "        rewards = {**black_rewards, **rewards}\n",
    "        next_state = {**black_obs, **next_state}\n",
    "        #print(rewards)\n",
    "\n",
    "        # mean_global_reward = np.mean(list(rewards.values())) #TODO Maybe ?\n",
    "        # rewards = {agent: rewards[agent]+mean_global_reward for agent in agents} \n",
    "        # Enregistrement de chaque expérience dans le buffer\n",
    "        for agent in agents:\n",
    "            if not done[agent]:\n",
    "                if(dones[agent]):\n",
    "                    rewards[agent]-= reward_penalty\n",
    "\n",
    "                # rewards[agent] += mean_global_reward # Ajout de la récompense moyenne globale \n",
    "                \n",
    "                buffers[agent].push(state[agent], actions[agent], rewards[agent], next_state[agent], dones[agent])\n",
    "                episode_reward[agent] += rewards[agent]\n",
    "\n",
    "            # Mise à jour des états et terminaux\n",
    "            \n",
    "            done[agent] = dones[agent]\n",
    "        \n",
    "        # Mise à jour des états pour le prochain pas de temps\n",
    "        for agent in agents:\n",
    "            if not done[agent]:\n",
    "                state[agent] = next_state[agent]  # Mettre à jour l'état pour chaque agent\n",
    "\n",
    "        # Apprentissage pour chaque agent\n",
    "        for agent in agents:\n",
    "            if len(buffers[agent]) >= batch_size: # and not done[agent]:\n",
    "\n",
    "                # Extraction d'un mini-lot d'expériences\n",
    "                batch = buffers[agent].sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Conversion des expériences en tenseurs\n",
    "                states = torch.FloatTensor(np.array([s.flatten() for s in states])) #states = torch.FloatTensor([s.flatten() for s in states])\n",
    "                next_states = torch.FloatTensor([ns.flatten() for ns in next_states])\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards = torch.FloatTensor(rewards)\n",
    "                dones = torch.FloatTensor(dones)\n",
    "\n",
    "                # Calcul des valeurs Q cibles\n",
    "                with torch.no_grad():\n",
    "                    target_values = rewards + gamma * torch.max(target_networks[agent](next_states), dim=1).values * (1 - dones)\n",
    "\n",
    "                # Calcul des valeurs Q prédites\n",
    "                q_values = q_networks[agent](states)\n",
    "                q_values = q_values.gather(1, actions)\n",
    "\n",
    "                # Calcul de la perte\n",
    "                loss = nn.MSELoss()(q_values, target_values.unsqueeze(1))\n",
    "\n",
    "                # Mise à jour du réseau\n",
    "                optimizers[agent].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizers[agent].step()\n",
    "                \n",
    "\n",
    "        # Mise à jour des réseaux cibles\n",
    "        if episode % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                target_networks[agent].load_state_dict(q_networks[agent].state_dict())\n",
    "\n",
    "        nb_steps += 1\n",
    "        total_nb_steps += 1\n",
    "\n",
    "\n",
    "\n",
    "    nb_steps = 0\n",
    "    # Décroissance de epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # print\n",
    "    print(f\"Steps {total_nb_steps}/{max_total_steps} ({round((total_nb_steps/max_total_steps)*100,3)}%) - Rewards: {tuple(episode_reward.values())}\")\n",
    "    \n",
    "    # Logs\n",
    "    episode_rewards.append(sum(episode_reward.values()))\n",
    "    step_count.append(total_nb_steps)\n",
    "\n",
    "    # for agent in agents:\n",
    "    #     agents_rewards[agent].append(episode_reward[agent])\n",
    "\n",
    "    with writer.as_default():\n",
    "        for agent in agents:\n",
    "            tf.summary.scalar(f\"Agent_{agent}/Reward\", agents_rewards[agent][-1], step=total_nb_steps)\n",
    "        tf.summary.scalar(\"Global/Reward\", intra_episode_rewards[-1], step=total_nb_steps)\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "    # Fin de l'entraînement\n",
    "    if total_nb_steps >= max_total_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Entraînement terminé!\")\n",
    "\n",
    "# Sauvegarde des réseaux Q\n",
    "for agent in agents:\n",
    "    torch.save(q_networks[agent].state_dict(), f\"q_network_agent_{agent}_knights_archers_zombies.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
